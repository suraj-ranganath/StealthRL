{"id": "cs224n_0090", "text": "Title: Towards a Robust Question Answering System through Domain-adaptive Pretraining and Data Augmentation\nAbstract: Large pretrained language models have shown great success over a bunch of tasks in the past few years. These large language models are trained on enormous corpus, and it now becomes a question whether they are robust to domain shift. We find in this paper that the domain of question answering (QA) problems has significant impact on the performance of these fine-tuned LMs and these fine-tuned QA models are still sensitive to domain shift during test time. This potentially causes problems in many real-word applications where broad or evolving domains are involved. So, how can we improve model robustness? In this paper, we offer two potential solutions. First, we propose to continue pretraining on the objective domains. This second-phase of pretraining helps model focus on information that is relevant to the problem. We find that domain-adaptive pretraining helps improve out-of-domain test performance. In some cases, we might have additional small amount of training data on the test domain. We propose to use data augmentation tricks to maximally utilize these data for domain adaptation purpose. We find that data augmentation tricks, including synonym replacement, random insertion and random deletion, can further improve the performance on out-of-domain test samples. Our work shows that the improvements in performance from domain-adaptive pretraining and data augmentation are additive. With both methods applied, our model achieves a test performance of 60.731 in F1 score and 42.248 in EM score. The experiments and methods discussed in this paper will contribute to a deeper understanding of LMs and efforts towards building a more robust QA system.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0055", "text": "As far as I am concerned, television is the most important invention of the last 100 years. First, from the news broadcasting in the television, we can know what is happening in the outside world even without stepping out our living room with television in it. And second, watching television with family members after supper is a good way to relax after a hard day's work. What's more, its visual effect makes people feel less stressed and has a great influence on the way people think and talk.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0037", "text": "I totally agree. First of all, classes are for everybody, you learn common things that everyone need to know in class, but they might not suit your special situation. And more importantly, the things taught in class are really limited, the teachers only give you a head start, they teach you how to learn by yourself. But the wisdom of the universe rests in our daily life. You can learn from books, from museums, from work, from the communication with other people. Those are the thing you actually need in your real life, and there's no way you can find them in textbooks.", "source": "TOEFL11", "is_esl": true}
{"id": "college_0039", "text": "M7652-000. Or at least that's how my bike-tire-greased, highlight-yellow, heel-cap-ripping-at-the-seams Chuck-Taylors are supposed to be colored. Freshman year, I tried so hard to keep them that pristine, popular M7652-000 color. Time progressed, however, and dirt, guitar chords, and conversations eventually covered the canvas of the shoes. When I first moved to Houston in eighth grade, I tried to follow the status quo and keep my shoes white. But as various conflicting influences crept into my life--Liberal vs. Conservative; Portland, OR vs. Houston, TX; LCD Sound system vs. Ed Sheeran--I began to realize how important it is to listen to the other side and to maintain the confidence to pursue my passions while inspiring others to do the same. I needed to appreciate Houston's voice and learn from its stories as much as it needed to hear mine, and my shoes grew dirtier every day as each person's testimony helped solidify and expand my own. As I walk, one can first make out \"Cheyenne yo yo\" engulfing the right inner canvas, weaving through clasps and eventually boarding \"PORTLAND!!!\" I met Cheyenne through Freshman year volleyball and we were friends because I tried; I borrowed cowboy boots for football games, didn't discuss my quirky music, and washed my shoes. As I grew, however, it was our differences that brought us together. She forced me to see the other side, forced me to make my own conclusions without the influence of my background or parents. In Portland, opinions are liberally voiced, and it's similar in my community in Houston, except rather than an abundance of Lizzie Fletcher stickers it's \"Come and Take It\". When I moved, I was bombarded by a completely foreign culture. By sophomore year, however, I realized that compromising myself in order to fit in was a mistake. I began vocally expressing my sentiments towards the world to my friends as I learned from theirs. While I introduced my friends to thrift-shopping and wrote articles about more environmentally friendly methods of transportation, they took me to my first line-dance and helped me examine the other side of gun-control in `Agora Coffee House'. As I grew more comfortable with expressing my beliefs, I began heading projects to install a bike rack around campus and took to writing more iconoclastic political pieces in English class. My left shoe houses various meme references, chords from songs I have written, sketches of the latest NASA star cluster discoveries, practice lines of Italian greetings from when I was set on learning it, and \"Lorrie Lake Ln.\" in small cursive letters. Sandalwood, my friends and I call it--a late-night, post-fast food, teen-angst polluted lake. Sandalwood is the cosmos and the meaning of God and the Sisyphus-like emotions that we discuss there. I never knew that Mormons couldn't drink coffee or that Romanians gut an entire pig to feast on for all of winter. Their philosophies, although often dissonating from my own, taught me that it's often beneficial to disagree. When I was hurled into Texas, I was miserable when I didn't express myself within the Kinkaid-bubble. However, I quickly began to realize that I didn't have to like Ed Sheeran or keep my shoes M7652-000 to enjoy life. Learning to embrace and assess so many dissonating ideas has enabled to grow more into myself--it makes me more nonpartisan and has educated me on what it truly means to listen to the other side. Now, whether it's Texas or Oregon, Republican or Democrat, my life is a playlist of contradictions. In college, where everyone works on discovering \"who they are\" or what their place is in the world, I know I can provide not only diversity of thought, but can educate people through my own stories on how crucial it is to maintain an open-minded ideology towards the world and an individual's power to change it.", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0070", "text": "I prefer to work at office for two reasons. First, it will be more efficient for me to work at office. Last week, I had a bad cough and I had to work at home. I found I couldn't concentrate on work because the environment was so comfortable that all I wanted to do was sleeping. Second, it will be helpful to work at office because I can discuss some problems with my colleagues directly. There is no doubt that in this way can I figure out the problem and finish the work more quickly.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0012", "text": "Speaking of celebrations, there is one day of the year that is second to none in china, it is the first day of the year in lunar calendar, which is called the Spring Festival by most Chinese people. Even if you are indifferent with festive events, you can not resist the charm of a family reunion dinner which marks the defining feature of the Spring Festival. And during the dinner, family members make toasts for each other in order to wish for joy, peace, health and fortune for the next year.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0063", "text": "First, the most common purpose for us to study is to find a good job and there are much more opportunities in big cities than the small town. If I study in the big cities, I can do a part time job and seize the chance to work there after I graduate from university. In addition, a big city means more museums and theaters, which is attractive to me. I was taken to theater with my grandpa when I was four years old and since then I fell in love with operas and history.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0046", "text": "Title: ALP-Net: Robust few-shot Question-Answering with Adversarial Training, Meta Learning, Data Augmentation and Answer Length Penalty\nAbstract: While deep learning has been very successful in the question answering tasks, it is very easy for models trained on a specific data to perform badly on other dataset. To overcome this, In our paper, we proposed ALP-Net to build a robust question answering system that can adapt to new tasks with few-shot learning using answer length penalty, data augmentation, adversarial training and meta learning.\n1. First, We proposed a new answer length penalty that penalizes the model if the predicted answer is too long, as the baseline QA model tends to generate very long answers. This simple optimization is proved to be very effective in shortening the answers and improving Exact Match.\n2. We also applied data augmentation to generate new data for low-resource datasets by doing synonym replacement and word addition. With data augmentation, the model is more unlikely to learn brittle features such as the occurrences of certain words and fixed answer positions, leading to improved F1.\n3. ALP-Net also adopted adversarial training. We applied a discriminator to determine whether the features learned by the model are domain specific. With adversarial learning, models can learn domain agnostic features that could be applied to unseen domains. We found that while being effective in the few-shot learning task, adversarial training should not be used on out-of-domain training data to keep its domain knowledge.\n4. We also tried meta learning to adopt the mean of different sets of model parameters learned from data of different domains. However, it did not perform well and we found that it is hard to learn general knowledge across domains for question answering tasks.\nAmong these approaches, data augmentation and answer length penalty contribute the most to our model performance, allowing us to achieve 60.962 F1 and 43.005 EM score on out-of-domain datasets test data.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0103", "text": "Title: RobustQA: Adversarial Training with Hyperparameter Tuning\nAbstract: In this project, I used adversarial training and hyperparameter tuning to build a question answering system that can adapt to unseen domains with only a few training examples from the domain.  From a high-level perspective, there are two model architectures: the baseline model provided by the starter code and my own adversarial model.  To compare the performance of the two model architectures, I experiment with ADAM debiasing, various batch sizes, and weight decay tuning.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0038", "text": "Title: Extending BiDAF and QANet NLP on SQuAD 2.0\nAbstract: By exploiting self-matching attention in BiDAF and multihead attention in QANet, our project demonstrates that attention helps to cope with long term interactions in the neural architecture for question answering system. Our addition of self-matching attention in BiDAF matches the question-aware passage representation against itself. It dynamically collects evidence from the whole passage and encodes the evidence relevant to the current passage word. In QANet, convolution and self-attention are building blocks of encoders that separately encode the query and the context. Our implementation of multihead attention in QANet, ran through the attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Multiple attention heads allow for attending to parts of the sequence differently, so longer-term dependencies are also taken into account, not just shorter-term dependencies.\n\nWe saw some interesting trends while doing Qualitative error analysis of our output. Model was able to answer \"who\" questions better than \"what\" questions. When the \"what\" question was framed differently, like \u201cEconomy, Energy and Tourism is one of the what?\u201d Even though the passage contains the answer, the model could not predict. Also, we observed wrong predictions in general  for questions involving relationships, like: \"Who was Kaidu's grandfather?\" The passage did not mention it explicitly \"Kaidu's grandfather was ...\", however it had the clue: \"Ogedei's  grandson Kaidu ...\", but it could not interpret the correct answer from the passage and instead provided a wrong answer. We also noticed the model could not predict at all a lot of \"which\" questions. Further analysis revealed that those \"which\" questions require a bit more contextual understanding. It was a good learning experience and the model prediction provided a lot of clues as to how we can improve the model to the next level.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0104", "text": "Title: Multi-Task Learning and Domain-Specific Models to Improve Robustness of QA System\nAbstract: In CS224N course project, we develop a Robust Question Answering (QA) language model that works well on low resource out-of-domain (OOD) data from three domains. Our approach is to take the pre-trained DistilBERT model on high-resource in-domain dataset and then perform multi-task training. We implement multi-task training model that uses unlabeled text from OOD data for Masked Language Model Objective as well as labeled QA data from high-resource setting. The model jointly trains on unlabeled text and QA data to preserve the QA representation from high-resource data and adapt to low-resource OOD. We also explore data augmentation techniques such as synonym replacement, random word deletions and insertions, word swapping, and back-translation to expand our out-of-domain dataset. Finally, we use Domain-Specific Models to have separate models for different datasets and observe that we get the best result on different datasets using different strategies. As a result we achieved the score of 59.203 F1 and 42.362 EM on the test set, 54.41 F1 and 41.62 EM on the validation set.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0107", "text": "Title: Mixture of Experts and Back-Translation to improve QA robustness\nAbstract: This work improves the generalization of a DistilBERT-based Question Answering (QA) model with the addition of a Mixture of Experts (MoE) layer as well as through data augmentation via back-translation. QA models generally struggle to perform in contexts that differ from those present in the model's training data. As a step towards addressing this limitation, our MoE implementation effectively learns domain-invariant features without explicitly training each expert on individual subdomains. We also apply top-k sampling back-translation and introduce a new technique to more effectively retrieve the answer span from the back-translated context. We find that the addition of the MoE layer yields an improvement of 3.19 in F1 score on an out-of-domain validation set, with back-translation granting a further 1.75 in F1 score. This represents a net improvement of 10.1% over the DistilBERT baseline.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0011", "text": "Title: An Unsupervised Pretraining Task for the BiDAF Model\nAbstract: Over the past few years and particularly since \"Attention is All You Need\" was published, the NLP community has moved away from LSTM-based architectures because of the benefits seen by attention-only networks with extensive unsupervised pre-training. This project demonstrated that EM, F1 and AvNA scores can be improved on a BiDAF model simply by pretraining on a similar task to that used in the original BERT paper. While the BERT paper used a Masked Language Model (MLM) and Next Sentence Predictions (NSP), this paper utilizes a novel variant of MLM, termed Obscured Replacement Language Model (ORLM), to enable the strict input-output mappings of a BiDAF model to learn from an unsupervised task. Specifically, this paper shows that performance gains over the baseline BiDAF model can be achieved using ORLM, as judged by the EM and F1 scores. Furthermore, pretraining the BiDAF model with this method decreases the amount of training required on the SQuAD 2.0 training dataset to achieve similar performances, while boosting task-specific metrics such as the AvNA score. As the community concretely moves away from LSTM-based architectures, there is room to ask whether the true top-end performance of those architectures was explored, even if they continue to fall short of state-of-the-art.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0023", "text": "I prefer to take several short vacations in a year. For one thing, the beauty of vacation rests on our excitement, the excitement to plan trips, to spend time with loved ones or so on. But too long a vacation will only gradually consume the excitement. And finally you will be boring, so the rest of the vacation will be totally wasted. And also, people need to release their stresses once in a while, you can't expect them to do that once a year, it's likely that they will snap at some point. So I don't think taking a long vacation once a year is a good choice.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0059", "text": "Title: Question Answering with Binary Objective\nAbstract: We added a secondary binary objective of predicting answerability to QANet. As shown in the picture, this objective is computed using the three outputs from the modeling layer in QANet. More specifically, we concatenate the 0th words of m0, m1, m2 (these are the outputs of the first, second, and third pass of the modeling encoder) and pass it through a single feed-forward layer with sigmoid activation. Our results showed that adding this secondary objective resulted in meaningful improvements in both EM and F1 over our implementation of QANet, which mostly follows the official QANet but we added a project layer on the output of the context-query attention layer to reduce memory usage. We also were able to produce the performance gains from adding character-level encoding, replacing RNN with multi-head self-attention and convolutions, and applying layer-wise dropout (stochastic depth).", "source": "CS224N", "is_esl": false}
{"id": "toefl_0068", "text": "I think the government has responsibility to help to build the museums and theaters. First, there is no doubt that these places can reflect the unique culture of the city and help tourists learn about the city quickly. Therefore, more tourists will be attracted to the city. Second, this measure is the protection of the culture and it can encourage people to learn about the culture. I was taken to theater when I was 4 years old with my grandpa. Since then, I became a fans of Beijing Opera and it is also a valuable memory between me and my grandpa.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0005", "text": "Title: Fine Grained Gating on SQUAD\nAbstract: The purpose of this project is to implement an embedding mechanism on top of the BiDaf model that serves as a compromise between word-level embeddings and character-based embeddings that can compete with a simple concatenation of word and character level embeddings. In particular, the mechanism is what is called a fine-grained gating method, in which, given a character level embedding $c$ and a word-level embedding $w$, a parameter $g$ is learned such that final embedding of a given word is $g \\odot c + (1-g) \\odot w$, where $\\odot$ represents termwise multiplication. After various experiments varying the methods by which the parameter $g$ is learned, results ultimately show that none of the fine-tuned gating methods perform better than mere concatenation of the word and character embeddings.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0018", "text": "in my opinion, it is my family members and friends who influence me most. The information provided by newspaper, TV, radio or other formats of media can just let us know what has happened and only equip us with knowledge, they are incapable of giving me spiritual satisfaction. Speaking of some issues, we need more specific suggestions. Family members and friends can offer us this kind of information and can give us adequate help to meet the issues.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0086", "text": "Title: Domain-Adversarial Training For Robust Question-Answering\nAbstract: In this project, we created a domain-adversarial model to improve upon the baseline DistilBERT model on the task of robustly answering reading comprehension questions across domains. The way the adversarial model works is by creating a discriminator, which is trained to decide based on the last layer of our question-answering model which domain the question came from. Then, our question answering model is trying to not only answer questions correctly but also to trick the discriminator as much as possible, which forces it to prioritize features of the question and context which are not domain-specific in this final hidden layer. Our model got an EM score of 41.353 and F1 score of 59.803 on the test set.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0136", "text": "Title: Attention-aware attention (A^3): combining coattention and self-attention for question answering\nAbstract: Attention has been one of the biggest recent breakthrough in NLP, paving the way for the improvement of state-of-art models in many tasks. In question answering, it has been successfully applied under many forms, especially with recurrent models (encoder-decoder fashion). Co-attention and multihead self-attention have been two interesting attention variations, but a larger study trying to combine them has never been conducted to the best of our knowledge. Hence, the purpose of this paper is to experiment different attention-based architecture types for question answering, as variations from one of the first successful recurrent encoder-decoder models for this task: BiDAF.  We implement a variation of the attention layer, starting with a multi-head self-attention mechanism, on both the query and the context tokens separately, as provided by the encoder layer. Then, these contextualized tokens, added to the input tokens through a skip connection, are passed to a trilinear cross-attention and used to compute two matrices: a context to query matrix and a context to query to context matrix. These two matrices are concatenated with the self-attended context tokens into an output matrix. In addition, we provide our model a character embedding, which proves to have an important positive impact on the performance, as well as a conditional output layer. We test the performance of our model on the Stanford Question Answering Dataset 2.0 and achieved a performance of EM = 62.730 and F1 = 66.283 on the dev set, and EM = 60.490 and F1 = 64.081 on the test set. This provides +7.26 EM score and +6.95 F1 score compared to our coattention baseline, and +4.72 EM score and +4.97 F1 score compared to our BiDAF baseline.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0059", "text": "I prefer working in offices. We know that for most of situations, working is all about cooperation and communication, which can be seriously affected if you are working alone at home. And when you have problems, it's obviously more efficient to discuss with other people, they may provide another respect of thinking. Studies also show that people are likely to lose focus when working alone. And besides, working in offices can help make friends, which can make you love more about your job. People are social animals, spending too much time alone is not healthy for our minds.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0058", "text": "Title: Building a Robust QA System Via Diverse Backtranslation\nAbstract: While question answering (QA) systems have been an active topic of research in recent years, these models typically perform poorly on out-of-domain datasets. Thus, the goal for our project was to build a question answering system that is robust to distributional shift. Utilizing a pretrained DistilBERT model as our baseline, we tested two adaptation methods: backtranslation and few-sample fine-tuning. Backtranslation, which involves translating input data into an intermediate language before translating back to the original language, is a common data augmentation technique in many NLP tasks.  We found that implementing standard backtranslation on out-of-domain training examples yielded significant increases in Exact Match (EM) and F1 scores over our baseline model. We compared these results to several modified backtranslation schemes including one in which we combined backtranslation with techniques from few-sample fine-tuning. Ultimately, we found that combining few-sample fine-tuning techniques with backtranslation did not improve performance. Our best model achieved an EM of 42.225 and F1 of 59.162 on the test set, and an EM of 38.74 and F1 of 51.19 on the development set.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0036", "text": "There is no doubt that my favorite sport is swimming. It is scientific fact that swimming is the best exercise you can do. It requires both upper and lower strength, and it's all about endurance and stamina, which is essential in building up your body. When you swim for competition you are going to burn more calories in 20 minutes than you would in an hour in most other sports. Not to mention the indirect effects of swimming. Swimming in a pool rather than sitting there staring at the computer screen will certainly benefit both your physical and mental condition.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0118", "text": "Title: Answer Pointer Inspired BiDAF And QANet For Machine Comprehension\nAbstract: Imagine that you are trying to find the answer for a question given a context paragraph. This kind of tasks fall into the category of one of the hottest topics in NLP - machine comprehension. With the help of emerging high-performance GPUs, deep learning for machine comprehension has progressed tremendously. RNN based methods, such as Match-LSTM and Bidirectional Attention Flow (BiDAF), and transformer-like methods, such as QANet, keep pushing the performance boundary of machine comprehension on the SQuAD datasets. Our team proposes to improve the performance of the baseline BiDAF and the QANet models on SQuAD 2.0. We replace the original output layer of BiDAF and QANet with Answer Pointer inspired output layers and add character level embedding and ReLU MLP fusion function to the baseline BiDAF model.  We achieve significantly better performance using ensemble learning with majority voting on modified BiDAF, QANet1, and QANet3 models. Specifically, the ensemble learning achieves a F1 score of 66.219 and a EM score of 62.840 on the test datasets and a F1 score of 68.024 and a EM score of 64.561 on the validation datasets.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0013", "text": "Title: QANet for SQuAD 2.0\nAbstract: QANet model was one of the state-of-the-art models for SQuAD 1.1. Does its top-notch performance transfer to the more challenging SQuAD 2.0 dataset containing unanswerable questions? How does the model size affect performance? Is the bi-directional attention layer really necessary in a transformer-style architecture? These are the questions, I tried to answer in this project. Compared to the three baselines derived from the BiDAF model, QANet achieved substantially higher F1 and EM scores of 67.54 and 63.99 respectively. However, these scores are significantly lower than those of the current state-of-the-art models, mainly because the model couldn't correctly handle unanswerable questions. Next, experiments with model size showed no performance degradation with smaller-sized QANet variants. In fact, these variants slightly outperformed the base QANet. Lastly, a new model built entirely using QANet's building blocks (without an explicit bi-directional attention layer) outperformed all of the baseline models even without finetuning. Its performance is still below the base QANet model most likely because the model started overfitting roughly midway through training. I believe adding more regularization and further finetuning would bring its performance close to that of the base QANet model.", "source": "CS224N", "is_esl": false}
{"id": "college_0027", "text": "Red, orange, purple, gold...I was caught in a riot of shifting colors. I pranced up and down the hill, my palms extended to the moving collage of butterflies that surrounded me. \u201cWould you like to learn how to catch one?\u201d Grandfather asked, holding out a glass jar. \u201cYes!\u201d I cheered, his huge calloused fingers closing my chubby five-year-old hands around it carefully. Grandfather put his finger to his lips, and I obliged as I watched him deftly maneuver his net. He caught one marvelous butterfly perched on a flower, and I clutched the open jar in anticipation as he slid the butterfly inside. It quivered and fell to the bottom of the jar, and I gasped. It struggled until its wings, ablaze in a glory of orange and red, quivered to a stop. I watched, wide-eyed, as it stopped moving. \u201cGrandpa! What\u2019s happening?\u201d My grandfather had always had a collection of butterflies, but that was the first time I saw him catch one. After witnessing the first butterfly die, I begged him to keep them alive; I even secretly let some of them go. Therefore, to compromise, he began carrying a special jar for the days I accompanied him on his outings, a jar to keep the living butterflies. But the creatures we caught always weakened and died after a few days in captivity, no matter how tenderly I fed and cared for them. Grandfather took me aside and explained that the lifespan of an adult butterfly was very short. They were not meant to live forever: their purpose was to flame brilliantly and then fade away. Thus, his art serves as a memory of their beauty, an acknowledgement of nature\u2019s ephemeral splendor. But nothing could stay the same. I moved to America and as the weekly excursions to the mountainside ended, so did our lessons in nature and science. Although six thousand miles away, I would never forget how my grandpa\u2019s wrinkles creased when he smiled or how he always smelled like mountain flowers. As I grew older and slowly understood how Grandfather lived his life, I began to follow in his footsteps. He protected nature\u2019s beauty from decay with his art, and in the same way, I tried to protect my relationships, my artwork, and my memories. I surrounded myself with the journals we wrote together, but this time I recorded my own accomplishments, hoping to one day show him what I had done. I recorded everything, from the first time I spent a week away from home to the time I received a gold medal at the top of the podium at the California Tae Kwon Do Competition. I filled my new home in America with the photographs from my childhood and began to create art of my own. Instead of catching butterflies like my grandpa, I began experimenting with butterfly wing art as my way of preserving nature\u2019s beauty. Soon my home in America became a replica of my home in China, filled from wall to wall with pictures and memories. Nine long years passed before I was reunited with him. The robust man who once chased me up the hillside had developed arthritis, and his thick black hair had turned white. The grandfather I saw now was not the one I knew; we had no hobby and no history in common, and he became another adult, distant and unapproachable. With this, I forgot all about the journals and photos that I had kept and wanted to share with him. After weeks of avoidance, I gathered my courage and sat with him once again. This time, I carried a large, leather-bound book with me. \u201cGrandfather,\u201d I began, and held out the first of my many journals. These were my early days in America, chronicled through pictures, art, and neatly-printed English. On the last page was a photograph of me and my grandfather, a net in his hand and a jar in mine. As I saw our faces, shining with proud smiles, I began to remember our days on the mountainside, catching butterflies and halting nature\u2019s eventual decay. My grandfather has weakened over the years, but he is still the wise man who raised me and taught me the value of capturing the beauty of life. Although he has grown old, I have grown up. His legs are weak, but his hands are still as gentle as ever. Therefore, this time, it will be different. This time, I will no longer recollect memories, but create new ones. Lean extra in this pirouette; it\u2019s more aesthetic. But is it always better to be safe than sorry? Glancing toward the wings, I see my teacher\u2019s wild gesticulations: Stretch your arms out, she seems to mime, More! A genuine smile replaces one of forced enthusiasm; alone on the stage, this is my chance to shine. I breathe in the movements, forget each individual step. More than just imagining, but finally experiencing the jubilation of the music, I allow my splits to stretch across the stage and my steps to extend longer and longer, until I\u2019m no longer safe and my heart is racing. Exhilarated and scared in the best way, I throw myself into my jumps. I no longer need to imagine scenes to get in the mood; the emotions are twirling and leaping within me. Reaching, stretching, grabbing, flinging ... My fear no longer shields me. I find my old passion for ballet, and remember the grace and poise that can nevertheless convey every color of emotion. Playing it safe will leave me part of the backdrop; only by taking risks can I step into the limelight. Maybe I\u2019ll fall, but the rush is worth it. I\u2019ll captain an all-male science bowl team, run a marathon, audition for a musical, and embrace the physical and intellectual elation of taking risks.", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0039", "text": "I would choose health and nutrition classes for two main reasons. First, I deal with nutrition every single day. When I am trying to make healthy choices for my body, I am sometimes uncertain about what the best foods are the most nutritious. A health and nutrition class would surely ease that trouble and help me make choices that are good for my body. Second, I am not very interested in sciences, so I don\u2019t think an energy and environment or solar system class would be very intriguing to me. I would rather take classes that excite me rather than the class I was totally disinterested. So, for these two reasons, I would choose to take a health and nutrition class, rather than energy and environment, or solar system course.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0089", "text": "Well, I think there are several ways to keep myself healthy. As for physical health, I'll do some exercise regularly. For example, I'll work out in a gym or play ball games with my friends, such as basketball or volleyball and I will try to keep a healthy diet which means to avoid eating fast food that are high in oil, sugar and salt. And I will try to eat more fresh fruits and vegetables. And as for my mental health, I think I will try to listen to some light music before going to bed. It will help improve the quality of sleep. And when I run into some bad things I'll complain to my close friends or parents rather than keep all the pressure to myself.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0044", "text": "Title: Rediscovering R-NET: An Improvement and In-Depth Analysis on SQUAD 2.0\nAbstract: Question-answering is a discipline within the fields of information retrieval (IR) and natural language processing (NLP) that is concerned with building systems that automatically answer questions posed by humans. In this project, we address the question-answering task by attempting to improve the R-NET model. Specifically, our goals are to 1. reproduce R-NET and evaluate its performance on  SQuAD 2.0 compared to that on the original SQuAD dataset and 2. change certain features of the R-NET model to further improve its accuracy on SQuAD 2.0. We present an implementation of R-NET using LSTM's instead of GRU's, larger embedding and hidden dimensions, higher dropout, and more layers that achieves an improvement in performance from our baseline R-NET model.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0133", "text": "Title: Self-attention and convolution for question answering on SQuAD 2.0: revisiting QANet\nAbstract: QANet was the first Question Answering model that combined self-attention and convolution, without any use of Recurrent Neural Networks. Convinced by the \"Attention is all you need\" motto (or, more accurately in this context, the \"You don't need RNNs\" motto), we were naturally interested in seeing how this applies to the specific task of Question Answering. In this project, we therefore tackle the Question Answering task on the SQuAD 2.0 dataset using different variations of the QANet architecture. We first re-implement the QANet model, and then explore different versions of the architecture, tweaking some parameters such as attention mechanisms and model size. We then propose 3 ensemble models with different inference methods: our best model, using a novel two-step answerability prediction based inference method, achieves 71.21 F1/ 68.14 EM on the development set, and 69.04 F1 / 65.87 EM on the test set.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0110", "text": "Title: Robust QA on out of domain dataset over pretraining and fine tuning\nAbstract: We have seen tremendous progress on natural language understanding problems over the last few years. Meanwhile, we face issues that models learnt from a specific domain couldn't be easily generalized to a different domain. I explored different models to build robust question answering system that can be applied to out-of-domain datasets. Models explored are baseline with and without fine tuning, adding dataset prefix in question with and without fine tuning, switching question and context in question answering system with and without fine tuning, and shorter question and context in model input with and without fine tuning. Different fine tuning techniques like changing epochs, batch size and Adam optimization learning rate were explored to find the best model performance. The best model achieved 40.367 EM and 58.467 F1.", "source": "CS224N", "is_esl": false}
{"id": "college_0059", "text": "Before I came to America, I drank Puer Tea with my father every morning in my bedroom, sitting cross-legged on Suzhou-silk mats beside a view of the Lakeside reservoir. Beside a dark end table, we picked up teacups as the mild aroma greeted our noses. As we faced the French window, my father would share the news he read in China Daily: the Syrian civil war, climate change, and gender equality in Hollywood. Most of the time, I only listened. With each piece of news, my curiosity piqued. Secretly, I made a decision that I wanted to be the one to discuss the news with him from my perspective. So, I decided to study in America to learn more about the world. After one year\u2019s extensive research and hours of interviews, I came to America for 9th grade and moved in with a host family. But, my new room lacked stories and cups of tea. Fortunately, I found Blue House Cafe on my walk home from church, and started studying there. With white walls, comfortable sofas, and high stools, Blue House is spacious and bright. Hearing people\u2019s stories and looking at their warm smiles when they taste various pastries as I sat by the window, I watched as a production designer scouted locations for his film, or a painter took notes while brainstorming for his freehand brushwork of Blue House. With a cup of coffee, I dig into differential and parametric equations for my upcoming AP Calculus test, learn the nuances of public speaking by watching Michael Sandel\u2019s Justice lectures on my laptop, and plan fundraising events for my non-profit. I\u2019ve also learned by watching leaders host meetings at the rectangle conference table at the back of the cafe and I learn from the leaders of meetings, watching as they hold the edge of the table and express their ideas. Similarly, as president of the International Students Club, I invited my teammates to have meetings with me at the cafe. Coordinating the schedule with other members in Blue House has become a frequent event. Consuming several cups of coffee, my team and I have planned Lunar New Year events, field trip to the Golden Gate Bridge, and Chinese lunch in school to help international students feel more at home. Straightening my back and bracing my shoulders, I stood up behind the conference table and expressed my creative ideas passionately. After each meeting, we shared buttermilk coffee-cake. In my spot next to the window, I also witnessed different kinds of people. I viewed visitors dragging their luggage, women carrying shopping bags, and people wandering in tattered clothes --the diversity of San Francisco. Two years ago I saw volunteers wearing City Impact shirts offering sandwiches and hot chocolate to homeless people outside of the cafe. I investigated more about City Impact and eventually signed up to volunteer. No longer was I a bystander. At holiday outreach events, I prepared and delivered food to homeless people. While sharing my coffee, I listened to a story from an older Chinese man who told me, in Mandarin, how he had been abandoned by his children and felt lonely. Last summer, I returned to Xiamen, China, and taught my father how to drink coffee. Now, a Chemex and teapot are both on the end table. Instead of simply listening, I shared my experiences as a club president, a community leader, and a volunteer. I showed him my business plan and prototypes. My father raised his cup of coffee and made a toast to me, \u201cGood girl! I am so proud of you.\u201d Then, he patted my head as before. Together, we emptied our cups while the smell of coffee lingered.", "source": "CollegeEssay", "is_esl": false}
{"id": "college_0052", "text": "When I was very little, I caught the travel bug. It started after my grandparents first brought me to their home in France and I have now been to twenty-nine different countries. Each has given me a unique learning experience. At five, I marveled at the Eiffel Tower in the City of Lights. When I was eight, I stood in the heart of Piazza San Marco feeding hordes of pigeons, then glided down Venetian waterways on sleek gondolas. At thirteen, I saw the ancient, megalithic structure of Stonehenge and walked along the Great Wall of China, amazed that the thousand-year-old stones were still in place. It was through exploring cultures around the world that I first became interested in language. It began with French, which taught me the importance of pronunciation. I remember once asking a store owner in Paris where Rue des Pyramides was. But when I pronounced it PYR\u2013a\u2013mides instead of pyr\u2013A\u2013mides, with more accent on the A, she looked at me bewildered. In the eighth grade, I became fascinated with Spanish and aware of its similarities with English through cognates. Baseball in Spanish, for example, is b\u00e9isbol, which looks different but sounds nearly the same. This was incredible to me as it made speech and comprehension more fluid, and even today I find that cognates come to the rescue when I forget how to say something in Spanish. Then, in high school, I developed an enthusiasm for Chinese. As I studied Chinese at my school, I marveled how if just one stroke was missing from a character, the meaning is lost. I loved how long words were formed by combining simpler characters, so Hu\u01d2 (\u706b) meaning fire and Sh\u0101n (\u5c71) meaning mountain can be joined to create Hu\u01d2sh\u0101n (\u706b\u5c71), which means volcano. I love spending hours at a time practicing the characters and I can feel the beauty and rhythm as I form them. Interestingly, after studying foreign languages, I was further intrigued by my native tongue. Through my love of books and fascination with developing a sesquipedalian lexicon (learning big words), I began to expand my English vocabulary. Studying the definitions prompted me to inquire about their origins, and suddenly I wanted to know all about etymology, the history of words. My freshman year I took a world history class and my love for history grew exponentially. To me, history is like a great novel, and it is especially fascinating because it took place in my own world. But the best dimension that language brought to my life is interpersonal connection. When I speak with people in their native language, I find I can connect with them on a more intimate level. I\u2019ve connected with people in the most unlikely places, finding a Bulgarian painter to use my few Bulgarian words with in the streets of Paris, striking up a conversation in Spanish with an Indian woman who used to work at the Argentinian embassy in Mumbai, and surprising a library worker by asking her a question in her native Mandarin. I want to study foreign language and linguistics in college because, in short, it is something that I know I will use and develop for the rest of my life. I will never stop traveling, so attaining fluency in foreign languages will only benefit me. In the future, I hope to use these skills as the foundation of my work, whether it is in international business, foreign diplomacy, or translation. I think of my journey as best expressed through a Chinese proverb that my teacher taught me, \u201cI am like a chicken eating at a mountain of rice.\u201d Each grain is another word for me to learn as I strive to satisfy my unquenchable thirst for knowledge. Today, I still have the travel bug, and now, it seems, I am addicted to language too.", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0043", "text": "I think it's a really terrible idea to tell employees that they can't use their phones during working hours. First of all, our personal phones are the only way for us to hear about emergencies affecting our loved ones.  And this is something we want to hear about as soon as humanly possible.  You know, if a worker feels nervous about being out of touch with his loved ones during the day, he might actually start looking around for a job that lets him use his phone and this would be really bad for business. Secondly, I think personal phones can maybe improve our morale at work.  We can relieve our stress by making a quick phone call or looking at a social media post.  And if we feel happier we'll probably performer a lot better.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0006", "text": "Title: Domain Adversarial Training for QA Systems\nAbstract: In our CS224N project, we examine a QA model trained on SQuAD, NewsQA, and Natural Questions and augment it to improve its ability to generalize to data from other domains. We apply a method known as domain adversarial training (as seen in a research paper we reviewed by Seanie Lee and associates) which involves an adversarial neural network attempting to detect domain-specific model behavior and discouraging this to produce a more general model. We explore the efficacy of this technique as well as the scope of what can be considered a \"domain\" and how the choice of domains affects the performance of the trained model. We find that, in our setting, using a clustering algorithm to sort training data into categories yields a performance benefit for out-of-domain data. We compare the partitioning method used by Lee et al. and our own unsupervised clustering method of partitioning and demonstrate a substantial improvement.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0049", "text": "Title: QANet without Backtranslation on SQUAD 2.0\nAbstract: This paper investigates two different approaches to the question answering problem on the SQuAD 2.0 dataset. We explore a baseline model based on the BiDaF architecture, and improve its performance through the implementation of character embeddings and hyperparameter tuning. Further, we implement variations on the convolution and self-attention based QANet architecture. While the original QANet architecture uses backtranslation to do data augmentation, we explore a simple and effective method that does not have dependencies on machine translation systems to do augmentation. This involves concatenating contexts together and reusing the same query/answer to generate a new answerable query, and dropping an answer span from the context of an answerable query to create an unanswerable query. The effectiveness of this approach demonstrates the importance of data augmentation for the QANet model. Finally, we form an ensemble model based on our different experiments which achieves an F1 score of 70.340 and an EM score of 67.354 on the test set.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0056", "text": "I think what makes a good friend is someone who is honest, supportive, and has a good sense of humor. I just look for someone who's honest to me no matter what. He should not tell stories behind my back\u037e he should tell me my short-comings at my mouth, but never praises me in front of me. It is a well known saying that A FRIEND IN NEED IS A FRIEND INDEED. Therefore, a friend should stand by you in the hour of any sort of need. I just think that a good friend won't leave me if I'm in trouble. They can be my friends in sunshine and in shade, care for me even when the times are bad and even more when the times are great. Also, I'd love to spend more time with someone who can make me laugh and is fun to be around.", "source": "TOEFL11", "is_esl": true}
{"id": "college_0025", "text": "My Ye-Ye always wears a red baseball cap. I think he likes the vivid color\u2014bright and sanguine, like himself. When Ye-Ye came from China to visit us seven years ago, he brought his red cap with him and every night for six months, it sat on the stairway railing post of my house, waiting to be loyally placed back on Ye-Ye\u2019s head the next morning. He wore the cap everywhere: around the house, where he performed magic tricks with it to make my little brother laugh; to the corner store, where he bought me popsicles before using his hat to wipe the beads of summer sweat off my neck. Today whenever I see a red hat, I think of my Ye-Ye and his baseball cap, and I smile. Ye-Ye is the Mandarin word for \u201cgrandfather.\u201d My Ye-Ye is a simple, ordinary person\u2014not rich, not \u201csuccessful\u201d\u2014but he is my greatest source of inspiration and I idolize him. Of all the people I know, Ye-Ye has encountered the most hardship and of all the people I know, Ye-Ye is the most joyful. That these two aspects can coexist in one individual is, in my mind, truly remarkable. Ye-Ye was an orphan. Both his parents died before he was six years old, leaving him and his older brother with no home and no family. When other children gathered to read around stoves at school, Ye-Ye and his brother walked in the bitter cold along railroad tracks, looking for used coal to sell. When other children ran home to loving parents, Ye-Ye and his brother walked along the streets looking for somewhere to sleep. Eight years later, Ye-Ye walked alone\u2014his brother was dead. Ye-Ye managed to survive, and in the meanwhile taught himself to read, write, and do arithmetic. Life was a blessing, he told those around him with a smile. Years later, Ye-Ye\u2019s job sent him to the Gobi Desert, where he and his fellow workers labored for twelve hours a day. The desert wind was merciless; it would snatch their tent in the middle of the night and leave them without supply the next morning. Every year, harsh weather took the lives of some fellow workers. After eight years, Ye-Ye was transferred back to the city where his wife lay sick in bed. At the end of a twelve-hour workday, Ye-Ye took care of his sick wife and three young children. He sat with the children and told them about the wide, starry desert sky and mysterious desert lives. Life was a blessing, he told them with a smile. But life was not easy; there was barely enough money to keep the family from starving. Yet, my dad and his sisters loved going with Ye-Ye to the market. He would buy them little luxuries that their mother would never indulge them in: a small bag of sunflower seeds for two cents, a candy each for three cents. Luxuries as they were, Ye-Ye bought them without hesitation. Anything that could put a smile on the children\u2019s faces and a skip in their steps was priceless. Ye-Ye still goes to the market today. At the age of seventy-eight, he bikes several kilometers each week to buy bags of fresh fruits and vegetables, and then bikes home to share them with his neighbors. He keeps a small patch of strawberries and an apricot tree. When the fruit is ripe, he opens his gate and invites all the children in to pick and eat. He is Ye-Ye to every child in the neighborhood. I had always thought that I was sensible and self-aware. But nothing has made me stare as hard in the mirror as I did after learning about the cruel past that Ye-Ye had suffered and the cheerful attitude he had kept throughout those years. I thought back to all the times when I had gotten upset. My mom forgot to pick me up from the bus station. My computer crashed the day before an assignment was due. They seemed so trivial and childish, and I felt deeply ashamed of myself. Now, whenever I encounter an obstacle that seems overwhelming, I think of Ye-Ye; I see him in his red baseball cap, smiling at me. Like a splash of cool water, his smile rouses me from grief, and reminds me how trivial my worries are and how generous life has been. Today I keep a red baseball cap at the railing post at home where Ye-Ye used to put his every night. Whenever I see the cap, I think of my Ye-Ye, smiling in his red baseball cap, and I smile. Yes, Ye-Ye. Life is a blessing. But what could I do to help? I scoured my mind for the words to settle his demons. But nothing came to me. Impulsively, I hugged him\u2014a gesture of intimacy we camp leaders were encouraged not to initiate, and an act I later discovered no friend had ever offered James before. Then, I put my hand on his shoulder and looked him straight in the eyes. I assured him that external features didn\u2019t matter, and that as long as he was friendly, people would eventually come around. I listed successful individuals who had not been hindered by their abnormalities. And finally, I told him he would always be my favorite camper, regardless of whether he had two, five, or a hundred toes. On the last day of camp, I was jubilant\u2014James was starting to fit in. Although the teasing had not completely disappeared, James was speaking up and making friends. And when, as we were saying our good-byes, James gave me one last hug and proclaimed that I was his \u201cbestest friend in the whole wide world,\u201d my heart swelled up. From my campers, I learned that working with children is simply awesome. And from James, I learned that a little love truly goes a long way.", "source": "CollegeEssay", "is_esl": false}
{"id": "cs224n_0034", "text": "Title: Improving Out-of-Domain Question Answering with Auxiliary Loss and Sequential Layer Unfreezing\nAbstract: The proliferation of pretrained Language Models such as BERT and T5  has been a key development is Natural Language Processing (NLP) over the past several years. In this work, we adapt a DistilBERT model, pretrained on masked language modeling (MLM), for the task of question answering (QA). We train the DistilBERT model on a set of in-domain data and finetune it on a smaller set of out-of-domain (OOD) data, with the goal of developing a model that generalizes well to new datasets. We significantly alter the baseline model by adapting an auxiliary language modeling loss, adding an additional DistilBERT layer, and undergoing training with sequential layer unfreezing. We find that adding an additional layer with sequential layer unfreezing offered the most improvement, producing a final model that achieve 5% over a naive baseline.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0051", "text": "In my opinion, a good leader should have the following qualities: confident and kind. The leader should be confident both of himself and his fellow members. Every time he walks in the working group, he should bring a strong feeling of authority, and makes his students feel a definite sense of trust. And the leader should be kind to his fellow members. If a leader is too strict, his fellow members would be afraid to ask questions, and that will decrease the efficiency of their work.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0079", "text": "Title: Exploring the Architecture of QANet\nAbstract: Before the advent of QANet, dominant question-answering models were based on recurrent neural networks. QANet shows that self-attention and convolutional neural networks can replace recurrent neural networks in question-answering models. We first implemented a version of QANet using the same architecture as that of the original QANet model, and then we conducted experiments on hyperparameters and model architecture. We incorporated attention re-use, gated self-attention, and conditional output into the QANet architecture. Our best QANet model obtained 59.3 EM and 62.82 F1 on the evaluation set. The ensemble of the two best QANet models and one BiDAF model with self-attention mechanism achieved 62.73 EM and 65.77 F1 on the evaluation set and 60.63 EM and 63.69 F1 on the test set.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0112", "text": "Title: BiDAF with Explicit Token Linguistic Features\nAbstract: How do you do reading comprehension? When I learned reading comprehension with English as my second language, I was taught a few tricks. One important trick is to find word correspondences between the text and the question. Another trick is to use information such as part of speech and sentiment of known words to infer meaning of other unknown words. In this project, I explore the effectiveness of those tricks when applied to SQuAD, by supplying BiDAF with explicit linguistic features from the tokenizer as part of the input. I found that although effective at improving the scores, using those features is prone to overfitting if not regulated.", "source": "CS224N", "is_esl": false}
{"id": "college_0048", "text": "When I failed math in my sophomore year of high school, a bitter dispute engulfed my household -- \u201cNicolas Yan vs. Mathematics.\u201d I was the plaintiff, appearing pro se, while my father represented the defendant (inanimate as it was). My brother and sister constituted a rather understaffed jury, and my mother presided over the case as judge. In a frightening departure from racial stereotype, I charged Mathematics with the capital offences of being \u201ctoo difficult\u201d and \u201cirrelevant to my aspirations,\u201d citing my recent shortcomings in the subject as evi. dence. My father entered a not guilty plea on the defendant's behalf, for he had always harbored hopes that I would follow in his entrepreneurial footsteps -- and who ever heard of a businessman who wasn't an accomplished mathematician? He argued that because I had fallen sick before my examination and had been unable to sit one of the papers, it would be a travesty of justice to blame my \u201cUngraded\u201d mark on his client. The judge nodded sagely. With heartrending pathos, I recalled how I had studied A-Level Mathematics with calculus a year before the rest of my cohort, bravely grappling with such perverse concepts as the poisson distribution to no avail. I decried the subject's lack of real-life utility and lamented my inability to reconcile further effort with any plausible success; so that to persist with Mathematics would be a Sisyphean endeavor. Since I had no interest in becoming the entrepreneur that my father envisioned, I petitioned the court for academic refuge in the humanities. The members of the jury exchanged sympathetic glances and put their heads together to deliberate. In hushed tones, they weighed the particulars of the case. Then, my sister announced their unanimous decision with magisterial gravity: \"Nicolas shouldn't have to do math if he doesn't want to!\" I was ecstatic; my father distraught. With a bang of her metaphorical gavel, the judge sentenced the defendant to \"Death by Omission\"-- and so I chose my subjects for 11th Grade sans Mathematics. To my father's disappointment, a future in business for me now seemed implausible. Over the next year, however, new evidence that threw the court's initial verdict into question surfaced. Languishing on death row, Mathematics exercised its right to appeal, and so our quasi-court reconvened in the living room. My father reiterated his client's innocence, maintaining that Mathematics was neither \"irrelevant\" nor \"too difficult.\" He proudly recounted how just two months earlier, when my friends had convinced me to join them in creating a business case competition for high school students (clerical note: the loftily-titled New Zealand Secondary Schools Case Competition), I stood in front of the Board of a company and successfully pitched them to sponsor us-- was this not evidence that l could succeed in business? I think I saw a tear roll down his cheek as he implored me to give Mathematics another chance. I considered the truth of his words. While writing a real-world business case for NZSSCC, l had been struck by how mathematical processes actually made sense when deployed in a practical context, and how numbers could tell a story just as vividly as words can. By reviewing business models and comparing financial projections to actual returns, one can read a company's story and identify areas of potential growth; whether the company then took advantage of these opportunities determined its success. It wasn't that my role in organizing NZSSCC had magically taught me to embrace all things mathematical or commercial -- I was still the same person -- but I recognized that no intellectual constraints prevented me from succeeding in Mathematics; I needed only the courage to seize an opportunity for personal growth. I stood up and addressed my family: \u201cI\u2019ll do it.\u201d Then, without waiting for the court\u2019s final verdict, I crossed the room to embrace my father: and the rest, as they (seldom) say, was Mathematics.", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0067", "text": "I believe that being open-minded is what it takes to makes a good teacher. Many of us were brought up with a set of believes and values, so sometimes it's hard to accept new things. But teachers with an open mind will offer you the chance to change how you view the world, and they will introduce countless of possibilities into your life. And also there's an honesty that comes with an open mind, you admit that there are still tons of things to be discovered, and that attitude will help you explore more about the world.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0081", "text": "Title: Meta-learning with few-shot models Analysis Final Project\nAbstract: This project focuses on understanding the various elements of Meta-learning and few-shot models and the effectiveness of the different detailed implementation approaches.  Using the default RobustQA project as a baseline, we explored the different implementations of the Meta-learning algorithm, LEOPARD, and evaluate the impact on performance of the prediction accuracy.  We have also experimented with the eval-every parameter to understand how fast each implementation can learn when presented with the out of domain questions initially.  We found that the multiple datasets implementation of the Leopard algorithm yields the best few-shot result. On the first evaluation at step 0 (after 1 batch of data for learning) this implementation already achieving a result of a EM score of 34.55 (on the validation set) compared to the ~32 EM scores that the other implementation and the baseline are getting.  However, after the model is trained for a longer time, we found that the baseline can actually achieve a better EM score overall with 42.202 on the test set.  Although, the difference in the overall accuracy of the test set score are very small for different implementations, we found the more simple implementation yields better accuracy in the long run.\nOur key finding is that the design of few-shot learning algorithm or model is actually a trade off between few-shot accuracy and the overall highest achievable accuracy.", "source": "CS224N", "is_esl": false}
{"id": "college_0018", "text": "Walking down a busy street, I see the quick glances and turned heads. The murmurs and giggles trickle toward me. I try to ignore the buzz, interspersed with, \u201cOh my God!\u201d and the occasional, \u201cDamn!\u201d Then, a complete stranger asks for a picture, so I stand with people foreign to me and politely smile and laugh. After the click of the camera, they go on their way. Sometimes I wish I weren\u2019t so tall. Maybe then I could take a friend to a movie and just blend into the crowd. Attention from strangers is nothing new to me. Questions about my height dominate almost every public interaction. My friends say my height is just a physical quality and not a personality trait. However, when I reflect on my life, I realize that my height has shaped my character in many ways and has helped to define the person I am. I learned how to be comfortable in my own skin. If I had the introverted personality my older brother had in high school, I\u2019d probably be overwhelmed by the constant public attention. Even as a young child, parents at the sidelines of my baseball games, as well as the umpire, would, in front of all my teammates, demand by birth certificate to prove my age. I grew acquainted early on with the fact that I am abnormally tall and stick out about the crowd. It\u2019s just the way it is. Being self-conscious about it would be paralyzing. I learned how to be kind. When I was younger, some parents in my neighborhood deemed me a bully because I was so much larger than children my age. I had to be extra welcoming and gentle simply to play with other children. Of course, now my coaches wish I weren\u2019t quite so kind on the basketball court. Even More Essays That WorkdI learned humility. At 7 feet tall, everyone expects me to be an amazing basketball player. They come expecting to see Dirk Nowitzki, and instead they might see a performance more like Will Ferrell in Semi-Pro. I have learned to be humble and to work even harder than my peers to meet their (and my) expectations. I developed a sense of lightheartedness. When people playfully make fun of my height, I laugh at myself too. On my first day of high school, a girl dropped her books in a busy hallway. I crouched down to her level and gathered some of her notebooks. As we both stood up, her eyes widened as I kept rising over her. Dumbfounded, she dropped her books again. Embarrassed, we both laughed and picked up the books a second time. All of these lessons have defined me. People unfamiliar to me have always wanted to engage me in lengthy conversations, so I have had to become comfortable interacting with all kinds of people. Looking back, I realize that through years of such encounters, I have become a confident, articulate person. Being a 7-footer is both a blessing and a curse, but in the end, accepting who you are is the first step to happiness.", "source": "CollegeEssay", "is_esl": false}
{"id": "college_0017", "text": "The black void descends toward the young girl standing in the grassy field. It slowly creeps up on her, and as it reaches for her perfectly white dress \u2026 Swipe. I quickly wipe away the paint without a thought except for panic. Before I realize what I have done, the black droop becomes an ugly smear of black paint. The peaceful picture of the girl standing in the meadow is nowhere to be seen. Even though I successfully avoid having the spilled paint touch the dress, all I can focus on is the black smudge. The stupid black smudge. As I continue to stare at the enemy in front of me, I hear Bob Ross\u2019s annoyingly cheerful voice in my head: \u201cThere are no mistakes, only happy accidents.\u201d At this moment, I completely disagree. There is nothing happy about this, only frustration. Actually, there is one other emotion: excitement. Don\u2019t get me wrong; I\u2019m not excited about making a mistake and definitely not happy about the accident. But I am thrilled at the challenge. The black smudge is taunting me, challenging me to fix the painting that took me hours to do. It is my opponent, and I am not planning to back off, not planning to lose. Looking back at the painting, I refuse to see only the black smudge. If lacrosse has taught me one thing, it is that I will not be bested by my mistakes. I snatch my picture and run downstairs, carefully setting it against the living room window. The TV newscaster drones in the background, \u201cCalifornia continues to be engulfed in flames as the fires continue to burn.\u201d I slowly step back from my painting. California fires, I think, as I look up into the blood-orange sky. California Fires! I look at the painting, imagining the black smudge not as a black void, but smoke creeping up on the girl as she watches the meadow burn. I grab my painting and run back to my room. The orange sky casts eerie shadows as I throw open my blinds. My hands reach first toward the reds, oranges, and yellows: reds as rich as blood; oranges as beautiful as California poppies; yellows as bright as the sun. I splatter them on my palette, making a beautiful assortment of colors that reminds me of one thing: fire. A rich, beautiful, bright thing, but at the same time, dangerous. My hand levitates toward the white and black. White, my ally: peaceful, wonderful, simple white. Black, my enemy: annoying, frustrating, chaotic black. I splat both of them onto a different palette as I create different shades of gray. My brush first dips into red, orange, and yellow as I create the flame around the girl. The flame engulfs the meadow, each stroke of red covering the serene nature. Next is the smoke, I sponge the dull colors onto the canvas, hazing over the fire and the trees, and, most importantly, hiding the smudge. But it doesn\u2019t work. It just looks like more blobs to cover the black smudge. What could make the gray paint turn into the hazy clouds that I have been experiencing for the past several days? I crack my knuckles in habit, and that\u2019s when a new idea pops into my head. My calloused fingers dip into the cold, slimy gray paint, which slowly warms as I rub it between my fingers. My fingers descend onto the canvas, and as they brush against the fabric, I can feel the roughness of the dried paint as I add the new layer. As I work, the tension from my body releases. With each stroke of my fingers, I see what used to be the blobs turn into the thing that has kept me inside my house for weeks. As I lift my last finger off the canvas, I step back and gaze at my new creation. I have won.", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0041", "text": "I believe that being open-minded is what it takes to make a good leader. Many of us were brought up with a set of believes and values, so sometimes it's hard to accept new things. But leader with an open mind will offer you other respects of thinking, rather than getting you trapped by dogma. And also there's an honesty that comes with an open mind, you admit that there are still tons of things to be discovered, and that attitude will always urge you to learn more from others which is definitely great for teamwork.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0053", "text": "Title: Adversarial Training Methods for Cross-Domain Question Answering\nAbstract: Even though many deep learning models surpass human-level performance on tasks like question answering when evaluated on in-domain test-sets, they might perform relatively poorly on out-of-domain datasets. To address this problem, domain adaptation techniques aim to adapt models trained for a task on in-domain datasets to a target domain by using efficiently samples from the latter. On the contrary, domain generalization techniques aim to incentivate the model to learn domain-invariant features directly from in-domain data to generalize the model for any out-of-domain dataset, pushing to learn task-relevant features and preventing overfitting on in-domain data. We like to compare this approach the way humans learn a task, as they can generally perform the same task on different domains from only a few examples. However, domain generalization is often performed by augmenting in-domain data by applying semantic-preserving transformations to challenge the model during training, leveraging some kind of rules or domain knowledge. Contrarily, in this project our goal is to explore domain generalization techniques applied to question answering based on adversarial training without leveraging any set of rules or domain knowledge but using adversarial terms to make more robust the regular loss with or without adopting task-agnostic critic networks. Such extremely general methodology does not suffer from the limitations of synonym replacement approaches and can be applied to other NLP tasks. Our best variant combines two different and complementary approaches of adversarial training on a DistilBERT baseline, achieving >3% F1-score improvement over the regular fine-tuning process, outperforming several other adversarial and energy-based approaches.", "source": "CS224N", "is_esl": false}
{"id": "college_0053", "text": "This was written for a Common App college application essay prompt that no longer exists, which read: Evaluate a significant experience, risk, achievement, ethical dilemma you have faced and its impact on you. Smeared blood, shredded feathers. Clearly, the bird was dead. But wait, the slight fluctuation of its chest, the slow blinking of its shiny black eyes. No, it was alive. I had been typing an English essay when I heard my cat's loud meows and the flutter of wings. I had turned slightly at the noise and had found the barely breathing bird in front of me. The shock came first. Mind racing, heart beating faster, blood draining from my face. I instinctively reached out my hand to hold it, like a long-lost keepsake from my youth. But then I remembered that birds had life, flesh, blood. Death. Dare I say it out loud? Here, in my own home? Within seconds, my reflexes kicked in. Get over the shock. Gloves, napkins, towels. Band-aid? How does one heal a bird? I rummaged through the house, keeping a wary eye on my cat. Donning yellow rubber gloves, I tentatively picked up the bird. Never mind the cat's hissing and protesting scratches, you need to save the bird. You need to ease its pain. But my mind was blank. I stroked the bird with a paper towel to clear away the blood, see the wound. The wings were crumpled, the feet mangled. A large gash extended close to its jugular rendering its breathing shallow, unsteady. The rising and falling of its small breast slowed. Was the bird dying? No, please, not yet. Why was this feeling so familiar, so tangible? Oh. Yes. The long drive, the green hills, the white church, the funeral. The Chinese mass, the resounding amens, the flower arrangements. Me, crying silently, huddled in the corner. The Hsieh family huddled around the casket. Apologies. So many apologies. Finally, the body lowered to rest. The body. Kari Hsieh. Still familiar, still tangible. Hugging Mrs. Hsieh, I was a ghost, a statue. My brain and my body competed. Emotion wrestled with fact. Kari Hsieh, aged 17, my friend of four years, had died in the Chatsworth Metrolink Crash on Sep. 12, 2008. Kari was dead, I thought. Dead. But I could still save the bird. My frantic actions heightened my senses, mobilized my spirit. Cupping the bird, I ran outside, hoping the cool air outdoors would suture every wound, cause the bird to miraculously fly away. Yet there lay the bird in my hands, still gasping, still dying. Bird, human, human, bird. What was the difference? Both were the same. Mortal. But couldn't I do something? Hold the bird longer, de-claw the cat? I wanted to go to my bedroom, confine myself to tears, replay my memories, never come out. The bird's warmth faded away. Its heartbeat slowed along with its breath. For a long time, I stared thoughtlessly at it, so still in my hands. Slowly, I dug a small hole in the black earth. As it disappeared under handfuls of dirt, my own heart grew stronger, my own breath more steady. The wind, the sky, the dampness of the soil on my hands whispered to me, \u201cThe bird is dead. Kari has passed. But you are alive.\u201d My breath, my heartbeat, my sweat sighed back, \u201cI am alive. I am alive. I am alive.\u201d", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0065", "text": "For one thing it is what I've always wanted to do since I was a little kid and I've spend each hour of mine getting prepared to be one. I've learned to communicate with my team members and find out what they are best at, I've learn to cherish my mistakes and try to do better in the next time. I think I really like it. And plus, I am a sociable person, I've found myself quite good at dealing with all kinds of relationships, which clearly is a good quality for being a leader.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0076", "text": "My goal is to become an engineer someday. Being an engineer is something I've always been looking forward to since I was a little kid. It will allow me to bring out my creativity in my daily work, that's just like dreams coming true. And I'd like it if I can build something that would last much longer even after I am gone, and seeing people benefit from my design will definitely give me great satisfaction. So going to engineering school really is a brand new start of my life. That's why it's so important to me.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0061", "text": "Living in a big city can provide people with a range of benefits. Now, I am studying in Guangzhou, a big and modern city in my country. My life is totally different from the past. Now I can receive better education, experience new things, get more job opportunities and so on. I feel energetic and motivated everyday!Besides, the basic infrastructure in big city is perfect, because there are some museums, hospitals, schools, libraries and so on", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0078", "text": "My favorite room in my house is my study room. There are various books in my study room, such as fashion magazines, inspirational books and so on. Also there is a computer in my study room. The reasons why it is my favorite room is because in my spare time I usually like to surf the internet to pay close attention to new fashion trade or find some valuable books to recharge myself. These books can make me healthier and energetic.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0072", "text": "I believe being open-minded is what it takes to make a good friend. Most of us are brought up with a set of believes and values, so sometimes it can be tough to accept new things. But a friend with an open mind will introduce countless possibilities into your life and he or she can offer you the chance to change how you view the world. And also, there's an honesty that comes with an open mind, being open-minded means admitting that you do not know everything. It's always comfortable to be around friends like that, and they will help you explore more about the world.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0122", "text": "Title: Combining QANet and Retro-Reader Models\nAbstract: Our task is to design a machine reading comprehension (MRC) model that can\naccurately solve question answering problems from the Stanford Question Answering Dataset (SQuAD). For our model, we aimed to 1) implement the QANet\nmodel,  which is one of the highest performing non-pretrained models, and 2)\nextend QANet with a verification module inspired by Zhang et al. (2020) to better\nidentify unanswerable questions and improve performance on SQuAD 2.0.  We\nexplored variants on both the QANet architecture as well as the Retro-Reader\nArchitecture experimenting with different values for hyperparameters and our best single model achieved an F1/EM score of 66.10/62.28 on the development set and 64.422/60.659 on the test set. We explored a variant on the Retro Reader architecture that involved training one model to always predict an answer and training a separate model that does all the answerability prediction. Despite not significantly improving the performance of the model, through our error analysis, we gained deep insights into what components degraded model performance and developed potential hypotheses for future improvements. In particular when testing the Retro QANet model, we discovered that the Intensive QANet model was prone to false negatives and false positives thus we hypothesize that the main shortcoming of our model is its reading comprehension ability. Overall, we explored the application of retro reader and verification techniques to one of the highest performing non-PCE models and experimented with parameters and the architecture.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0046", "text": "When I was in college, I loved to study until midnight and then go back to my dorm to sleep. The campus was so peaceful at that time, no one was on the street, and all lights were off, it felt that the whole campus belonged to me alone. I always go through a park in the middle of the campus, moonlight slanted through the branches down to the ground, birds stopped tweeting and the occasional chirping of crickets was the only thing you could hear. You would doubt if time had stopped running if not for the cool breeze caressing every inch of your skin. (There were always faint fragrance from flowers you can't name, but it could certainly remind you of the sweetest peaches in summer. Oh how I love the school.)", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0049", "text": "I think that governmental regulations have the biggest impact on environmental protection. Compared to individual efforts, governmental regulations make citizens follow the law, rather than make good choices. For example, in Korea, if you don\u2019t recycle properly, you can receive a fine. They provide all the bins and resources to make it easy for the public to make the right choice to recycle. Second, when the government steps in to make regulations, it also educates the public, which can lead to individuals going above and beyond for the environment. When the government introduces a new regulation, it will explain why they are doing it, which may have people think twice about before they throw garbage out the window of the car. For these two reasons, I believe that governmental regulations have the biggest impact on environmental protection.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0003", "text": "Title: RobustQA\nAbstract: In recent years, question-answering (QA) models have vastly improved and achieved superhuman standards in several benchmarks. Yet, these same superhuman models often do not perform well on out-of-distribution (OOD) datasets or tasks. In contrast, humans appear to easily and quickly generalize to new unseen domains. In this project, we aim to train a QA model that is able to perform well across different datasets, especially on OOD datasets. Specifically, we experiment with the use of adversarial training applied to a pretrained DistiIBERT model. The adversarial training takes the form of a critic model that tries to classify the origin domain of the QA embedding. In addition to the regular QA loss, the QA model has the additional objective of fooling the critic model. This encourages the QA model to learn a domain-agnostic embedding, which we hope to help with generalization and robustness to OOD datasets.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0001", "text": "Title: Building a QA system (IID SQuAD track)\nAbstract: Question Answering is a interesting machine learning task which shows how machine can understand the relationship and the meaning of the words. There are lots of existing models built to solve this task. This paper draws inspiration from the paper Bidirectional Attention Flow for Machine Comprehension and dive deeper into the effect of character level embedding on the performance of the model. Through experimenting on different CNN model for character level embedding, we have concluded that a more complex CNN model does not result in a better performance metrics. However, through manually evaluate the model's prediction, we have found that a more complex model does perform better in certain cases.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0008", "text": "I am studying in a top university, which provides me with a platform to gain and enrich knowledge for my major. My major is business administration. In university, I have learned some related academic subjects, such as economics, management, e-commerce and so on. At the same time, I can do some internships in some business companies.All these can lay a solid foundation for my future career.What\u2019s more, in university, I can make friends with some talented people who can give me a lot of inspiration.", "source": "TOEFL11", "is_esl": true}
{"id": "college_0023", "text": "Gazing up at the starry sky, I see Cygnus, Hercules, and Pisces, remnants of past cultures. I listen to waves crash on the beach, the forces of nature at work. Isn\u2019t it odd how stars are flaming spheres and electrical impulses make beings sentient? The very existence of our world is a wonder; what are the odds that this particular planet developed all the necessary components, parts that all work in unison, to support life? How do they interact? How did they come to be? I thought back to how my previously simplistic mind-set evolved this past year. The very existence of our world is a wonder; what are the odds that this particular planet developed all the necessary components, parts that all work in unison, to support life? At Balboa, juniors and seniors join one of five small learning communities, which are integrated into the curriculum. Near the end of sophomore year, I ranked my choices: Law Academy first\u2014it seemed the most prestigious\u2014and WALC, the Wilderness Arts and Literacy Collaborative, fourth. So when I was sorted into WALC, I felt disappointed at the inflexibility of my schedule and bitter toward my classes. However, since students are required to wait at least a semester before switching pathways, I stayed in WALC. My experiences that semester began shifting my ambition-oriented paradigm to an interest-oriented one. I didn\u2019t switch out. Beyond its integrated classes, WALC takes its students on trips to natural areas not only to build community among its students, but also to explore complex natural processes and humanity\u2019s role in them. Piecing these lessons together, I create an image of our universe. I can visualize the carving of glacial valleys, the creation and gradation of mountains by uplift and weathering, and the transportation of nutrients to and from ecosystems by rivers and salmon. I see these forces on the surface of a tiny planet rotating on its axis and orbiting the sun, a gem in this vast universe. Through WALC, I have gained an intimate understanding of natural systems and an addiction to understanding the deep interconnections embedded in our cosmos. Understanding a system\u2019s complex mechanics not only satisfies my curiosity, but also adds beauty to my world; my understanding of tectonic and gradational forces allows me to appreciate mountains and coastlines beyond aesthetics. By physically going to the place described in WALC\u2019s lessons, I have not only gained the tools to admire these systems, but have also learned to actually appreciate them. This creates a thirst to see more beauty in a world that\u2019s filled with poverty and violence, and a hunger for knowledge to satisfy that thirst. There are so many different systems to examine and dissect\u2014science alone has universal, planetary, molecular, atomic, and subatomic scales to investigate. I hope to be able to find my interests by taking a variety of courses in college, and further humanity\u2019s understanding through research, so that all can derive a deeper appreciation for the complex systems that govern this universe.", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0062", "text": "Personally, I would like to say that I like magazine most. There are a couple of reasons to name. The first reason I wanna say is there are a lot of funny stories in the magazine, so I can learn a lot of jokes and share them with my friends, which can improve my interpersonal and communication skills. The second reason is I can make friends with those who have the same love for magazines, and we can have a get-together.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0033", "text": "I believe that it is beneficial to evaluate professors at the end of the semester, but there can also be some disadvantages. First, since superior staff can\u2019t sit it on every class, they should take input from the students on how the class went. Students are likely to be honest in their reviews, so the professor can also learn from their mistakes. However, I believe that there are disadvantages as well. If a student knows that they will receive a failing grade, they may be dishonest in their review just to spite the professor. They may say that the professor wasn\u2019t fair, even if that isn\u2019t true. This can be hurtful to the professor and detrimental to their career. So, I believe that there are advantages and disadvantages to asking students to evaluate their professors at the end of the semester.", "source": "TOEFL11", "is_esl": true}
{"id": "college_0065", "text": "\u201cHow prone to doubt, how cautious are the wise!\u201d -Homer \u201cD\u2019oh!\u201d -Homer Simpson I\u2019m not a philosopher; eloquence eludes me, the meaning of life is unquestioned, and thinking, beyond what is required to carry out a potential, is postponed to a more leisurely time. I\u2019ve experienced doubt, and proceeded with caution; and in my experience, I\u2019ve learned to discard unnecessary thought and conventional wisdom in favor of progress. Philosophy amounts to nothing unless it results in action. \u201cYou\u2019re kidding.\u201d Scanning my schedule, my classmate shakes her head. \u201cWhy didn\u2019t you take Dual Credit?\u201d During Junior year, my high school began to incentivize Dual Credit courses with a GPA multiplier. Advertised to be less demanding than an AP class, Dual Credit was extolled as the wise man\u2019s curriculum. So, mustering all the wisdom I had, I took 6 AP classes, and frankly, I enjoyed their depth. When it comes to education, I\u2019m not cautious \u2013 and I\u2019m prone to doubt. I just act. If I want chemistry, then I get chemistry; if I\u2019m intrigued by psychology, then I pursue psychology. There is no point in pondering the inevitable; I am determined to take educational opportunities. I\u2019ll judge the difficulty for myself after I complete it. The practice of prioritizing action has proved useful in my pursuits. In ninth grade, I could have doubted my capability; instead I ran for office in the school\u2019s health club and earned a position in the eleventh grade. That year, there was a debate amongst the members over meeting schedules: if the Technology Students Association meeting coincided with ours, how would we attract new members? As the club officers weighed the costs and benefits amongst themselves, I left the meeting and signed up for the technology club, discussed an agreement, and voted for the technology club to move its meetings to the second half of lunch before scheduling the Health club meetings for the first half. Did it require thinking? No. Eloquence? Hardly. Contrary to the anticipated speeches and club-based patriotism, it only took clear action and a request to solve the conflict. Attendance increased, and as a bonus, I enjoyed a continued membership with both organizations. Beyond the sphere of public education, doubt-free determination facilitated my impact in the community. I am seventeen; I cannot vote in the upcoming elections. However, that does not mean I will hesitate to make a mark with my city. Small actions, from teaching addition to a church member\u2019s kindergartener to tutoring three classmates for the SAT, matter in the long run. Can a teenage end world hunger? Doubtful; but by pulling weeds from the community garden, I can further progress one step at a time. Not all actions end successfully. However, between cautious wisdom and failure, I choose action. I don\u2019t fancy myself as wise; I\u2019m not prone to doubt, nor am I perpetually cautious. I simply pursue my goal. As the wiser Homer has taught America, when torn between success and potential peril, one must simply \u201cD\u2019oh.\u201d", "source": "CollegeEssay", "is_esl": false}
{"id": "toefl_0019", "text": "The most important invention in my life is definitely the computer. I spend almost half of my waking hours on it. Definitely the computer\uff0cthrough the computer I can communicate with the world using the internet. And it helps me to become a more successful person. When I was selecting which university and which major to take after I graduated from the high school, the internet gave me a lot of useful information about the future of some of my prospective professions. I even talked with several people in those particular professions and got their opinions about it. And I think it is really helpful.", "source": "TOEFL11", "is_esl": true}
{"id": "toefl_0086", "text": "There's no doubt that I will choose to be an engineer. Being an engineer is something I've always been looking forward to since I was a little kid. It will allow me to bring out my creativity in my daily work, that's just like dreams coming true. And I'd like it if I can build something that would last much longer even after I am gone, and seeing people benefit from my design will definitely give me great satisfaction. So going to engineering school really is a brand new start of my life. That's why it's so important to me.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0026", "text": "Title: BiDAF with Dependency Parse Tree for Question Answering in SQUAD 2\nAbstract: One of the key areas of interest in Natural Language Processing is building systems capable of answering questions in our native language. The task is called Question Answering (QA) and is the focus of this paper where we explore our idea to enhance an existing solution called BiDAF (Seo et al, 2016). Our intuition is that language understanding involves at least two broad capabilities. First one has to understand what words individually mean. And second, based on the structure of the sentences one has to make sense of the complete sentence. Individual word are usually represented by word embeddings in most solutions. But the second piece is where different approaches diverge greatly. To address this part, we were interested  to see, if syntactic information  can help. Specifically, we explored the idea of using dependency parse trees (DPT) to enrich the embedding of individual words. DPT provides a representation of syntactic relationships between words in a sentence. We defined the relationship between words as the path between them in the dependency tree. We hypothesized that even though grammatical structure doesn't enable a system to do a lot of things such as reasoning, the best a model could do with a limited dataset is to learn the patterns between syntax of questions with that of the answer phrases. This inspired us to augment the input word embeddings to the model with dependency parse tree based information. Our model  not only scored significantly higher (+7% on F1 & EM) compared to the baseline, it also learned almost twice as fast even with the extra preprocessing time. DPTs are produced by deep learning model, so end to end there is in no manual feature engineering. We find this idea particularly interesting  as it could be potentially added to other QA models with minimal adaptation.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0105", "text": "Title: Building a QA system (IID SQuAD track)\nAbstract: In order to improve our baseline model, we have experimented many approaches and methods. We have started by adding a \"Character Embedding Layer\", which allows us to condition on the internal morphology of words and better handle out-of-vocabulary words. Then we have focused on improving our attention layer by trying different approaches.\nWe developed a \"Co-Attention Flow Layer\", which involves a second-level attention computation, attending over representations that are themselves attention outputs. Furthermore, we added a \"Self-Matching-Attention\" from the R-Net consisting on extracting evidence from the whole passage according to the current passage word and question information. Besides, we experimented an idea from the \"QANet\" by adapting ideas from the Transformer and applying them to question answering, doing away with RNNs and replacing them entirely with self-attention and convolution. Then, we tried a new idea consisting on adding another BiDAF layer, this layer accounts not only for the interactions between the context and question and for the ones within the context. We wanted some-how to account also for the Context-to-Context interaction, this is will provide valuable information about the co-dependence between different words in the context.\nTo put this idea into practice we have added another BiDAF layer performing a self-attention process like the one between the context and the query. The input to this layer will be the representation we get from the first BiDAF attention layer and the words context representations we get from the first encoder. The output of this layer will successfully account not only for the interactions between the context and question and for the ones within the context. This is the model that provided the highest score. We have also being experimenting with additional gates and nonlinearities applied to the summary vector after the attention step. These gates and nonlinearities enable the model to focus on important parts of the attention vector for each word.\nOur devised model \"Double BiDAF\" achieved the best score of 63.03 on the validation set. This is exceptional because we have only made a small change to the model architecture and it yielded such improvement.", "source": "CS224N", "is_esl": false}
{"id": "cs224n_0071", "text": "Title: Improved QA systems for SQUAD 2.0\nAbstract: We worked on the default project: Building a question-answering system (IID SQuAD track). Motivated by recent publications (such as \"Attention is All You Need,\"\" \"Machine Comprehension Using Match-LSTM and Answer Pointer,\" and \"Convolutional Neural Networks for Sentence Classification\"), we decided to extend the baseline BiDAF model with implementations of a character embedding layer, an answer pointer decoder in place of the original output layer, and a self-attention layer immediately after the bidirectional attention flow layer. We experimented with two versions of character embedding layers, and found that back-to-back convolutional layers allowed for better performances. Our implementations dramatically improved learning speed in the training process. Through multiple rounds of training with various hyperparameters, we achieved F1 scores of 64.83 on the dev set and 63.37 on the test set. We anticipate that this work will aid in the continuing development of efficient question answering systems.", "source": "CS224N", "is_esl": false}
{"id": "toefl_0030", "text": "If I had a small amount of money, I would save it. The first reason is I grew up in a family that prided themselves on having money in the bank, so it is easy for me to save money. The second reason is if there was an emergency and you would need some money. If you saved it, you would be ok. However, if you did not save it, you could be in trouble. In addition, it is more important to save money and pay off your debts, rather than waste your money on something useless. So for these two reasons, I would save a small amount of money.", "source": "TOEFL11", "is_esl": true}
{"id": "cs224n_0048", "text": "Title: Bidirectional Attention Flow with Self-Attention\nAbstract: I extended the BiDAF model with varies optimization techniques on the SQuAD 2.0 dataset. With character embedding and multi head self attention been added to the model, my results shows an improvement of +4 point on the EM and +4 point on F1 score compared with the default project. The performance is as expected, but there are also rooms for improvements. One notable finding is I could also generate a masking for each word while training to force the attention computation not focus on the current word but other words of the given inputs.Right after the completion of the project report, i have noticed that other findings reported that a pure  Self-Attention is not that helpful without the bias and rank collapse. It seems a pure self attention layer can be converted into a shallow network", "source": "CS224N", "is_esl": false}
{"id": "college_0031", "text": "\u201cBring the ace of spades up,\u201d my Grandmother said as we started our first game of solitaire after I got home from school. \u201cNow, put the black eight onto the red nine.\u201d We played solitaire often, working together to reorganize the cards most efficiently. While it was meant to be a single-player game, solitaire was the one thing we did together, moving and dealing the cards in a symphony of order: red to black, red to black. Pulling the pattern out of the random array of cards. For hours, we sat at our glossy kitchen table, playing game after game. If there were no more moves to make, I would always sneak a card from below a column without my grandma seeing. She always did. I couldn\u2019t understand- What was the big deal of revealing the cards? We might win one out of ten games played. But if we just \u2018helped ourselves,\u2019 as I liked to call it, we could win them all. I didn\u2019t understand her adherence to the \u201cTurn Three\u201d rule. Why not just turn the cards one by one? It was too frustrating to see the cards go by, but turn exactly three and not be able to pick them up! After one game we lost, I asked my grandma, \u201cWhy do we play this way? There\u2019s a much better way to play.\u201d In response, she quickly explained her adamancy to the rules, what before had made no sense to me. Her polished fingernails scratched against the cards as she shuffled them and told me. \u201cSolitaire isn\u2019t just a game for one person.\u201d Her deep brown eyes sharply glanced at me, \u201cNo.\u201d It wasn\u2019t just a game for one person, but rather for two sides of a person. It was an internal battle, a strengthening of the mind. One playing against oneself. \u201cIf one side of you cheats, how would either side get better?\u201d Red lipsticked lips slightly grinned as my grandma saw me trying to understand, but I didn\u2019t agree with this thought at once. The cards rhythmically slapped down onto the table as my grandmother, small yet stoic, effortlessly moved the cards with frail hands. I watched her. I thought about any other way to understand this idea. I desperately wanted to. Trying to think, I couldn\u2019t imagine another instance where this sense of tranquility, bringing the melody of organization out of a cacophony of random cards, came from such intense competition. The slow manipulation of life around her precedent made me think back to my grandma, to what she told me, and made me understand. Two years later, pushing myself harder than I ever had before in a field hockey match, I realized how much I had been cheating myself and my team by not putting this effort in before. Four years later, I was helping my parents clean after dinner when I saw the value in not taking the easy way out. Five years later, I found once again the difficult ease in pottery. Lifting the pot off the wheel, I found satisfaction. Looking back, I hadn\u2019t realized that this notion of self-accountability appears in almost every aspect of my life. Seven columns. Four aces. Fifty-two cards. Laying these down, I\u2019m brought back to playing solitaire with my grandmother. Through time, her inner spirit never crumbled as her body began to deteriorate. Her mind stayed strong and proud. I admired her for that more than she could\u2019ve imagined. Each challenge I face, or will face, in life, I think back to her lesson one inconspicuous afternoon. Never let myself cheat. Always hold myself accountable. Work hard in every competition, especially the ones against myself, as those are the ones that better me the most. I did not understand what my grandmother meant that day. Now, with each day, I do more.", "source": "CollegeEssay", "is_esl": false}
{"id": "college_0003", "text": "These days, birds are losing the battle of favored domestic animal to dogs and cats. At best, they're an easily forgotten blot in the otherwise clear sky, and at worst, they're nasty pests associated with filth and disease. But for many years, birds were something much greater, the catalyst of folklore and tales for nearly every culture around the world.         We've all heard some iteration of a bird story before: Common characters you might recall include the wise owl, mischievous raven, vain peacock, and motherly hen. I was introduced to these stories early on, first captivated by the avian parables I listened to on CDs, and they became an integral part of my early years. I can still remember proudly reciting \"The Ant and the Magpie\" word for word to my parents, an important tale reminding listeners to save resources for a time in need, represented by the winter in the animal world. As I got older, my love for birds persisted, but the influence those childlike stories had on me waned. After all, none of my classmates proclaimed their love of dogs stemmed from a Danish fairytale or Chinese folklore. I figured the reason I loved birds was shallower: I enjoyed the startling, colorful plumage and the joyous calls I heard outside my window. No longer were birds a central part of my identity; instead, they became an answer when I had to state my favorite animal during a summer camp icebreaker.        It wasn't until I was well into high school, nearly a decade after I last closed the cover, that I found one of my favorite childhood books, \"Why Snails Have Shells,\" in the depths of my closet. Rediscovering this book reminded me of the importance I placed on the lessons I learned from the cherished bird characters. Leafing through the pages and rereading the familiar stories, I realized the straightforward teachings of the birds were more relevant to my current life than they ever were in my childhood. Birds once again were not simply my favorite animal, they guided the way I reacted in challenging situations, which - like for most of my peers - came in a barrage as I got older.        The lesson that permeates my life today is from an old Chinese proverb, famously summed up by poet Maya Angelou as \"A bird doesn't sing because it has an answer, it sings because it has a song.\" High school life, especially for my generation, is hyper-focused on the approval of others. Instagram is littered with polls asking if outfits are \"ok,\" popularity is measured by the average number of comments you get in response to your posts, and every joke uttered is followed by a scan of the room to make sure at least someone is laughing. Contrastingly, the bird doesn't focus on the answer it receives from its song; in fact, it doesn't even expect an answer. The bird sings because it wishes to, because of the joy it experiences when doing so.        It can be easy to get swept away in the desire to please, but the personal mantra I've adopted reminds me of the importance of doing things for the sake of making yourself happy, not others. I build relationships I genuinely value, I invest my time in activities I love to do, and I express myself in ways that bring me joy. Although the stories and proverbs I learned when I was younger originated from distant times and places, they have woven themselves into my values and shaped me into the person I am today.", "source": "CollegeEssay", "is_esl": false}
{"id": "cs224n_0106", "text": "Title: Robust QA with Task-Adaptive Pretraining\nAbstract: It is often hard to find a lot of labeled data to train a QA (question answering) model.\nOne possible approach to overcome this challenge is to use TAPT (task-adaptive\npretraining) in which the model is pretrained further using the unlabeled data from\nthe task itself. We implement the TAPT technique to make a QA model perform\nrobustly on a task with low-resource training data by first pertaining on the larger unlabeled data set. We then fine tune the model with a smaller labeled dataset. The results are mixed. Although a preliminary model that is pretrained on just the  out-of-domain train data performed better than the baseline, additional pretraining using more out-of-domain data performed worse than expected.", "source": "CS224N", "is_esl": false}

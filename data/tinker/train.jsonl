{"ai_text": "The ground beneath me began to shake as an oil truck instantly burst into flames. A massive ball of fire flared into the sky, illuminating my awestruck eyes. Suddenly, hundreds of gallons of water rushed down onto the truck, safely extinguishing the blaze. \u201cCUT!\u201d a director yelled. I cheered, astonished by the scene I had just witnessed.        My love for Hollywood began with moments like these from my childhood. Disney\u2019s Hollywood Studios was home to attractions like The Great Movie Ride and The Studio Backlot Tour, both of which introduced me to the special effects, intricate illusions, and thrilling stunts seen in professional films. These two attractions were early indicators of my love for filmmaking, I just didn\u2019t know it yet.        Years later, I am still captivated by the magic of cinema. Whether it be a summer blockbuster, an Oscar-hopeful, or a cult classic, I\u2019ll take any opportunity I can get to experience an original film. For a few hours, I can forget about the world around me, becoming completely immersed in the universe on-screen. Characters come alive, their personalities and stories intertwining themselves with real-life experiences of my own. I\u2019ve always been what you would call a \u201ctomboy\u201d, a far-from-fragile girl who loves football and loathes dresses. Having strong female characters like Hermione Granger and Princess Leia to look up to on-screen has had a profound impact on my confidence as a young woman. Seeing another woman hold her ground and stand up for herself was truly inspiring to me. I may not wield a wand or a blaster, but I\u2019ve certainly used the strength of these characters as a personal inspiration to stay confident and secure in myself.        My passion for film does not end with characterization. I am just as invested in the technical, behind-the-scenes aspects of cinema. Cinematographers bring stunning landscapes and perfectly-framed shots to life, invoking awe and emotion in both casual moviegoers and film fanatics. Lighting designers shape a film\u2019s mood and tone, adding flares of emotion and rich symbolism to climatic scenes.        I still have so much to learn about filmmaking, and I cannot wait to tackle the challenges that come with producing a film. When I do, I know that I\u2019ll put my heart into it. Maybe my protagonist will defy the stereotypes that surround young women, choosing jeans over skirts and football over dance. Maybe she\u2019ll love brisk autumn mornings, and never understand the appeal of hot, sticky, summer afternoons. Maybe she\u2019ll discover her peculiar affinity for both science and cinema. Whichever direction I decide to take my characters and my story, my life experiences will have a huge impact on the final product. This is yet another thing that I love about movies; they are entirely unique to the individual who creates them. No two people could create the same exact film no matter how hard they tried \u2014 there\u2019s always a little bit of a director\u2019s soul woven into their work.        I\u2019m still unsure whether I\u2019ll follow my passion for film into a full-time career or a part-time hobby. If I decide to pursue filmmaking, I hope to use my platform to spread a message of hope, perseverance, and strength. Films can reach millions, possibly even billions of people, giving me the perfect opportunity to make a profound impact on someone\u2019s life. If just one person can be inspired by one of my characters, much like I was by Hermione and Leia, I\u2019ll be satisfied. Even if I never sell out theaters or break a box office record, I will have achieved success if I can make someone\u2019s life just a little bit better through my work. Through filmmaking, I hope to invoke the same sense of wonder and awe that I once felt as I experienced the magic of cinema for the very first time.", "human_reference": "The ground beneath me began to shake as an oil truck instantly burst into flames. A massive ball of fire flared into the sky, illuminating my awestruck eyes. Suddenly, hundreds of gallons of water rushed down onto the truck, safely extinguishing the blaze. \u201cCUT!\u201d a director yelled. I cheered, astonished by the scene I had just witnessed.        My love for Hollywood began with moments like these from my childhood. Disney\u2019s Hollywood Studios was home to attractions like The Great Movie Ride and The Studio Backlot Tour, both of which introduced me to the special effects, intricate illusions, and thrilling stunts seen in professional films. These two attractions were early indicators of my love for filmmaking, I just didn\u2019t know it yet.        Years later, I am still captivated by the magic of cinema. Whether it be a summer blockbuster, an Oscar-hopeful, or a cult classic, I\u2019ll take any opportunity I can get to experience an original film. For a few hours, I can forget about the world around me, becoming completely immersed in the universe on-screen. Characters come alive, their personalities and stories intertwining themselves with real-life experiences of my own. I\u2019ve always been what you would call a \u201ctomboy\u201d, a far-from-fragile girl who loves football and loathes dresses. Having strong female characters like Hermione Granger and Princess Leia to look up to on-screen has had a profound impact on my confidence as a young woman. Seeing another woman hold her ground and stand up for herself was truly inspiring to me. I may not wield a wand or a blaster, but I\u2019ve certainly used the strength of these characters as a personal inspiration to stay confident and secure in myself.        My passion for film does not end with characterization. I am just as invested in the technical, behind-the-scenes aspects of cinema. Cinematographers bring stunning landscapes and perfectly-framed shots to life, invoking awe and emotion in both casual moviegoers and film fanatics. Lighting designers shape a film\u2019s mood and tone, adding flares of emotion and rich symbolism to climatic scenes.        I still have so much to learn about filmmaking, and I cannot wait to tackle the challenges that come with producing a film. When I do, I know that I\u2019ll put my heart into it. Maybe my protagonist will defy the stereotypes that surround young women, choosing jeans over skirts and football over dance. Maybe she\u2019ll love brisk autumn mornings, and never understand the appeal of hot, sticky, summer afternoons. Maybe she\u2019ll discover her peculiar affinity for both science and cinema. Whichever direction I decide to take my characters and my story, my life experiences will have a huge impact on the final product. This is yet another thing that I love about movies; they are entirely unique to the individual who creates them. No two people could create the same exact film no matter how hard they tried \u2014 there\u2019s always a little bit of a director\u2019s soul woven into their work.        I\u2019m still unsure whether I\u2019ll follow my passion for film into a full-time career or a part-time hobby. If I decide to pursue filmmaking, I hope to use my platform to spread a message of hope, perseverance, and strength. Films can reach millions, possibly even billions of people, giving me the perfect opportunity to make a profound impact on someone\u2019s life. If just one person can be inspired by one of my characters, much like I was by Hermione and Leia, I\u2019ll be satisfied. Even if I never sell out theaters or break a box office record, I will have achieved success if I can make someone\u2019s life just a little bit better through my work. Through filmmaking, I hope to invoke the same sense of wonder and awe that I once felt as I experienced the magic of cinema for the very first time.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0002", "source": "CollegeEssay"}}
{"ai_text": "My first dream job was to be a pickle truck driver. I saw it in my favorite book, Richard Scarry's \"Cars and Trucks and Things That Go,\" and for some reason, I was absolutely obsessed with the idea of driving a giant pickle. Much to the discontent of my younger sister, I insisted that my parents read us that book as many nights as possible so we could find goldbug, a small little golden bug, on every page. I would imagine the wonderful life I would have: being a pig driving a giant pickle truck across the country, chasing and finding goldbug. I then moved on to wanting to be a Lego Master. Then an architect. Then a surgeon. Then I discovered a real goldbug: gold nanoparticles that can reprogram macrophages to assist in killing tumors,produce clear images of them without sacrificing the subject, and heat them to obliteration. Suddenly the destination of my pickle was clear. I quickly became enveloped by the world of nanomedicine; I scoured articles about liposomes, polymeric micelles, dendrimers, targeting ligands, and self-assembling nanoparticles, all conquering cancer in some exotic way. Completely absorbed, I set out to find a mentor to dive even deeper into these topics. After several rejections, I was immensely grateful to receive an invitation to work alongside Dr. Sangeeta Ray at Johns Hopkins. In the lab, Dr. Ray encouraged a great amount of autonomy to design and implement my own procedures. I chose to attack a problem that affects the entire field of nanomedicine: nanoparticles consistently fail to translate from animal studies into clinical trials. Jumping off recent literature, I set out to see if a pre-dose of a common chemotherapeutic could enhance nanoparticle delivery in aggressive prostate cancer, creating three novel constructs based on three different linear polymers, each using fluorescent dye (although no gold, sorry goldbug!). Though using radioactive isotopes like Gallium and Yttrium would have been incredible, as a 17-year-old, I unfortunately wasn't allowed in the same room as these radioactive materials (even though I took a Geiger counter to a pair of shoes and found them to be slightly dangerous). I hadn't expected my hypothesis to work, as the research project would have ideally been led across two full years. Yet while there are still many optimizations and revisions to be done, I was thrilled to find -- with completely new nanoparticles that may one day mean future trials will use particles with the initials \"RK-1\" -- thatcyclophosphamide did indeed increase nanoparticle delivery to the tumor in a statistically significant way. A secondary, unexpected research project was living alone in Baltimore, a new city to me, surrounded by people much older than I. Even with moving frequently between hotels, AirBnB's, and students' apartments, I strangely reveled in the freedom I had to enjoy my surroundings and form new friendships with graduate school students from the lab. We explored The Inner Harbor at night, attended a concert together one weekend, and even got to watch the Orioles lose (to nobody's surprise). Ironically, it's through these new friendships I discovered something unexpected: what I truly love is sharing research. Whether in a presentation or in a casual conversation, making others interested in science is perhaps more exciting to me than the research itself. This solidified a new pursuit to angle my love for writing towards illuminating science in ways people can understand, adding value to a society that can certainly benefit from more scientific literacy. It seems fitting that my goals are still transforming: in Scarry's book, there is not just one goldbug, there is one on every page. With each new experience, I'm learning that it isn't the goldbug itself, but rather the act of searching for the goldbugs that will encourage, shape, and refine my ever-evolving passions. Regardless of the goldbug I seek -- I know my pickle truck has just begun its journey.", "human_reference": "My first dream job was to be a pickle truck driver. I saw it in my favorite book, Richard Scarry's \"Cars and Trucks and Things That Go,\" and for some reason, I was absolutely obsessed with the idea of driving a giant pickle. Much to the discontent of my younger sister, I insisted that my parents read us that book as many nights as possible so we could find goldbug, a small little golden bug, on every page. I would imagine the wonderful life I would have: being a pig driving a giant pickle truck across the country, chasing and finding goldbug. I then moved on to wanting to be a Lego Master. Then an architect. Then a surgeon. Then I discovered a real goldbug: gold nanoparticles that can reprogram macrophages to assist in killing tumors,produce clear images of them without sacrificing the subject, and heat them to obliteration. Suddenly the destination of my pickle was clear. I quickly became enveloped by the world of nanomedicine; I scoured articles about liposomes, polymeric micelles, dendrimers, targeting ligands, and self-assembling nanoparticles, all conquering cancer in some exotic way. Completely absorbed, I set out to find a mentor to dive even deeper into these topics. After several rejections, I was immensely grateful to receive an invitation to work alongside Dr. Sangeeta Ray at Johns Hopkins. In the lab, Dr. Ray encouraged a great amount of autonomy to design and implement my own procedures. I chose to attack a problem that affects the entire field of nanomedicine: nanoparticles consistently fail to translate from animal studies into clinical trials. Jumping off recent literature, I set out to see if a pre-dose of a common chemotherapeutic could enhance nanoparticle delivery in aggressive prostate cancer, creating three novel constructs based on three different linear polymers, each using fluorescent dye (although no gold, sorry goldbug!). Though using radioactive isotopes like Gallium and Yttrium would have been incredible, as a 17-year-old, I unfortunately wasn't allowed in the same room as these radioactive materials (even though I took a Geiger counter to a pair of shoes and found them to be slightly dangerous). I hadn't expected my hypothesis to work, as the research project would have ideally been led across two full years. Yet while there are still many optimizations and revisions to be done, I was thrilled to find -- with completely new nanoparticles that may one day mean future trials will use particles with the initials \"RK-1\" -- thatcyclophosphamide did indeed increase nanoparticle delivery to the tumor in a statistically significant way. A secondary, unexpected research project was living alone in Baltimore, a new city to me, surrounded by people much older than I. Even with moving frequently between hotels, AirBnB's, and students' apartments, I strangely reveled in the freedom I had to enjoy my surroundings and form new friendships with graduate school students from the lab. We explored The Inner Harbor at night, attended a concert together one weekend, and even got to watch the Orioles lose (to nobody's surprise). Ironically, it's through these new friendships I discovered something unexpected: what I truly love is sharing research. Whether in a presentation or in a casual conversation, making others interested in science is perhaps more exciting to me than the research itself. This solidified a new pursuit to angle my love for writing towards illuminating science in ways people can understand, adding value to a society that can certainly benefit from more scientific literacy. It seems fitting that my goals are still transforming: in Scarry's book, there is not just one goldbug, there is one on every page. With each new experience, I'm learning that it isn't the goldbug itself, but rather the act of searching for the goldbugs that will encourage, shape, and refine my ever-evolving passions. Regardless of the goldbug I seek -- I know my pickle truck has just begun its journey.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0041", "source": "CollegeEssay"}}
{"ai_text": "The one that I personally admire the most is a character named Alan Shore in a TV series called Boston Legal. Maybe you have heard about it. This character has changed my definition of what a perfect man is and what characteristics one should hopefully possess. He is decent, a man of his word, one of the very few that I regard as having a strong sense of justice. Yet he is not bound up by the rules and knows when to break them to achieve the ultimate good. And he is interesting and eloquent, all the things that I desire to have.", "human_reference": "The one that I personally admire the most is a character named Alan Shore in a TV series called Boston Legal. Maybe you have heard about it. This character has changed my definition of what a perfect man is and what characteristics one should hopefully possess. He is decent, a man of his word, one of the very few that I regard as having a strong sense of justice. Yet he is not bound up by the rules and knows when to break them to achieve the ultimate good. And he is interesting and eloquent, all the things that I desire to have.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0087", "source": "TOEFL11"}}
{"ai_text": "\u201cHow many times did I wake up at 4:15 a.m. this summer?\u201d I found myself once again asking this question as I climbed endless stone steps with bruised shins and dirt-filled fingernails. The answer: twenty-two times. I was in a rush to finish the 48th peak before school began in order to fulfill a goal I set in fifth grade after meeting a wild pack of Appalachian Trail through-hikers. I marveled at their determination. Climbing all 48 four thousand foot peaks within New Hampshire is an ambitious goal that takes some people a lifetime to finish. There I was, at 6:15 a.m., gasping for air and wondering who I should blame for the pain. Maybe I had my parents to blame for my drive to be in the wilderness. They exposed me to the outdoors at a young age, sparking my passion for hiking and backpacking. Having lived in China for four and a half years and traveling the world, I always knew my childhood was unique. Unlike other expatriates, my family dismissed four-star resorts and instead chose to stumble through the alleyways of Hong Kong with an array of camping supplies. As a six-year-old, I was fortunate enough to find myself in Italy running from a wild herd of cattle in the Alps. During our summers in Oregon, instead of renting a car, we pedaled through the hilly streets on a three-person bike. These experiences, that made my family different, instilled in me a sense of adventure. The 48 strenuous climbs and endless miles also brought beautiful vistas. If we were lucky, we got to end the day at a high mountain hut where we drank endless cups of rich hot chocolate. I would sit in the corner of the dining room engrossed in books about rare lichen. At Mizpah hut, I had the chance to talk with a female naturalist about some of the endangered alpine flora. I sat and stared in awe. I didn't know that someone could have a job doing field studies in the mountains. I\u2019ve spent the last six years looking at the sides of the trails for the dwarf Cinquefoil she introduced to me. That\u2019s when I knew I wanted to become a hands-on environmentalist so I could spend more time doing the things I love. Maybe I have the naturalist to blame for all the blisters and early mornings on the trail.        Mount Isolation was my last peak. One last push. Number 48. 13.6 miles. After the first grueling thirty minutes, the path opened up and I could see all the way to the Atlantic Ocean. This is the way it always goes. First, the struggle, and then the reward. Mt. Washington glowed like amber. The wind nipped at my fingertips and shook the crooked trees. My heavy breathing competed with the sounds of the white-throated sparrows. I had the entire mountain to myself. Overwhelmed by emotion, I began to cry bittersweet tears. No more waking up at 4:15 a.m. but then again, no more celebratory Cokes at the top. I was done. I decided to let go of the blame for all the early mornings. Instead, I would love to give my fifth grade-self a big \u201cthank you\u201d.        The struggles only augmented the joy I felt on the car ride home with music playing and my feet wiggling in the wind. I felt that I had graduated from my childhood. Hiking over the past seventeen years with my family has created endless memories, yet it's time for me to start a new chapter of my life. Maybe I\u2019ll hike the Adirondack 46ers, explore sections of the Appalachian Trail, or guide others through the wilderness. But I know I will always continue to look around and search for rare specimens and marvel at the ordinary.", "human_reference": "\u201cHow many times did I wake up at 4:15 a.m. this summer?\u201d I found myself once again asking this question as I climbed endless stone steps with bruised shins and dirt-filled fingernails. The answer: twenty-two times. I was in a rush to finish the 48th peak before school began in order to fulfill a goal I set in fifth grade after meeting a wild pack of Appalachian Trail through-hikers. I marveled at their determination. Climbing all 48 four thousand foot peaks within New Hampshire is an ambitious goal that takes some people a lifetime to finish. There I was, at 6:15 a.m., gasping for air and wondering who I should blame for the pain. Maybe I had my parents to blame for my drive to be in the wilderness. They exposed me to the outdoors at a young age, sparking my passion for hiking and backpacking. Having lived in China for four and a half years and traveling the world, I always knew my childhood was unique. Unlike other expatriates, my family dismissed four-star resorts and instead chose to stumble through the alleyways of Hong Kong with an array of camping supplies. As a six-year-old, I was fortunate enough to find myself in Italy running from a wild herd of cattle in the Alps. During our summers in Oregon, instead of renting a car, we pedaled through the hilly streets on a three-person bike. These experiences, that made my family different, instilled in me a sense of adventure. The 48 strenuous climbs and endless miles also brought beautiful vistas. If we were lucky, we got to end the day at a high mountain hut where we drank endless cups of rich hot chocolate. I would sit in the corner of the dining room engrossed in books about rare lichen. At Mizpah hut, I had the chance to talk with a female naturalist about some of the endangered alpine flora. I sat and stared in awe. I didn't know that someone could have a job doing field studies in the mountains. I\u2019ve spent the last six years looking at the sides of the trails for the dwarf Cinquefoil she introduced to me. That\u2019s when I knew I wanted to become a hands-on environmentalist so I could spend more time doing the things I love. Maybe I have the naturalist to blame for all the blisters and early mornings on the trail.        Mount Isolation was my last peak. One last push. Number 48. 13.6 miles. After the first grueling thirty minutes, the path opened up and I could see all the way to the Atlantic Ocean. This is the way it always goes. First, the struggle, and then the reward. Mt. Washington glowed like amber. The wind nipped at my fingertips and shook the crooked trees. My heavy breathing competed with the sounds of the white-throated sparrows. I had the entire mountain to myself. Overwhelmed by emotion, I began to cry bittersweet tears. No more waking up at 4:15 a.m. but then again, no more celebratory Cokes at the top. I was done. I decided to let go of the blame for all the early mornings. Instead, I would love to give my fifth grade-self a big \u201cthank you\u201d.        The struggles only augmented the joy I felt on the car ride home with music playing and my feet wiggling in the wind. I felt that I had graduated from my childhood. Hiking over the past seventeen years with my family has created endless memories, yet it's time for me to start a new chapter of my life. Maybe I\u2019ll hike the Adirondack 46ers, explore sections of the Appalachian Trail, or guide others through the wilderness. But I know I will always continue to look around and search for rare specimens and marvel at the ordinary.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0001", "source": "CollegeEssay"}}
{"ai_text": "When I was young, I lived in a suburban area. And there was a corn field right beside our community. One day me and some other kids decided to do something exciting, so we sneaked into the field. They made me stand watch for I was the slow one. After grabbing some corns we ran back home as fast as we could, not wanting to be caught. And then we found some bricks to build up a little stove, collected lots of dry leaves to make fire, and roasted the corns on fire. I have to say that that roast corn was the best that I've ever had.", "human_reference": "When I was young, I lived in a suburban area. And there was a corn field right beside our community. One day me and some other kids decided to do something exciting, so we sneaked into the field. They made me stand watch for I was the slow one. After grabbing some corns we ran back home as fast as we could, not wanting to be caught. And then we found some bricks to build up a little stove, collected lots of dry leaves to make fire, and roasted the corns on fire. I have to say that that roast corn was the best that I've ever had.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0050", "source": "TOEFL11"}}
{"ai_text": "Title: Reformed QANet - Optimizing the Spatial Complexity of QANet\nAbstract: The feed-forward QANet architecture replaced the bidirectional LSTMs of traditional question and answering models by using encoder components with convolution + self-attention to increase the speed of the model without sacrificing accuracy. We achieved scores of 64.5 EM/67.9 F1 on the dev set and 61.64 EM/65.30 F1 on the test set. While the parallel nature of QANet's CNN architecture allows for a significant speed boost, it means that minimizing GPU memory usage is crucial to attain these benefits. In this report we perform an exhaustive study investigating changes to spatial complexity, speed, and performance on the QANet architecture by replacing components in the encoder block with memory-efficient alternatives such as LSH Self Attention, reversible residual networks, and reformer blocks. The image above depicts the QANet encoder block where the self-attention and feed-forward layer are replaced with a reformer, a stack of reversible LSH Self Attention and feed-forward layers. We found that implementing LSH attention successfully decreased memory usage on long sequences while maintaining reasonable performance. While the other modifications did not quite maintain the original QANet model's EM and F1 scores, they significantly decreased GPU memory usage. Additionally, we used data augmentation to enrich training data through back translation and found slight improvements on our larger model.", "human_reference": "Title: Reformed QANet - Optimizing the Spatial Complexity of QANet\nAbstract: The feed-forward QANet architecture replaced the bidirectional LSTMs of traditional question and answering models by using encoder components with convolution + self-attention to increase the speed of the model without sacrificing accuracy. We achieved scores of 64.5 EM/67.9 F1 on the dev set and 61.64 EM/65.30 F1 on the test set. While the parallel nature of QANet's CNN architecture allows for a significant speed boost, it means that minimizing GPU memory usage is crucial to attain these benefits. In this report we perform an exhaustive study investigating changes to spatial complexity, speed, and performance on the QANet architecture by replacing components in the encoder block with memory-efficient alternatives such as LSH Self Attention, reversible residual networks, and reformer blocks. The image above depicts the QANet encoder block where the self-attention and feed-forward layer are replaced with a reformer, a stack of reversible LSH Self Attention and feed-forward layers. We found that implementing LSH attention successfully decreased memory usage on long sequences while maintaining reasonable performance. While the other modifications did not quite maintain the original QANet model's EM and F1 scores, they significantly decreased GPU memory usage. Additionally, we used data augmentation to enrich training data through back translation and found slight improvements on our larger model.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0017", "source": "CS224N"}}
{"ai_text": "Definitely by phone calls. First of all, letter and emails takes too much of time. You have to organize your thoughts and write them down, and it's likely that you will miss something while writing, and it would be days before people can get your letter. Plus, writing a letter is like telling your feelings to a piece of paper, it doesn't feel real. But using phone calls is totally different, the distance doesn't seem to matter anymore when you hear the voice of your loved ones. And you can do it any time you want, it's much more convenient.", "human_reference": "Definitely by phone calls. First of all, letter and emails takes too much of time. You have to organize your thoughts and write them down, and it's likely that you will miss something while writing, and it would be days before people can get your letter. Plus, writing a letter is like telling your feelings to a piece of paper, it doesn't feel real. But using phone calls is totally different, the distance doesn't seem to matter anymore when you hear the voice of your loved ones. And you can do it any time you want, it's much more convenient.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0025", "source": "TOEFL11"}}
{"ai_text": "Title: Improving Out-of-Domain Question Answering with Mixture of Experts\nAbstract: Question answering (QA) is an important problem with numerous applications in real life. Sometimes, the resource of certain QA tasks is limited. Our work aims to build a robust QA system that can generalize to novel QA tasks with few examples and gradient steps. We propose a Mixture-of-Experts (MoE) style training framework, where we learn a gating network to construct the embeddings by performing a weighted sum of the base \"expert\" models with fixed parameters. We find that using the mixture of expert models improves generalization performance and reduces overfitting, especially when using \"expert\" models trained with data augmentation. We use meta-learning methods, specifically the MAML algorithm, to train the gating network for domain adaptation. Training the gating network with the MAML algorithm and finetuning on out-of-domain tasks improved out-of-domain QA performance of baseline models on all metrics. We also discovered a correlation between expert-model performance and the weight the MoE framework puts on each of them. Our approach achieves a F-1 score of 60.8 and EM score of 42.2 on the out-of-domain QA testing leaderboard.", "human_reference": "Title: Improving Out-of-Domain Question Answering with Mixture of Experts\nAbstract: Question answering (QA) is an important problem with numerous applications in real life. Sometimes, the resource of certain QA tasks is limited. Our work aims to build a robust QA system that can generalize to novel QA tasks with few examples and gradient steps. We propose a Mixture-of-Experts (MoE) style training framework, where we learn a gating network to construct the embeddings by performing a weighted sum of the base \"expert\" models with fixed parameters. We find that using the mixture of expert models improves generalization performance and reduces overfitting, especially when using \"expert\" models trained with data augmentation. We use meta-learning methods, specifically the MAML algorithm, to train the gating network for domain adaptation. Training the gating network with the MAML algorithm and finetuning on out-of-domain tasks improved out-of-domain QA performance of baseline models on all metrics. We also discovered a correlation between expert-model performance and the weight the MoE framework puts on each of them. Our approach achieves a F-1 score of 60.8 and EM score of 42.2 on the out-of-domain QA testing leaderboard.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0032", "source": "CS224N"}}
{"ai_text": "I learned the definition of cancer at the age of fourteen. I was taking my chapter 7 biology test when I came upon the last question, \u201cWhat is cancer?\u201d, to which I answered: \u201cThe abnormal, unrestricted growth of cells.\u201d After handing in the test, I moved on to chapter 8, oblivious then to how earth-shattering such a disease could be. I learned the meaning of cancer two years later. A girl named Kiersten came into my family by way of my oldest brother who had fallen in love with her. I distinctly recall her hair catching the sea breeze as she walked with us along the Jersey shore, a blonde wave in my surrounding family's sea of brunette. Physically, she may have been different, but she redefined what family meant to me. She attended my concerts, went to my award ceremonies, and helped me study for tests. Whenever I needed support, she was there. Little did I know that our roles would be reversed, forever changing my outlook on life. Kiersten was diagnosed with Stage II Hodgkin's lymphoma at the age of 22. Tears and hair fell alike after each of her 20 rounds of chemotherapy as we feared the worst. It was an unbearable tragedy watching someone so vivacious skirt the line between life and death. Her cancer was later classified as refractory, or resistant to treatment. Frustration and despair flooded my mind as I heard this news. And so I prayed. In what universe did this dynamic make any sense? I prayed to God and to even her cancer itself to just leave her alone. Eventually, Kiersten was able to leave the hospital to stay for six weeks at my home. My family and I transformed the house into an antimicrobial sanctuary, protecting Kiersten from any outside illness. I watched TV with her, baked cookies for her, and observed her persistence as she regained strength and achieved remission. We beat biology, time, and death, all at the same time, with cookies, TV, and friendship. Yet I was so concerned with helping Kiersten that I had not realized how she helped me during her battle with cancer. I had been so used to solving my problems intellectually that when it came time to emotionally support someone, I was afraid. I could define cancer, but what do I say to someone with it? There were days where I did not think I could be optimistic in the face of such adversity. But the beauty that resulted from sympathizing as opposed to analyzing and putting aside my own worries and troubles for someone else was an enormous epiphany for me. My problems dissipated into thin air the moment I came home and dropped my books and bags to talk with Kiersten. The more I talked, laughed, smiled, and shared memories with her, the more I began to realize all that she taught me. She influenced me in the fact that she demonstrated the power of loyalty, companionship, and optimism in the face of desperate, life-threatening situations. She showed me the importance of loving to live and living to love. Most of all, she gave me the insight necessary to fully help others not just with intellect and preparation, but with solidarity and compassion. In this way, I became able to help myself and others with not only my brain, but with my heart. And that, in the words of Robert Frost, \u201chas made all the difference.\u201d", "human_reference": "I learned the definition of cancer at the age of fourteen. I was taking my chapter 7 biology test when I came upon the last question, \u201cWhat is cancer?\u201d, to which I answered: \u201cThe abnormal, unrestricted growth of cells.\u201d After handing in the test, I moved on to chapter 8, oblivious then to how earth-shattering such a disease could be. I learned the meaning of cancer two years later. A girl named Kiersten came into my family by way of my oldest brother who had fallen in love with her. I distinctly recall her hair catching the sea breeze as she walked with us along the Jersey shore, a blonde wave in my surrounding family's sea of brunette. Physically, she may have been different, but she redefined what family meant to me. She attended my concerts, went to my award ceremonies, and helped me study for tests. Whenever I needed support, she was there. Little did I know that our roles would be reversed, forever changing my outlook on life. Kiersten was diagnosed with Stage II Hodgkin's lymphoma at the age of 22. Tears and hair fell alike after each of her 20 rounds of chemotherapy as we feared the worst. It was an unbearable tragedy watching someone so vivacious skirt the line between life and death. Her cancer was later classified as refractory, or resistant to treatment. Frustration and despair flooded my mind as I heard this news. And so I prayed. In what universe did this dynamic make any sense? I prayed to God and to even her cancer itself to just leave her alone. Eventually, Kiersten was able to leave the hospital to stay for six weeks at my home. My family and I transformed the house into an antimicrobial sanctuary, protecting Kiersten from any outside illness. I watched TV with her, baked cookies for her, and observed her persistence as she regained strength and achieved remission. We beat biology, time, and death, all at the same time, with cookies, TV, and friendship. Yet I was so concerned with helping Kiersten that I had not realized how she helped me during her battle with cancer. I had been so used to solving my problems intellectually that when it came time to emotionally support someone, I was afraid. I could define cancer, but what do I say to someone with it? There were days where I did not think I could be optimistic in the face of such adversity. But the beauty that resulted from sympathizing as opposed to analyzing and putting aside my own worries and troubles for someone else was an enormous epiphany for me. My problems dissipated into thin air the moment I came home and dropped my books and bags to talk with Kiersten. The more I talked, laughed, smiled, and shared memories with her, the more I began to realize all that she taught me. She influenced me in the fact that she demonstrated the power of loyalty, companionship, and optimism in the face of desperate, life-threatening situations. She showed me the importance of loving to live and living to love. Most of all, she gave me the insight necessary to fully help others not just with intellect and preparation, but with solidarity and compassion. In this way, I became able to help myself and others with not only my brain, but with my heart. And that, in the words of Robert Frost, \u201chas made all the difference.\u201d", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0049", "source": "CollegeEssay"}}
{"ai_text": "I\u2019m 6. The sounds of hornpipe and laughter drift across the gymnasium-turned-cafeteria-turned-auditorium. Mum caught me dancing to some of her old Irish tapes \u2014 the Chieftains, Sinead O\u2019Connor. She asked me if I wanted to do it for real. I said sure and went back to dancing. Now a freckled woman digs around in a cardboard box and pulls out a pair of dusty, worn black shoes. \u201cDon\u2019t worry,\u201d she says, \u201cyou\u2019ll learn eventually.\u201d The shoes are too big; they sag at the toes. I approach the stage. Twenty-five pairs of eyes fix on me. In a room bustling with motion, everything stands still. It doesn\u2019t matter that I feel like a clown in an ill-fitting costume. All that matters is the dancing. I\u2019m 9. I sit in the hallway of the Times Square Marriott watching girls in big wigs and sparkly dresses run around, squawking like glamorous, unhinged chickens. In my tartan skirt and simple bun, I feel like an ugly duckling. The bobby pins dutifully securing my bun in place make my scalp ache. My hands slide to my shoes. They\u2019re too tight. Mum put them on her feet to \u201ctry and stretch them out a little.\u201d I pass some over-enthusiastic dance moms who put the \u201cmother\u201d in \u201csmother.\u201d I reach the stage. A hundred pairs of eyes fix on me. In a hotel bustling with motion, everything stands still. It doesn\u2019t matter that I\u2019m out of place. All that matters is the dancing. I\u2019m 12. My brain won\u2019t stop flipping through disastrous scenarios as I stand with my teammates in a hotel in Orlando, Florida. We\u2019ve trained for months, sacrificed everything for this moment. I try to think of happy things: the pride on Dad\u2019s face when he watches me dance, the freedom of flying across a stage on invisible wings. We recite our steps like a poem, the sequences like a song that carries us through an ocean of fiddles, pipes, and drums. My parents sacrificed a lot to send me here. I want to make them proud. I want to make myself proud. We approach the national stage. A thousand pairs of eyes fix on me. In a world bustling with motion, everything stands still. It doesn\u2019t matter that I feel like a fraud. All that matters is the dancing. I\u2019m 15. An Irish accent lilts through the ballroom of the World Championships. It sounds like mashed potatoes and Sunday bests and the green hills of home that I know so well. We mutter a prayer. I\u2019m not sure I believe in God, though I should. I look at my partner and wish we were more than friends. She smiles. I don\u2019t think God believes in me. We ascend the stage. A million pairs of eyes fix on me. In a universe bustling with motion, everything stands still. It doesn\u2019t matter that I\u2019ll never be enough. All that matters is the dancing. I\u2019ll be 18. Murmuring voices will hover in the air of the gymnasium-turned-cafeteria-turned-auditorium. A little girl will approach me timidly, wearing a very old tartan skirt. I\u2019ll reach out softly, adjusting her bun to soothe her aching scalp. Then, I\u2019ll slide my hands toward her feet, toward a pair of small, dusty shoes. \u201cYou\u2019ll learn,\u201d I\u2019ll say. They\u2019ll sag at the toes, but I\u2019ll reassure her: \u201cDon\u2019t worry. You\u2019ll grow into them.\u201d Then, she and I will look at my own beloved shoes. They\u2019ll be worn, but I\u2019ll tell her the creases are like a map, evidence of the places I\u2019ve been, the heartbreaks I\u2019ve suffered, the joy I\u2019ve danced. My life is in these shoes. We\u2019ll hear the music begin to play, the tide of fiddles, and pipes, and drums. I\u2019ll take her hand and, with a deep breath, we\u2019ll climb the stage. \u201cAhd mor.\u201d It won\u2019t matter that this is the end. All that has ever mattered is the dancing.", "human_reference": "I\u2019m 6. The sounds of hornpipe and laughter drift across the gymnasium-turned-cafeteria-turned-auditorium. Mum caught me dancing to some of her old Irish tapes \u2014 the Chieftains, Sinead O\u2019Connor. She asked me if I wanted to do it for real. I said sure and went back to dancing. Now a freckled woman digs around in a cardboard box and pulls out a pair of dusty, worn black shoes. \u201cDon\u2019t worry,\u201d she says, \u201cyou\u2019ll learn eventually.\u201d The shoes are too big; they sag at the toes. I approach the stage. Twenty-five pairs of eyes fix on me. In a room bustling with motion, everything stands still. It doesn\u2019t matter that I feel like a clown in an ill-fitting costume. All that matters is the dancing. I\u2019m 9. I sit in the hallway of the Times Square Marriott watching girls in big wigs and sparkly dresses run around, squawking like glamorous, unhinged chickens. In my tartan skirt and simple bun, I feel like an ugly duckling. The bobby pins dutifully securing my bun in place make my scalp ache. My hands slide to my shoes. They\u2019re too tight. Mum put them on her feet to \u201ctry and stretch them out a little.\u201d I pass some over-enthusiastic dance moms who put the \u201cmother\u201d in \u201csmother.\u201d I reach the stage. A hundred pairs of eyes fix on me. In a hotel bustling with motion, everything stands still. It doesn\u2019t matter that I\u2019m out of place. All that matters is the dancing. I\u2019m 12. My brain won\u2019t stop flipping through disastrous scenarios as I stand with my teammates in a hotel in Orlando, Florida. We\u2019ve trained for months, sacrificed everything for this moment. I try to think of happy things: the pride on Dad\u2019s face when he watches me dance, the freedom of flying across a stage on invisible wings. We recite our steps like a poem, the sequences like a song that carries us through an ocean of fiddles, pipes, and drums. My parents sacrificed a lot to send me here. I want to make them proud. I want to make myself proud. We approach the national stage. A thousand pairs of eyes fix on me. In a world bustling with motion, everything stands still. It doesn\u2019t matter that I feel like a fraud. All that matters is the dancing. I\u2019m 15. An Irish accent lilts through the ballroom of the World Championships. It sounds like mashed potatoes and Sunday bests and the green hills of home that I know so well. We mutter a prayer. I\u2019m not sure I believe in God, though I should. I look at my partner and wish we were more than friends. She smiles. I don\u2019t think God believes in me. We ascend the stage. A million pairs of eyes fix on me. In a universe bustling with motion, everything stands still. It doesn\u2019t matter that I\u2019ll never be enough. All that matters is the dancing. I\u2019ll be 18. Murmuring voices will hover in the air of the gymnasium-turned-cafeteria-turned-auditorium. A little girl will approach me timidly, wearing a very old tartan skirt. I\u2019ll reach out softly, adjusting her bun to soothe her aching scalp. Then, I\u2019ll slide my hands toward her feet, toward a pair of small, dusty shoes. \u201cYou\u2019ll learn,\u201d I\u2019ll say. They\u2019ll sag at the toes, but I\u2019ll reassure her: \u201cDon\u2019t worry. You\u2019ll grow into them.\u201d Then, she and I will look at my own beloved shoes. They\u2019ll be worn, but I\u2019ll tell her the creases are like a map, evidence of the places I\u2019ve been, the heartbreaks I\u2019ve suffered, the joy I\u2019ve danced. My life is in these shoes. We\u2019ll hear the music begin to play, the tide of fiddles, and pipes, and drums. I\u2019ll take her hand and, with a deep breath, we\u2019ll climb the stage. \u201cAhd mor.\u201d It won\u2019t matter that this is the end. All that has ever mattered is the dancing.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0016", "source": "CollegeEssay"}}
{"ai_text": "To me, I prefer eating at restaurants over eating at food stands for two main reasons (or I prefer to eat at restaurants for two main reasons). The first reason is I grew up dinning in restaurants, so I am accustomed to eating in them. Actually, I cannot remember the last time I ate at a food stand. Plus, eating in a nice building is much more comfortable compared to eating outside. The second reason is there is a better food selection. I can have more choices and my food is cooked better, because they have a proper cooking system. Therefore, these reasons are why I prefer to eat at a restaurant rather than a food stand.", "human_reference": "To me, I prefer eating at restaurants over eating at food stands for two main reasons (or I prefer to eat at restaurants for two main reasons). The first reason is I grew up dinning in restaurants, so I am accustomed to eating in them. Actually, I cannot remember the last time I ate at a food stand. Plus, eating in a nice building is much more comfortable compared to eating outside. The second reason is there is a better food selection. I can have more choices and my food is cooked better, because they have a proper cooking system. Therefore, these reasons are why I prefer to eat at a restaurant rather than a food stand.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0081", "source": "TOEFL11"}}
{"ai_text": "I'm afraid that I disagree. It's not like that I don't like a relaxing life style, it's just that there are so many goals to fulfill in my life. There are tons of books I plan to read, I want to buy a fancy RV for my parents, I want to travel around the world, to learn several new languages, but none of them will happen unless financial issues are no longer a problem for me. So I have to study and work hard to earn enough money for those things. I just can't afford a relaxing life.", "human_reference": "I'm afraid that I disagree. It's not like that I don't like a relaxing life style, it's just that there are so many goals to fulfill in my life. There are tons of books I plan to read, I want to buy a fancy RV for my parents, I want to travel around the world, to learn several new languages, but none of them will happen unless financial issues are no longer a problem for me. So I have to study and work hard to earn enough money for those things. I just can't afford a relaxing life.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0075", "source": "TOEFL11"}}
{"ai_text": "Title: Building a QA System using R-net\nAbstract: Question-answering task is an important problem for research in natural language processing, for which many deep learning models have been designed. Here we implement R-Net and evaluate its performance on SQuAD 2.0. While the performance of R-Net itself is worse than BiDAF, it showed a strong capability of its attention mechanism compared to BiDAF as shown in the image. We have also experimented with an ensemble model using BiDAF and R-Net that achieved better performance than the baseline BiDAF. Our study suggests that a promising future direction is to combine BiDAF and R-Net for building better models.", "human_reference": "Title: Building a QA System using R-net\nAbstract: Question-answering task is an important problem for research in natural language processing, for which many deep learning models have been designed. Here we implement R-Net and evaluate its performance on SQuAD 2.0. While the performance of R-Net itself is worse than BiDAF, it showed a strong capability of its attention mechanism compared to BiDAF as shown in the image. We have also experimented with an ensemble model using BiDAF and R-Net that achieved better performance than the baseline BiDAF. Our study suggests that a promising future direction is to combine BiDAF and R-Net for building better models.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0101", "source": "CS224N"}}
{"ai_text": "Title: Extending a BiDAF model with DCN for Question Answering\nAbstract: Our goal in this project is to improve the performance of the Bidirectional Attention Flow (BiDAF) model for the NLP task of question answering on the SQuAD 2.0 dataset. To do this, we 1) integrate character-level embeddings into the baseline BiDAF model and 2) replace the default attention layer with a coattention layer. While adding character-level embeddings has shown to improve the baseline BiDAF model's EM and F1 scores substantially, their addition to the DCN model actually decreased its scores slightly. Moreover, transforming the BiDAF model into a Dynamic Coattention Network (DCN) decreased the model's performance. Thus, the best model architecture we found is BiDAF with character-level embeddings. Future work includes tuning hyperparameters, experimenting with data processing techniques, adding optimizations like the Adam optimizer, and exploring different forms of attention.", "human_reference": "Title: Extending a BiDAF model with DCN for Question Answering\nAbstract: Our goal in this project is to improve the performance of the Bidirectional Attention Flow (BiDAF) model for the NLP task of question answering on the SQuAD 2.0 dataset. To do this, we 1) integrate character-level embeddings into the baseline BiDAF model and 2) replace the default attention layer with a coattention layer. While adding character-level embeddings has shown to improve the baseline BiDAF model's EM and F1 scores substantially, their addition to the DCN model actually decreased its scores slightly. Moreover, transforming the BiDAF model into a Dynamic Coattention Network (DCN) decreased the model's performance. Thus, the best model architecture we found is BiDAF with character-level embeddings. Future work includes tuning hyperparameters, experimenting with data processing techniques, adding optimizations like the Adam optimizer, and exploring different forms of attention.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0082", "source": "CS224N"}}
{"ai_text": "Title: Investigating the effectiveness of Transformers and Performers on SQuAD 2.0\nAbstract: In this project, I explored aspects of the Transformer architecture in the context of question answering on SQuAD 2.0, the Stanford Question Answering Dataset. I split this exploration into several phases, which built upon each other.\nIn Phase 1, I gained familiarity with the default baseline (based on BiDAF, a recurrent LSTM-based algorithm) by upgrading it to support character-level embeddings, in addition to the existing word-level embeddings.  This resulted in a 2-point performance increase on all scoring metrics.\nIn Phase 2, I incrementally refactored the baseline from BiDAF into QANet, a question answering architecture which is similar in structure but uses convolution and Transformers instead of recurrent neural networks. After hyperparameter tuning, I found this improved performance by an additional 3.5 points on all scoring metrics.\nIn Phase 3, I replaced the Transformer with an architectural variant, the Performer, which aims to solve the issue of quadratic scaling in vanilla Transformers'runtime and memory usage by using kernel methods to approximate the self-attention calculation. I found that this was effective within QANet, enabling linear scaling from hundreds to tens of thousands of tokens, with minimal impact to performance.\nIn Phase 4, I prepared to make use of this scale to support open-domain question answering. I wrote a TF-IDF based document retriever, which returned the most similar Wikipedia page to the current context passage. I found this to be reasonably effective in locating similar passages.\nFinally, in Phase 5, I fed this new input into QANet via a new, large Background input, which supplemented the existing Context and Question inputs. I upgraded QANet to support this by adding a Context-Background attention and a Query-Background attention layer to the current Context-Query attention layer. This appears to start training correctly, with training and validation loss both decreasing over time.", "human_reference": "Title: Investigating the effectiveness of Transformers and Performers on SQuAD 2.0\nAbstract: In this project, I explored aspects of the Transformer architecture in the context of question answering on SQuAD 2.0, the Stanford Question Answering Dataset. I split this exploration into several phases, which built upon each other.\nIn Phase 1, I gained familiarity with the default baseline (based on BiDAF, a recurrent LSTM-based algorithm) by upgrading it to support character-level embeddings, in addition to the existing word-level embeddings.  This resulted in a 2-point performance increase on all scoring metrics.\nIn Phase 2, I incrementally refactored the baseline from BiDAF into QANet, a question answering architecture which is similar in structure but uses convolution and Transformers instead of recurrent neural networks. After hyperparameter tuning, I found this improved performance by an additional 3.5 points on all scoring metrics.\nIn Phase 3, I replaced the Transformer with an architectural variant, the Performer, which aims to solve the issue of quadratic scaling in vanilla Transformers'runtime and memory usage by using kernel methods to approximate the self-attention calculation. I found that this was effective within QANet, enabling linear scaling from hundreds to tens of thousands of tokens, with minimal impact to performance.\nIn Phase 4, I prepared to make use of this scale to support open-domain question answering. I wrote a TF-IDF based document retriever, which returned the most similar Wikipedia page to the current context passage. I found this to be reasonably effective in locating similar passages.\nFinally, in Phase 5, I fed this new input into QANet via a new, large Background input, which supplemented the existing Context and Question inputs. I upgraded QANet to support this by adding a Context-Background attention and a Query-Background attention layer to the current Context-Query attention layer. This appears to start training correctly, with training and validation loss both decreasing over time.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0077", "source": "CS224N"}}
{"ai_text": "Title: Robust Question Answering with Task Adaptive Pretraining and Data Augmentation\nAbstract: Existing research suggests that task adaptive pretraining (TAPT) with data augmentation can enhance classification accuracy on a wide array of natural language processing (NLP) tasks. This project aims to evaluate whether TAPT improves performance on a robust question answering (QA) system. The baseline model, which finetunes DistilBERT on SQuAD, NewsQA, and Natural Questions datasets, achieves an EM score of 33.25 and F1 score of 48.43 when validated on the out-of-sample DuoRC, RACE, and RelationExtraction datasets. Applying TAPT to the out-of-domain unlabeled training datasets using masked language modeling (MLM) without data augmentation, we do not observe an increase in either metric of performance. However, not using TAPT, our model performance is enhanced when we use backtranslations to augment only a small portion of the training data for finetuning, achieving an EM of 36.91 and F1 score of 50.16 on the out of domain validation set. This model also achieves an EM of 41.628 and F1 of 58.91 on the out of domain test set. These results thus suggest that data augmentation alone, even to a highly limited extent, may account for the improvements in model performance.", "human_reference": "Title: Robust Question Answering with Task Adaptive Pretraining and Data Augmentation\nAbstract: Existing research suggests that task adaptive pretraining (TAPT) with data augmentation can enhance classification accuracy on a wide array of natural language processing (NLP) tasks. This project aims to evaluate whether TAPT improves performance on a robust question answering (QA) system. The baseline model, which finetunes DistilBERT on SQuAD, NewsQA, and Natural Questions datasets, achieves an EM score of 33.25 and F1 score of 48.43 when validated on the out-of-sample DuoRC, RACE, and RelationExtraction datasets. Applying TAPT to the out-of-domain unlabeled training datasets using masked language modeling (MLM) without data augmentation, we do not observe an increase in either metric of performance. However, not using TAPT, our model performance is enhanced when we use backtranslations to augment only a small portion of the training data for finetuning, achieving an EM of 36.91 and F1 score of 50.16 on the out of domain validation set. This model also achieves an EM of 41.628 and F1 of 58.91 on the out of domain test set. These results thus suggest that data augmentation alone, even to a highly limited extent, may account for the improvements in model performance.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0007", "source": "CS224N"}}
{"ai_text": "I want to read biography books, because although we'd love to say that the society is changing and developing, human nature never changes much. And the biography books help me understand the nature behind the more visible events. I may encounter the same problems in the future and I'd like to know how others deal with them. And everyone is trying to present a more glamorous version of the self and it creates lots of illusions. Biography is a kind of way to break down these illusions and offers the truth about humanity.", "human_reference": "I want to read biography books, because although we'd love to say that the society is changing and developing, human nature never changes much. And the biography books help me understand the nature behind the more visible events. I may encounter the same problems in the future and I'd like to know how others deal with them. And everyone is trying to present a more glamorous version of the self and it creates lots of illusions. Biography is a kind of way to break down these illusions and offers the truth about humanity.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0013", "source": "TOEFL11"}}
{"ai_text": "Novels is no doubt my favorite form of literature. I've always loved reading since I was a little kid, and like most of the kids, novels are the first books I started to read. They are fun, they are relatively easy to understand, and most importantly, I loved to adventure with the heroes in the novels. When I grow up, I start to seek the wisdom in those novels, to find out what the authors really want to say behind the words. I grow so emotional attached to the people in the stories. I began to share their joy and sorrow. And I've learned so much from them.", "human_reference": "Novels is no doubt my favorite form of literature. I've always loved reading since I was a little kid, and like most of the kids, novels are the first books I started to read. They are fun, they are relatively easy to understand, and most importantly, I loved to adventure with the heroes in the novels. When I grow up, I start to seek the wisdom in those novels, to find out what the authors really want to say behind the words. I grow so emotional attached to the people in the stories. I began to share their joy and sorrow. And I've learned so much from them.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0054", "source": "TOEFL11"}}
{"ai_text": "Title: Multi-Phase Adaptive Pretraining on DistilBERT for Compact Domain Adaptation\nAbstract: While modern natural language models such as transformers have made significant leaps in performance relative to their predecessors, the fact that they are so large usually means that they learn small correlations that do not improve the model's predictive power. As a result, such models fail to generalize to other data, thus hampering performance in real-world cases where data is not independently and identically distributed (IID). Luckily, the use of domain-adaptive pretraining (DAPT), which involves pretraining on unlabeled target domain data, and task-adaptive pretraining (TAPT), which entails pretraining on all of the unlabeled data of a given task, can dramatically improve performance on large models like RoBERTa when the original and target domain distributions have a small amount of overlap. Consistent with the Robust QA track of the default project, this report investigates and tests the hypothesis that TAPT in tandem with DAPT (also known as multi-phase adaptive pretraining, or MAPT) can improve performance on the target domain for smaller transformers like DistilBERT on the question answering task, especially in the presence of domain shift. The final results show that the use of TAPT can lead to a slight increase in Exact Match (EM) performance without DAPT. However, implementing DAPT, even with the use of word-substitution data augmentation, significantly degrades the performance of the model on the held-out target domain dataset.", "human_reference": "Title: Multi-Phase Adaptive Pretraining on DistilBERT for Compact Domain Adaptation\nAbstract: While modern natural language models such as transformers have made significant leaps in performance relative to their predecessors, the fact that they are so large usually means that they learn small correlations that do not improve the model's predictive power. As a result, such models fail to generalize to other data, thus hampering performance in real-world cases where data is not independently and identically distributed (IID). Luckily, the use of domain-adaptive pretraining (DAPT), which involves pretraining on unlabeled target domain data, and task-adaptive pretraining (TAPT), which entails pretraining on all of the unlabeled data of a given task, can dramatically improve performance on large models like RoBERTa when the original and target domain distributions have a small amount of overlap. Consistent with the Robust QA track of the default project, this report investigates and tests the hypothesis that TAPT in tandem with DAPT (also known as multi-phase adaptive pretraining, or MAPT) can improve performance on the target domain for smaller transformers like DistilBERT on the question answering task, especially in the presence of domain shift. The final results show that the use of TAPT can lead to a slight increase in Exact Match (EM) performance without DAPT. However, implementing DAPT, even with the use of word-substitution data augmentation, significantly degrades the performance of the model on the held-out target domain dataset.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0140", "source": "CS224N"}}
{"ai_text": "Title: Comparing Mixture of Experts and Domain Adversarial Training with Data Augmentation in Out-of-Domain Question Answering\nAbstract: Generalization is a major challenge across machine learning; Question Answering in Natural Language Processing is no different. Models often fail on data domains in which they were not trained. In this project, we compare two promising, though opposite, solutions to this problem: ensembling specialized models (a Mixture of Experts approach) and penalizing specialization (Domain Adversarial Training). We also study the supplementary effects of data augmentation. Our work suggests that Domain Adversarial Training is a more effective method at generalization in our setup. We submit our results to the class leaderboard where we place 20th in EM.", "human_reference": "Title: Comparing Mixture of Experts and Domain Adversarial Training with Data Augmentation in Out-of-Domain Question Answering\nAbstract: Generalization is a major challenge across machine learning; Question Answering in Natural Language Processing is no different. Models often fail on data domains in which they were not trained. In this project, we compare two promising, though opposite, solutions to this problem: ensembling specialized models (a Mixture of Experts approach) and penalizing specialization (Domain Adversarial Training). We also study the supplementary effects of data augmentation. Our work suggests that Domain Adversarial Training is a more effective method at generalization in our setup. We submit our results to the class leaderboard where we place 20th in EM.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0043", "source": "CS224N"}}
{"ai_text": "Title: Comparing Model Size and Attention Layer Design Impact on Question-Answer Tasks\nAbstract: In this project, we explore the use of various Neural Language Models applied to Question Answer tasks from the SQuAD dataset. We're specifically interested in exploring the transition from RNN-based models to transformer-based models. RNN Neural Language Models were dominant in language tasks for many years, but the introduction of the transformer demonstrated that the fall-backs of RNN models could be overcome by using architectures that optimize for larger, more parallelizable models. In this work, we compare the impacts of expanding model size with the impact of changing attention layer implementations using a Bi-Directional Attention Flow baseline model. We find that model size has a significantly greater impact on model performance on the SQuAD dataset, but larger models fail to improve performance on unanswerable question-answer examples.", "human_reference": "Title: Comparing Model Size and Attention Layer Design Impact on Question-Answer Tasks\nAbstract: In this project, we explore the use of various Neural Language Models applied to Question Answer tasks from the SQuAD dataset. We're specifically interested in exploring the transition from RNN-based models to transformer-based models. RNN Neural Language Models were dominant in language tasks for many years, but the introduction of the transformer demonstrated that the fall-backs of RNN models could be overcome by using architectures that optimize for larger, more parallelizable models. In this work, we compare the impacts of expanding model size with the impact of changing attention layer implementations using a Bi-Directional Attention Flow baseline model. We find that model size has a significantly greater impact on model performance on the SQuAD dataset, but larger models fail to improve performance on unanswerable question-answer examples.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0125", "source": "CS224N"}}
{"ai_text": "I do agree that a higher education means a better career. To begin with, with the advancement of technology and the advancement of the world itself, companies are becoming more complex. If they want their company to succeed, they will need to have intelligent, knowledgeable, and competent individuals working for them. In addition, having a higher education will give you an advantage over the other candidates applying for the same job. Therefore, these reasons are why I agree that a higher education means a better career.", "human_reference": "I do agree that a higher education means a better career. To begin with, with the advancement of technology and the advancement of the world itself, companies are becoming more complex. If they want their company to succeed, they will need to have intelligent, knowledgeable, and competent individuals working for them. In addition, having a higher education will give you an advantage over the other candidates applying for the same job. Therefore, these reasons are why I agree that a higher education means a better career.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0022", "source": "TOEFL11"}}
{"ai_text": "Title: Building a Robust QA System\nAbstract: The robustness to domain shifts is very important for NLP, as in real world, test data are rarely IID with training data. This NLP task is to explore a Question Answering system that is robust to unseen domains with few training samples. In this task, three out-of-domain datasets show very different characteristics and they are trained with different in-domain datasets which are more beneficial for their challenges. Multiple transfer learning models are mixed in different ways: mixture of logits, mixture with custom output, and mixture with more features. Three majority vote strategies were taken to ensemble the models.", "human_reference": "Title: Building a Robust QA System\nAbstract: The robustness to domain shifts is very important for NLP, as in real world, test data are rarely IID with training data. This NLP task is to explore a Question Answering system that is robust to unseen domains with few training samples. In this task, three out-of-domain datasets show very different characteristics and they are trained with different in-domain datasets which are more beneficial for their challenges. Multiple transfer learning models are mixed in different ways: mixture of logits, mixture with custom output, and mixture with more features. Three majority vote strategies were taken to ensemble the models.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0135", "source": "CS224N"}}
{"ai_text": "Every Saturday morning, I\u2019d awaken to the smell of crushed garlic and piquant pepper. I would stumble into the kitchen to find my grandma squatting over a large silver bowl, mixing fat lips of fresh cabbages with garlic, salt, and red pepper. That was how the delectable Korean dish, kimchi, was born every weekend at my home. My grandma\u2019s specialty always dominated the dinner table as kimchi filled every plate. And like my grandma who had always been living with us, it seemed as though the luscious smell of garlic would never leave our home. But even the prided recipe was defenseless against the ravages of Alzheimer\u2019s that inflicted my grandma\u2019s mind. Dementia slowly fed on her memories until she became as blank as a brand-new notebook. The ritualistic rigor of Saturday mornings came to a pause, and during dinner, the artificial taste of vacuum-packaged factory kimchi only emphasized the absence of the family tradition. I would look at her and ask, \u201cGrandma, what\u2019s my name?\u201d But she would stare back at me with a clueless expression. Within a year of diagnosis, she lived with us like a total stranger. One day, my mom brought home fresh cabbages and red pepper sauce. She brought out the old silver bowl and poured out the cabbages, smothering them with garlic and salt and pepper. The familiar tangy smell tingled my nose. Gingerly, my grandma stood up from the couch in the living room, and as if lured by the smell, sat by the silver bowl and dug her hands into the spiced cabbages. As her bony hands shredded the green lips, a look of determination grew on her face. Though her withered hands no longer displayed the swiftness and precision they once did, her face showed the aged rigor of a professional. For the first time in years, the smell of garlic filled the air and the rattling of the silver bowl resonated throughout the house. That night, we ate kimchi. It wasn\u2019t perfect; the cabbages were clumsily cut and the garlic was a little too strong. But kimchi had never tasted better. I still remember my grandma putting a piece in my mouth and saying, \u201cHere, Dong Jin. Try it, my boy.\u201d Seeing grandma again this summer, that moment of clarity seemed ephemeral. Her disheveled hair and expressionless face told of the aggressive development of her illness. But holding her hands, looking into her eyes, I could still smell that garlic. The moments of Saturday mornings remain ingrained in my mind. Grandma was an artist who painted the cabbages with strokes of red pepper. Like the sweet taste of kimchi, I hope to capture those memories in my keystrokes as I type away these words. A piece of writing is more than just a piece of writing. It evokes. It inspires. It captures what time takes away. My grandma used to say: \u201cTigers leave furs when they die, humans leave their names.\u201d Her legacy was the smell of garlic that lingered around my house. Mine will be these words.", "human_reference": "Every Saturday morning, I\u2019d awaken to the smell of crushed garlic and piquant pepper. I would stumble into the kitchen to find my grandma squatting over a large silver bowl, mixing fat lips of fresh cabbages with garlic, salt, and red pepper. That was how the delectable Korean dish, kimchi, was born every weekend at my home. My grandma\u2019s specialty always dominated the dinner table as kimchi filled every plate. And like my grandma who had always been living with us, it seemed as though the luscious smell of garlic would never leave our home. But even the prided recipe was defenseless against the ravages of Alzheimer\u2019s that inflicted my grandma\u2019s mind. Dementia slowly fed on her memories until she became as blank as a brand-new notebook. The ritualistic rigor of Saturday mornings came to a pause, and during dinner, the artificial taste of vacuum-packaged factory kimchi only emphasized the absence of the family tradition. I would look at her and ask, \u201cGrandma, what\u2019s my name?\u201d But she would stare back at me with a clueless expression. Within a year of diagnosis, she lived with us like a total stranger. One day, my mom brought home fresh cabbages and red pepper sauce. She brought out the old silver bowl and poured out the cabbages, smothering them with garlic and salt and pepper. The familiar tangy smell tingled my nose. Gingerly, my grandma stood up from the couch in the living room, and as if lured by the smell, sat by the silver bowl and dug her hands into the spiced cabbages. As her bony hands shredded the green lips, a look of determination grew on her face. Though her withered hands no longer displayed the swiftness and precision they once did, her face showed the aged rigor of a professional. For the first time in years, the smell of garlic filled the air and the rattling of the silver bowl resonated throughout the house. That night, we ate kimchi. It wasn\u2019t perfect; the cabbages were clumsily cut and the garlic was a little too strong. But kimchi had never tasted better. I still remember my grandma putting a piece in my mouth and saying, \u201cHere, Dong Jin. Try it, my boy.\u201d Seeing grandma again this summer, that moment of clarity seemed ephemeral. Her disheveled hair and expressionless face told of the aggressive development of her illness. But holding her hands, looking into her eyes, I could still smell that garlic. The moments of Saturday mornings remain ingrained in my mind. Grandma was an artist who painted the cabbages with strokes of red pepper. Like the sweet taste of kimchi, I hope to capture those memories in my keystrokes as I type away these words. A piece of writing is more than just a piece of writing. It evokes. It inspires. It captures what time takes away. My grandma used to say: \u201cTigers leave furs when they die, humans leave their names.\u201d Her legacy was the smell of garlic that lingered around my house. Mine will be these words.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0051", "source": "CollegeEssay"}}
{"ai_text": "Title: Improving Domain Generalization for Question Answering\nAbstract: Domain generalization remains a major challenge for NLP systems. Our goal in this project is to build a question answering system that can adapt to new domains with very few training data from the target domain. We conduct experiments on three different techniques: 1) data augmentation, 2) task-adaptive pretraining (TAPT), and 3) multi-task finetuning to tackle the problem of producing a QA system that is robust to out-of-domain samples.  We found that simply augmenting the in-domain (ID) and out-of-domain (OOD) training samples available to us, specifically using insertions, substitutions, swaps and back-translations, boosted our model performance with just the baseline model architecture significantly. Further pretraining using the masked LM objective on the few OOD training samples also proved to be helpful for improving generalization. We also explored various model architectures in the realm of multi-task learning and found that jointly optimizing the QA loss with MLM loss allowed the model to generalize on the OOD samples significantly, confirming existing literature surrounding multi-task learning. Hoping that these gains from data augmentation, adaptive pretraining, and multi-task learning would be additive, we tried combining the techniques but found that the sum of the techniques performed only slightly better and sometimes worse than the smaller underlying systems alone. Our best model implements data augmentation on both ID and OOD train datasets with the DistilBERT base model and achieved EM/F1 scores of 35.34/51.58 on the OOD dev set and 42.32/60.17 on the held-out test set. We infer that we've comfortably met our goal of beating the baseline model's performance as the baseline model achieved 32.98/48.14 on the OOD dev set.", "human_reference": "Title: Improving Domain Generalization for Question Answering\nAbstract: Domain generalization remains a major challenge for NLP systems. Our goal in this project is to build a question answering system that can adapt to new domains with very few training data from the target domain. We conduct experiments on three different techniques: 1) data augmentation, 2) task-adaptive pretraining (TAPT), and 3) multi-task finetuning to tackle the problem of producing a QA system that is robust to out-of-domain samples.  We found that simply augmenting the in-domain (ID) and out-of-domain (OOD) training samples available to us, specifically using insertions, substitutions, swaps and back-translations, boosted our model performance with just the baseline model architecture significantly. Further pretraining using the masked LM objective on the few OOD training samples also proved to be helpful for improving generalization. We also explored various model architectures in the realm of multi-task learning and found that jointly optimizing the QA loss with MLM loss allowed the model to generalize on the OOD samples significantly, confirming existing literature surrounding multi-task learning. Hoping that these gains from data augmentation, adaptive pretraining, and multi-task learning would be additive, we tried combining the techniques but found that the sum of the techniques performed only slightly better and sometimes worse than the smaller underlying systems alone. Our best model implements data augmentation on both ID and OOD train datasets with the DistilBERT base model and achieved EM/F1 scores of 35.34/51.58 on the OOD dev set and 42.32/60.17 on the held-out test set. We infer that we've comfortably met our goal of beating the baseline model's performance as the baseline model achieved 32.98/48.14 on the OOD dev set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0040", "source": "CS224N"}}
{"ai_text": "Title: Improving Robustness of Question-Answering System Using Domain-adaptive Pretraining, Adversarial Training, Data Augmentation and Finetuning\nAbstract: From previous work, we know that Question-Answering (QA) system based on neural language models (NLM) is highly sensitive to the knowledge domain of training data and often has inferior performance when used for out-of-domain QA tasks. In this project, the authors attempt to combine a few published methods to improve the robustness of the QA system on out-of-domain data. We have tried methods including domain adversarial training, domain adaptive pretraining, finetuning on few samples, and data augmentation. We applied these methods through experimentation, improving the robustness of our baseline model on out-of-domain test datasets given two groups of training datasets: three large in-domain datasets and three very small out-of-domain datasets. We experimented and evaluated the effects of the above-mentioned methods both individually and combined, and found that while the individual method generates mixed results, the combination of them can improve the robustness of the baseline model in the QA task to the greatest extent on the out-of-domain datasets. We have also included a qualitative analysis of our results, shedding some light on the real capabilities of our model.", "human_reference": "Title: Improving Robustness of Question-Answering System Using Domain-adaptive Pretraining, Adversarial Training, Data Augmentation and Finetuning\nAbstract: From previous work, we know that Question-Answering (QA) system based on neural language models (NLM) is highly sensitive to the knowledge domain of training data and often has inferior performance when used for out-of-domain QA tasks. In this project, the authors attempt to combine a few published methods to improve the robustness of the QA system on out-of-domain data. We have tried methods including domain adversarial training, domain adaptive pretraining, finetuning on few samples, and data augmentation. We applied these methods through experimentation, improving the robustness of our baseline model on out-of-domain test datasets given two groups of training datasets: three large in-domain datasets and three very small out-of-domain datasets. We experimented and evaluated the effects of the above-mentioned methods both individually and combined, and found that while the individual method generates mixed results, the combination of them can improve the robustness of the baseline model in the QA task to the greatest extent on the out-of-domain datasets. We have also included a qualitative analysis of our results, shedding some light on the real capabilities of our model.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0144", "source": "CS224N"}}
{"ai_text": "Title: BiDAF Question Ansering with Character Embedding, Self-Attention, and Weighted Loss\nAbstract: Machine question answering remains a central problem in natural language processing. In this work, we build upon the default bidirectional attention flow model and explore the effect of adding character embeddings, self-attention, and a weighted loss function compared with the baseline. While character embeddings and self-attention have been demonstrated to improve the performance of language models, the motivation for a weighted loss function comes from the nature of the SQuAD dataset itself. We note that about half of the samples of the SQUAD dataset have no-answer, and is thus denoted by a start and end-pointer value of zero. Because the problem is effectively being treated as a classification problem (where the pointer locations are the classes to be predicted), this results in a ground truth distribution that is heavily skewed toward start and end-pointer class 0. To address this imbalance, we also propose the use of a weighted loss function, which down-weights no-answer examples, discouraging the model from simply guessing no-answer as a default choice. With a combined model, we achieve 62.11 EM and 65.54 F1 on the test set. We discover that a great deal of the error of the model comes from false-positives, and over-reliance on token matching.", "human_reference": "Title: BiDAF Question Ansering with Character Embedding, Self-Attention, and Weighted Loss\nAbstract: Machine question answering remains a central problem in natural language processing. In this work, we build upon the default bidirectional attention flow model and explore the effect of adding character embeddings, self-attention, and a weighted loss function compared with the baseline. While character embeddings and self-attention have been demonstrated to improve the performance of language models, the motivation for a weighted loss function comes from the nature of the SQuAD dataset itself. We note that about half of the samples of the SQUAD dataset have no-answer, and is thus denoted by a start and end-pointer value of zero. Because the problem is effectively being treated as a classification problem (where the pointer locations are the classes to be predicted), this results in a ground truth distribution that is heavily skewed toward start and end-pointer class 0. To address this imbalance, we also propose the use of a weighted loss function, which down-weights no-answer examples, discouraging the model from simply guessing no-answer as a default choice. With a combined model, we achieve 62.11 EM and 65.54 F1 on the test set. We discover that a great deal of the error of the model comes from false-positives, and over-reliance on token matching.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0023", "source": "CS224N"}}
{"ai_text": "I believe that I will follow my interest. I'm not saying that studying a subject for job opportunities is wrong, it's just that I'm not that kind of person. Me myself want to be a scientist in the future, and following my own interests are rather important, because doing research can be tedious or frustrating in many situations, and my interests may be the only thing to keep me going on and on. If you are only driven by profit, it's likely that you will abandon your current subject once it seems not so profitable, and that's clearly not good for the development of science.", "human_reference": "I believe that I will follow my interest. I'm not saying that studying a subject for job opportunities is wrong, it's just that I'm not that kind of person. Me myself want to be a scientist in the future, and following my own interests are rather important, because doing research can be tedious or frustrating in many situations, and my interests may be the only thing to keep me going on and on. If you are only driven by profit, it's likely that you will abandon your current subject once it seems not so profitable, and that's clearly not good for the development of science.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0004", "source": "TOEFL11"}}
{"ai_text": "The air thickened with red dust as I walked into the basement of Washington Studio School for my first sculpting class - a way to be creative after a stressful day. As I pulled back a thick curtain to enter, I looked around the room, examining the surfaces, all covered in a thin layer of that same dust. The bookshelves behind me were sporting a small collection of sculptures.        We were given a 4\u2019 by 6\u2019 block of clay to mold into a woman that was sitting in front of us. I stared at the block of clay, unable to imagine how to start. The woman next to me immediately started shaping her rust-colored slab. She took clumps from the bottom of the piece, adding it to the top, taking pieces away to form shoulders and arms. I spent more than an appropriate amount of time watching her work. I was amazed by the way she could see the woman inside her block of clay.                I turned back to my sculpture and gingerly shaved off a piece of clay from the top corner. I continued to work at that corner and that corner only as my instructor travelled around the room, visiting each of his students to offer tips and suggestions. When he made it to my table, he glanced at my piece. I had transformed the 4\u2019 by 6\u2019 rectangular prism into a pentagonal prism. He took one of my tools and started shaving away clay and suggested that I remove even more. He continued to visit the rest of his students as I continued to shave miniscule pieces of clay off of my now hexagonal prism.        I wanted to act on his advice, I wanted to take this opportunity to learn, but I did not want to do something wrong. I was afraid of the permanence of my choices. This fear continued to hold me back throughout the 3-hour lesson. By the end of the class, rather than my piece looking like the model sitting in front of me, my piece looked like Mario from the 1985 Super Mario Bros. I left the class, wondering when I started letting fear control my actions.        I remembered that I used to quite literally jump into new situations. The first time I went on a chair lift, for example, I had been so excited to \u201chit the slopes\u201d that instead of waiting for the chair lift to reach the end, I leaped off 8 feet too soon. Luckily, my dad caught me and held onto me until we reached the end of the lift.        The next week, I was determined to reclaim that feeling of fearlessness to make progress on my sculpture. This time, I took out clumps, rather than slithers. When my instructor reached my table, he pointed to plenty of problems with my piece. The arm was too high, the legs looked like a yeti\u2019s, and the head took the shape of a balloon. But I realized that at least I was doing it \u2014 and I was enjoying it, too.        My final piece was in no way a replica of the model who sat in front of me during those lessons: it was riddled with errors. But, while the person I was when I first entered the classroom may have hated the fact that she could see all the mistakes in her final structure, I now appreciate that I can see them, and that I can see how far I\u2019ve come since making them. No matter how deep under the surface of my sculpture the mistake might be, I know it is there. Every crack, air bubble, slip and score, is a working component in my sculpture. And I know that, like my sculpture, I\u2019ve been shaped by my mistakes, too: as long as I want to keep becoming myself, I\u2019ll need to keep making them.", "human_reference": "The air thickened with red dust as I walked into the basement of Washington Studio School for my first sculpting class - a way to be creative after a stressful day. As I pulled back a thick curtain to enter, I looked around the room, examining the surfaces, all covered in a thin layer of that same dust. The bookshelves behind me were sporting a small collection of sculptures.        We were given a 4\u2019 by 6\u2019 block of clay to mold into a woman that was sitting in front of us. I stared at the block of clay, unable to imagine how to start. The woman next to me immediately started shaping her rust-colored slab. She took clumps from the bottom of the piece, adding it to the top, taking pieces away to form shoulders and arms. I spent more than an appropriate amount of time watching her work. I was amazed by the way she could see the woman inside her block of clay.                I turned back to my sculpture and gingerly shaved off a piece of clay from the top corner. I continued to work at that corner and that corner only as my instructor travelled around the room, visiting each of his students to offer tips and suggestions. When he made it to my table, he glanced at my piece. I had transformed the 4\u2019 by 6\u2019 rectangular prism into a pentagonal prism. He took one of my tools and started shaving away clay and suggested that I remove even more. He continued to visit the rest of his students as I continued to shave miniscule pieces of clay off of my now hexagonal prism.        I wanted to act on his advice, I wanted to take this opportunity to learn, but I did not want to do something wrong. I was afraid of the permanence of my choices. This fear continued to hold me back throughout the 3-hour lesson. By the end of the class, rather than my piece looking like the model sitting in front of me, my piece looked like Mario from the 1985 Super Mario Bros. I left the class, wondering when I started letting fear control my actions.        I remembered that I used to quite literally jump into new situations. The first time I went on a chair lift, for example, I had been so excited to \u201chit the slopes\u201d that instead of waiting for the chair lift to reach the end, I leaped off 8 feet too soon. Luckily, my dad caught me and held onto me until we reached the end of the lift.        The next week, I was determined to reclaim that feeling of fearlessness to make progress on my sculpture. This time, I took out clumps, rather than slithers. When my instructor reached my table, he pointed to plenty of problems with my piece. The arm was too high, the legs looked like a yeti\u2019s, and the head took the shape of a balloon. But I realized that at least I was doing it \u2014 and I was enjoying it, too.        My final piece was in no way a replica of the model who sat in front of me during those lessons: it was riddled with errors. But, while the person I was when I first entered the classroom may have hated the fact that she could see all the mistakes in her final structure, I now appreciate that I can see them, and that I can see how far I\u2019ve come since making them. No matter how deep under the surface of my sculpture the mistake might be, I know it is there. Every crack, air bubble, slip and score, is a working component in my sculpture. And I know that, like my sculpture, I\u2019ve been shaped by my mistakes, too: as long as I want to keep becoming myself, I\u2019ll need to keep making them.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0006", "source": "CollegeEssay"}}
{"ai_text": "Title: Transformer Exploration\nAbstract: In this project we we build a question answering model for the SQuAD 2.0 dataset. Beginning with a baseline BiDAF model we make two extensions to improve the model. In the first extension we add character embeddings to match the model in the original BiDAF paper. Next we swap out the LSTM encoder for, the more parallelizable, Transformer block. After creating our word and character embeddings we add in positional encodings. Next we apply a single transformer encoder block featuring convolution and self attention to the embeddings of the context and the query. We then perform BiDirectional attention, before applying three more transformer blocks in the modeling layer. Finally we output a prediction of the answer or no answer if one does not exist.", "human_reference": "Title: Transformer Exploration\nAbstract: In this project we we build a question answering model for the SQuAD 2.0 dataset. Beginning with a baseline BiDAF model we make two extensions to improve the model. In the first extension we add character embeddings to match the model in the original BiDAF paper. Next we swap out the LSTM encoder for, the more parallelizable, Transformer block. After creating our word and character embeddings we add in positional encodings. Next we apply a single transformer encoder block featuring convolution and self attention to the embeddings of the context and the query. We then perform BiDirectional attention, before applying three more transformer blocks in the modeling layer. Finally we output a prediction of the answer or no answer if one does not exist.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0095", "source": "CS224N"}}
{"ai_text": "Title: RobustQA Using Data Augmentation\nAbstract: This project aims to explore possible improvements and extensions to the RobustQA Default baseline provided by the CS224N Winter quarter staff. Our goal is to create a domain-agnostic question answering system given DistilBERT as a pre-trained transformer model. The main method attempted in this paper is that of Task Adaptive Fine Tuning (TAPT), which entails a pre-training step utilizing the Masked Language Modeling task. This method was combined with experimentation on hyperparameters (batch size, number of epochs, and learning rate) to produce the highest-achieving model. Specifically, a pre-trained MLM model with a batch size of 32 yielded an EM of 42.75 and F1 of 61.14, which are each around 2 points higher than the baseline metrics.", "human_reference": "Title: RobustQA Using Data Augmentation\nAbstract: This project aims to explore possible improvements and extensions to the RobustQA Default baseline provided by the CS224N Winter quarter staff. Our goal is to create a domain-agnostic question answering system given DistilBERT as a pre-trained transformer model. The main method attempted in this paper is that of Task Adaptive Fine Tuning (TAPT), which entails a pre-training step utilizing the Masked Language Modeling task. This method was combined with experimentation on hyperparameters (batch size, number of epochs, and learning rate) to produce the highest-achieving model. Specifically, a pre-trained MLM model with a batch size of 32 yielded an EM of 42.75 and F1 of 61.14, which are each around 2 points higher than the baseline metrics.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0128", "source": "CS224N"}}
{"ai_text": "Title: A Dynamic Chunk Reader with Character Level Embeddings for Question Answering\nAbstract: In 2016, Yu et. al. proposed an  end-to-end neural reading comprehension model, know as a Dynamic Chunk Reader (DCR), for question answering. In this model they chose to input word embeddings as well as several other semantic and linguistic features such parts of speech and capitalization into their initial encoding layer. A natural follow-up to this is to experiment with different inputs to the encoding layer. One possibility is to input character embeddings in addition to the word embeddings. This paper describes a model that re-creates the DCR model from scratch and the creation of a character level embedding using CNNs to feed into the DCR model.", "human_reference": "Title: A Dynamic Chunk Reader with Character Level Embeddings for Question Answering\nAbstract: In 2016, Yu et. al. proposed an  end-to-end neural reading comprehension model, know as a Dynamic Chunk Reader (DCR), for question answering. In this model they chose to input word embeddings as well as several other semantic and linguistic features such parts of speech and capitalization into their initial encoding layer. A natural follow-up to this is to experiment with different inputs to the encoding layer. One possibility is to input character embeddings in addition to the word embeddings. This paper describes a model that re-creates the DCR model from scratch and the creation of a character level embedding using CNNs to feed into the DCR model.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0108", "source": "CS224N"}}
{"ai_text": "Title: SQuAD - Refined Implementation of Contextually Enriching Passage Sequences (SQUAD-RICEPS)\nAbstract: Our default project took on the task of SQuAD 2.0 Question Answering using inspiration from an approach described in Christopher Clark's 2017 paper, \"Simple and Effective Multi-Paragraph Reading Comprehension\". We combine the embedding, encoding, and bi-attention of BiDAF with an additional two layers of self attention. Our findings see an improvement when using a TriLinear attention layer on top of a Multiheaded Scaled Dot Product Self Attention layer. While we had promising results with character embeddings on the dev set, we were unable to refine our implementation of character embeddings to improve our model. We were able to produce an EM score of 59.5 and an F1 score of 62.7 which improved on the BiDAF baseline's score of 56.3 and 59.4.", "human_reference": "Title: SQuAD - Refined Implementation of Contextually Enriching Passage Sequences (SQUAD-RICEPS)\nAbstract: Our default project took on the task of SQuAD 2.0 Question Answering using inspiration from an approach described in Christopher Clark's 2017 paper, \"Simple and Effective Multi-Paragraph Reading Comprehension\". We combine the embedding, encoding, and bi-attention of BiDAF with an additional two layers of self attention. Our findings see an improvement when using a TriLinear attention layer on top of a Multiheaded Scaled Dot Product Self Attention layer. While we had promising results with character embeddings on the dev set, we were unable to refine our implementation of character embeddings to improve our model. We were able to produce an EM score of 59.5 and an F1 score of 62.7 which improved on the BiDAF baseline's score of 56.3 and 59.4.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0061", "source": "CS224N"}}
{"ai_text": "Oreos. On the exterior, a firm chocolate crust; however, when opened, a creamy white center awaits. Unbeknownst to me, a social meaning awaited behind an Oreo that left a lingering poor taste in my mouth. From the seductive, powerful attacks within a tango melody to the upbeat, peppy nature of Top 40 hits, I find myself within a new story with each note. Ballroom and pop music, while vastly different styles, have been interconnected since I was a little girl listening to both Hans Zimmer\u2019s \u2018Discombobulate and One Direction\u2019s Kiss You. In high school, when I shared my musical taste with my black peers, I received confused stares back. \u201cFaith, that is the whitest thing. You are such an Oreo!\u201d a friend exclaimed. I didn\u2019t recognize the connection between two seemingly different commodities until I later learned that an Oreo means a black person who displays characteristics typically associated with white people, therefore betraying their black roots. I never saw ballroom and pop music belonging to a certain race, but the negatively charged implications behind \u2018betraying\u2019 introduced new guilty sensations. Should I embrace my musical interests and face social alienation from those who share my skin tone? Or set aside my so-called white core and conform to the expectations of an African-American woman that have been placed upon me? I didn\u2019t cut music completely out of my life. Being a clarinet player in my band meant being exposed to various musical styles each day. During my freshman year, I decided to challenge myself and perform a solo for the county solo & ensemble festival. Lyrical Composition No. 6 was a piece for which I could play the notes, the rhythms, and everything else on the page. To me, that was all I needed to do, but my band director thought otherwise. \u201cYou\u2019re great at playing the right note at the right time. But where is your interpretation? What can you do to add to this piece?\u201d At first glance, all I saw were measures of black ink permanently etched into the sheet \u2013 resistant to change. How do I add to a composition that exudes such a definitive nature? Then at second glance, I looked below the measures. Beyond the notes, beyond the rhythms, I noticed white space \u2013 unblemished and waiting for me to create my own contribution. Once I stopped and determined what I wanted someone to feel from this composition, I picked up my pencil and wrote in crescendos, decrescendos, breath marks, and other musical markings that I felt needed to be included. I didn\u2019t want to simply regurgitate the black ink, but rather take the audience on a dynamic journey that reaches a climactic precipice. This realization made the distinction between style and stereotype clear. Being categorized as an Oreo was jarring because the documented definition couldn\u2019t simply be erased. Most stereotypes are never fully expunged because they are deeply ingrained in how society views certain races. While I cannot easily change the minds of the many, I can change the mind of my own. I am my own music maker. I will celebrate the intricacies of ballroom music and belt out a One Direction tune as a proud black woman. That is my style. That is my choice of expression. If allowed, stereotypes can snowball until I am completely consumed by my desire to become the black woman society expects. But I refuse to be held down by its grip because I decide my definition of the black experience. My musical interests are not a betrayal that isolates me from my roots, but rather a beautiful addition that enhances my ever-evolving character. Am I an Oreo? Yes, but by my own design. The creamy white center does not represent a betrayal, but rather a blank canvas patiently waiting for my own input. With pencil in hand, I will not hesitate to make my mark.", "human_reference": "Oreos. On the exterior, a firm chocolate crust; however, when opened, a creamy white center awaits. Unbeknownst to me, a social meaning awaited behind an Oreo that left a lingering poor taste in my mouth. From the seductive, powerful attacks within a tango melody to the upbeat, peppy nature of Top 40 hits, I find myself within a new story with each note. Ballroom and pop music, while vastly different styles, have been interconnected since I was a little girl listening to both Hans Zimmer\u2019s \u2018Discombobulate and One Direction\u2019s Kiss You. In high school, when I shared my musical taste with my black peers, I received confused stares back. \u201cFaith, that is the whitest thing. You are such an Oreo!\u201d a friend exclaimed. I didn\u2019t recognize the connection between two seemingly different commodities until I later learned that an Oreo means a black person who displays characteristics typically associated with white people, therefore betraying their black roots. I never saw ballroom and pop music belonging to a certain race, but the negatively charged implications behind \u2018betraying\u2019 introduced new guilty sensations. Should I embrace my musical interests and face social alienation from those who share my skin tone? Or set aside my so-called white core and conform to the expectations of an African-American woman that have been placed upon me? I didn\u2019t cut music completely out of my life. Being a clarinet player in my band meant being exposed to various musical styles each day. During my freshman year, I decided to challenge myself and perform a solo for the county solo & ensemble festival. Lyrical Composition No. 6 was a piece for which I could play the notes, the rhythms, and everything else on the page. To me, that was all I needed to do, but my band director thought otherwise. \u201cYou\u2019re great at playing the right note at the right time. But where is your interpretation? What can you do to add to this piece?\u201d At first glance, all I saw were measures of black ink permanently etched into the sheet \u2013 resistant to change. How do I add to a composition that exudes such a definitive nature? Then at second glance, I looked below the measures. Beyond the notes, beyond the rhythms, I noticed white space \u2013 unblemished and waiting for me to create my own contribution. Once I stopped and determined what I wanted someone to feel from this composition, I picked up my pencil and wrote in crescendos, decrescendos, breath marks, and other musical markings that I felt needed to be included. I didn\u2019t want to simply regurgitate the black ink, but rather take the audience on a dynamic journey that reaches a climactic precipice. This realization made the distinction between style and stereotype clear. Being categorized as an Oreo was jarring because the documented definition couldn\u2019t simply be erased. Most stereotypes are never fully expunged because they are deeply ingrained in how society views certain races. While I cannot easily change the minds of the many, I can change the mind of my own. I am my own music maker. I will celebrate the intricacies of ballroom music and belt out a One Direction tune as a proud black woman. That is my style. That is my choice of expression. If allowed, stereotypes can snowball until I am completely consumed by my desire to become the black woman society expects. But I refuse to be held down by its grip because I decide my definition of the black experience. My musical interests are not a betrayal that isolates me from my roots, but rather a beautiful addition that enhances my ever-evolving character. Am I an Oreo? Yes, but by my own design. The creamy white center does not represent a betrayal, but rather a blank canvas patiently waiting for my own input. With pencil in hand, I will not hesitate to make my mark.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0037", "source": "CollegeEssay"}}
{"ai_text": "\u201cYou\u2019re such a hipster.\u201d It\u2019s a phrase heard everyday in school hallways across America, and its usage often operates as a conundrum that obscures teenagers\u2019 perceptions of themselves and who they want to be. I, in turn, have struggled immensely with the paradoxical use of this label. Since the onset of my tween years and perhaps even before that, I have constantly carried with me an insistent urge for nonconformity; it has never sat well with me to be like everyone else. Throughout my middle school years, this natural instinct of mine manifested itself in many different ways: jeans tucked into knee-high socks, anything from punk to Harlem renaissance jazz bellowing from my headphones, Palahniuk novels peeking out of my backpack. As my identity shifted, my career as a social renegade flourished, and I found in myself a certain pride in being different and a passion for seeking out eccentric new ways to express myself. With the realization of my newfound passion, my nonconformist qualities were locked in, and I began high school without the usual freshman trepidation about getting labeled or branded. Thereby, I continued my habitual antics, rebelling against the social norm and doing what I could to think freely. In doing so, however, I encountered a particular subculture defined by certain fashion trends and, to some extent, genres of music. This subculture was and still is often associated with the term \u201chipster\u201d and regarded as having a correspondence with the \u201chipster lifestyle.\u201d Moreover, so-called \u201chipsters\u201d are known to have particularly poignant tendencies towards nonconformity. Thus, my rise to the hipster ideal began. I was enamored with various aspects of this subculture, so I enthusiastically donned a beanie and cardigan and crammed every Bon Iver and The Smiths album I could find on to my iPod. Such new developments in my identity perfectly suited my singularity as a nonconformist; no one I knew had adopted this flair. Admittedly, my new garb was somewhat funky, and thus the new look evoked, in both positive and negative renditions, choruses of \u201cYou\u2019re such a hipster!\u201d The attention was extraordinarily gratifying, and I consequently plunged into obsession with my new label, consumed in an effort to sustain my \u201chipster\u201d reputation. Much of my mental vitality was spent on keeping my appearance and status up to a sufficiently \u201chipster\u201d standard. The questions I asked myself about who I wanted to be quickly evolved into \u201cHow can I fit the ideal?\u201d and \u201cHow can I conform?\u201d Herein lies the continual paradox for people who identify themselves as \u201chipsters\u201d and the contradiction that brought me much confusion and uncertainty for parts of my high school career: implicit in the definition of the term \u201chipster\u201d is the prominence of nonconformity in all aspects of a \u201chipster\u2019s\u201d lifestyle. Individualist ideals permeate his clothes, his music, his social behavior, even his politics. Simultaneously, however, one who seeks to identify himself and be identified by others as a \u201chipster\u201d undoubtably strives to conform to the \u201chipster\u201d construct; he tries to fit himself inside an inflexible \u201chipster\u201d box. Nevertheless, as with most paradoxes, the problem at hand does not imply a real contradiction. I found the solution after many months of personal struggle with my own identity. It is not that there is something inherently wrong with the qualities of a \u201chipster.\u201d I have come to understand that a label such as \u201chipster\u201d must never precede my own actual characteristics, and I can never let such a notion inform my identity by itself. Before I ever begin to set words to my character, I have to figure out who I am free from outside influence. The adjectives come much later.", "human_reference": "\u201cYou\u2019re such a hipster.\u201d It\u2019s a phrase heard everyday in school hallways across America, and its usage often operates as a conundrum that obscures teenagers\u2019 perceptions of themselves and who they want to be. I, in turn, have struggled immensely with the paradoxical use of this label. Since the onset of my tween years and perhaps even before that, I have constantly carried with me an insistent urge for nonconformity; it has never sat well with me to be like everyone else. Throughout my middle school years, this natural instinct of mine manifested itself in many different ways: jeans tucked into knee-high socks, anything from punk to Harlem renaissance jazz bellowing from my headphones, Palahniuk novels peeking out of my backpack. As my identity shifted, my career as a social renegade flourished, and I found in myself a certain pride in being different and a passion for seeking out eccentric new ways to express myself. With the realization of my newfound passion, my nonconformist qualities were locked in, and I began high school without the usual freshman trepidation about getting labeled or branded. Thereby, I continued my habitual antics, rebelling against the social norm and doing what I could to think freely. In doing so, however, I encountered a particular subculture defined by certain fashion trends and, to some extent, genres of music. This subculture was and still is often associated with the term \u201chipster\u201d and regarded as having a correspondence with the \u201chipster lifestyle.\u201d Moreover, so-called \u201chipsters\u201d are known to have particularly poignant tendencies towards nonconformity. Thus, my rise to the hipster ideal began. I was enamored with various aspects of this subculture, so I enthusiastically donned a beanie and cardigan and crammed every Bon Iver and The Smiths album I could find on to my iPod. Such new developments in my identity perfectly suited my singularity as a nonconformist; no one I knew had adopted this flair. Admittedly, my new garb was somewhat funky, and thus the new look evoked, in both positive and negative renditions, choruses of \u201cYou\u2019re such a hipster!\u201d The attention was extraordinarily gratifying, and I consequently plunged into obsession with my new label, consumed in an effort to sustain my \u201chipster\u201d reputation. Much of my mental vitality was spent on keeping my appearance and status up to a sufficiently \u201chipster\u201d standard. The questions I asked myself about who I wanted to be quickly evolved into \u201cHow can I fit the ideal?\u201d and \u201cHow can I conform?\u201d Herein lies the continual paradox for people who identify themselves as \u201chipsters\u201d and the contradiction that brought me much confusion and uncertainty for parts of my high school career: implicit in the definition of the term \u201chipster\u201d is the prominence of nonconformity in all aspects of a \u201chipster\u2019s\u201d lifestyle. Individualist ideals permeate his clothes, his music, his social behavior, even his politics. Simultaneously, however, one who seeks to identify himself and be identified by others as a \u201chipster\u201d undoubtably strives to conform to the \u201chipster\u201d construct; he tries to fit himself inside an inflexible \u201chipster\u201d box. Nevertheless, as with most paradoxes, the problem at hand does not imply a real contradiction. I found the solution after many months of personal struggle with my own identity. It is not that there is something inherently wrong with the qualities of a \u201chipster.\u201d I have come to understand that a label such as \u201chipster\u201d must never precede my own actual characteristics, and I can never let such a notion inform my identity by itself. Before I ever begin to set words to my character, I have to figure out who I am free from outside influence. The adjectives come much later.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0045", "source": "CollegeEssay"}}
{"ai_text": "Title: QANet+: Improving QANet for Question Answering\nAbstract: In this work, we build a question answering (QA) system and apply it on the Stanford Question Answering Dataset, version 2.0. Our goal is to achieve strong performance on this task without using pre-trained language models. Our primary contribution is a highly performant implementation of the QANet model. Additionally, we experiment with various modifications to this architecture. Most notably, we show that modifying the output layer, such that answer span's ending position prediction is a function of the starting position prediction, yields significant improvements over the original design. Using a QANet ensemble, we reach an F1 score of 71.87 and an EM score of 68.89 on an unseen test set (rank #1 out of 100+ submissions to the test leaderboard for the IID SQuAD Track of CS 224N at Stanford, Winter 2021).", "human_reference": "Title: QANet+: Improving QANet for Question Answering\nAbstract: In this work, we build a question answering (QA) system and apply it on the Stanford Question Answering Dataset, version 2.0. Our goal is to achieve strong performance on this task without using pre-trained language models. Our primary contribution is a highly performant implementation of the QANet model. Additionally, we experiment with various modifications to this architecture. Most notably, we show that modifying the output layer, such that answer span's ending position prediction is a function of the starting position prediction, yields significant improvements over the original design. Using a QANet ensemble, we reach an F1 score of 71.87 and an EM score of 68.89 on an unseen test set (rank #1 out of 100+ submissions to the test leaderboard for the IID SQuAD Track of CS 224N at Stanford, Winter 2021).", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0141", "source": "CS224N"}}
{"ai_text": "My math teacher turns around to write an equation on the board and a sun pokes out from the collar of her shirt. A Starbucks barista hands me my drink with a hand adorned by a small music note. Where I work, a customer hands me her credit card wearing a permanent flower bracelet. Every day, I am on a scavenger hunt to find women with this kind of permanent art. I'm intrigued by the quotes, dates, symbols, and abstract shapes I see on people that I interact with daily. I've started to ask them questions, an informal interview, as an excuse to talk with these diverse women whose individuality continually inspires me. You can't usually ask the sorts of questions I have been asking and have the sorts of conversations I have been having, so I've created this project to make these kinds of encounters a bit more possible and acceptable. There is no school assignment, no teacher to give me a grade, and no deadline. I don't have a concrete outcome in mind besides talking with a mix of interesting women with interesting tattoos. So far I've conducted fifteen interviews with a range of women from my hometown to Hawaii, teenagers to senior citizens, teachers to spiritual healers. The same set of questions has prompted interviews lasting less than twenty minutes and over two hours. I'm being told stories about deaths of a parent, struggles with cancer, coming out experiences, sexual assaults, and mental illnesses. All of these things that may be taboo in today's society, these women are quite literally wearing on their sleeves. I'm eager to continue these interviews in college and use all of the material I've gathered to show the world the strength and creativity of these wonderful women I've encountered. I want to explore the art and stories behind the permanent transformations of personal landscapes. I attempt this by asking questions about why they decided to get their tattoos, how they were received in the workplace, the reactions from family and friends, and the tattoo's impact on their own femininity. Through these simple questions, I happened upon much greater lessons regarding human interaction, diversity, and connectedness. In my first interview, a local businesswoman told me about her rocky relationship with her mother, her struggles with mental illness, and her friend in jail, within 45 minutes of meeting her and in the middle of a busy Starbucks. An artist educator I worked with told me that getting a tattoo \"was like claiming a part of yourself and making it more visible and unavoidable.\" A model/homeopath said that having a tattoo is like \"giving people a little clue about you.\" A psychologist shared how she wishes that she could turn her tattoos \"on or off like a light switch to match different outfits and occasions.\" I've realized that tattoos show the complex relationship between the personal and the public (and how funny that can be when a Matisse cutout is thought to be phallic, or how a social worker's abstract doodle is interpreted as a tsunami of sticks, alien spaceship, and a billion other things by the children she works with). I've learned so much about the art of storytelling and storytelling through art. I've strengthened relationships with people that had conventional roles in my life and created friendships with some unconventional characters. Most importantly, I've realized that with the willingness to explore a topic and the willingness to accept not knowing where it will go, an idea can become a substantive reality.", "human_reference": "My math teacher turns around to write an equation on the board and a sun pokes out from the collar of her shirt. A Starbucks barista hands me my drink with a hand adorned by a small music note. Where I work, a customer hands me her credit card wearing a permanent flower bracelet. Every day, I am on a scavenger hunt to find women with this kind of permanent art. I'm intrigued by the quotes, dates, symbols, and abstract shapes I see on people that I interact with daily. I've started to ask them questions, an informal interview, as an excuse to talk with these diverse women whose individuality continually inspires me. You can't usually ask the sorts of questions I have been asking and have the sorts of conversations I have been having, so I've created this project to make these kinds of encounters a bit more possible and acceptable. There is no school assignment, no teacher to give me a grade, and no deadline. I don't have a concrete outcome in mind besides talking with a mix of interesting women with interesting tattoos. So far I've conducted fifteen interviews with a range of women from my hometown to Hawaii, teenagers to senior citizens, teachers to spiritual healers. The same set of questions has prompted interviews lasting less than twenty minutes and over two hours. I'm being told stories about deaths of a parent, struggles with cancer, coming out experiences, sexual assaults, and mental illnesses. All of these things that may be taboo in today's society, these women are quite literally wearing on their sleeves. I'm eager to continue these interviews in college and use all of the material I've gathered to show the world the strength and creativity of these wonderful women I've encountered. I want to explore the art and stories behind the permanent transformations of personal landscapes. I attempt this by asking questions about why they decided to get their tattoos, how they were received in the workplace, the reactions from family and friends, and the tattoo's impact on their own femininity. Through these simple questions, I happened upon much greater lessons regarding human interaction, diversity, and connectedness. In my first interview, a local businesswoman told me about her rocky relationship with her mother, her struggles with mental illness, and her friend in jail, within 45 minutes of meeting her and in the middle of a busy Starbucks. An artist educator I worked with told me that getting a tattoo \"was like claiming a part of yourself and making it more visible and unavoidable.\" A model/homeopath said that having a tattoo is like \"giving people a little clue about you.\" A psychologist shared how she wishes that she could turn her tattoos \"on or off like a light switch to match different outfits and occasions.\" I've realized that tattoos show the complex relationship between the personal and the public (and how funny that can be when a Matisse cutout is thought to be phallic, or how a social worker's abstract doodle is interpreted as a tsunami of sticks, alien spaceship, and a billion other things by the children she works with). I've learned so much about the art of storytelling and storytelling through art. I've strengthened relationships with people that had conventional roles in my life and created friendships with some unconventional characters. Most importantly, I've realized that with the willingness to explore a topic and the willingness to accept not knowing where it will go, an idea can become a substantive reality.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0043", "source": "CollegeEssay"}}
{"ai_text": "Title: Exploring Improvements to the SQuAD 2.0 BiDAF Model\nAbstract: We have explored different deep learning based approaches to the question answering problem on SQuAD 2.0 using an improved version of the BiDAF model. Our baseline was provided by the default project starter code, and is a modified BiDAF that has only word embeddings and performs on SQuAD 2.0. We explored three areas of improvements: character embeddings, conditioning the end prediction on the start prediction, and adding a self-attention layer. We found that the biggest improvement was from the Condition End Prediction on Start Prediction and Self-Attention with an F1 and EM score of 65.285 and 61.758 on the test set respectively. The model with character embeddings scored a 59.96 on EM and a 63.24 on F1, and the model with character embedding and self attention scored a 63 on EM and a 66.2 on F1 (both for the dev set). In our error analysis, we discovered that generally, all models performed well on questions that began with \"When\", and performed poorly on questions that begin with \"What\" and \"The\". Our future work includes investigating how further extensions, like transformers, co-attention, and different input features affect performance. Overall, this project was very educational, as it allowed us to read through numerous papers that outlined breakthrough improvements to this problem, and enabled us to implement ourselves the methods described in the papers.", "human_reference": "Title: Exploring Improvements to the SQuAD 2.0 BiDAF Model\nAbstract: We have explored different deep learning based approaches to the question answering problem on SQuAD 2.0 using an improved version of the BiDAF model. Our baseline was provided by the default project starter code, and is a modified BiDAF that has only word embeddings and performs on SQuAD 2.0. We explored three areas of improvements: character embeddings, conditioning the end prediction on the start prediction, and adding a self-attention layer. We found that the biggest improvement was from the Condition End Prediction on Start Prediction and Self-Attention with an F1 and EM score of 65.285 and 61.758 on the test set respectively. The model with character embeddings scored a 59.96 on EM and a 63.24 on F1, and the model with character embedding and self attention scored a 63 on EM and a 66.2 on F1 (both for the dev set). In our error analysis, we discovered that generally, all models performed well on questions that began with \"When\", and performed poorly on questions that begin with \"What\" and \"The\". Our future work includes investigating how further extensions, like transformers, co-attention, and different input features affect performance. Overall, this project was very educational, as it allowed us to read through numerous papers that outlined breakthrough improvements to this problem, and enabled us to implement ourselves the methods described in the papers.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0085", "source": "CS224N"}}
{"ai_text": "Several years ago, my mother told me I listen to \u201cwhite people music.\u201d And I suppose that\u2019s true\u2014rock 'n' roll tends to spring from the middle-class basements of young, white men. Though I did point out that its origins trace back to jazz musicians of the Harlem Renaissance. Also that one of the greatest guitarists of all time\u2014dear Mr.Hendrix; may he rest in peace\u2014was black. My devotion to punk rock began in seventh grade, when Green Day\u2019s \u201cBoulevard of Broken Dreams\u201d came up on my iTunes shuffle. I started to look into their other releases, eventually immersing myself into the complete punk discography. My mother, having grown up in a racially segregated New York, was more likely to listen to Stevie Wonder than Stevie Nicks. But, she must have figured, to each her own. So while my compatriots indulged in the music of Taylor Swift, One Direction, and Lady Gaga, my tacky Hot Topic headphones blasted Green Day, Ramones, and The Clash. My young adolescent ears drank in the raw, chaotic beauty, an echo of the pain of the past. The thrashing, pulsating vitality of the instruments painted a picture, connecting me to the disillusioned kids who launched an epic movement of liberation some 40 years ago. Punkers question authority. Aggressively contrarian, they advocate for the other side\u2014the side that seemed smothered silent during the post-Vietnam era. They rejected the established norms. They spoke out and weren\u2019t afraid. I had always felt different from my peers. In my girls\u2019s prep school, the goal was to be blond and good at soccer. I was neither, which automatically deemed me \u201cuncool\u201d. I had a few close friends but never felt like I was part of a whole. Then came the punk philosophy, for the outliers, for those who were different. That was something I could be part of. Instead of trying to conform to my peers, I adopted an anti-conformist attitude. Much like the prematurely gray anti-hero of my favorite book, I sneered at all the \u201cphonies\u201d around me. I resented anything popular. Uggs? Wouldn\u2019t buy them. Yoga pants? Never. Starbucks?Well, I could make a few concessions. But I felt more cynical than liberated. I wasted so much energy on being different than I lost track of what actually made me happy. I insisted I didn\u2019t care what people thought of me, which was true. Yet if I based my actions almost solely on their behavior, how could I deny their influence? Luckily, as I transitioned from a private school to a brand new public high school, I got to clean the slate. I bought yoga pants and found they were comfortable. I listened to a wide variety of music, even the eh kind that wasn\u2019t 100% hardcore punk. And I was happier. I revised my punk philosophy: Do as you like\u2014whether it fits into the \u201csystem\u201d or not. The Beatles\u2019s \u201cRevolution\u201d lyrics sum it up well: You tell me it\u2019s the institution Well, you know You\u2019d better free your mind instead What I think Lennon was getting at is questioning everything does not entail opposing everything. Defiance for the sake of defiance is unproductive at best, destructive at worst. I believe in life\u2019s greater Truths, like Love and Justice. These Truths are what should govern my actions\u2014not what\u2019s popular and what isn\u2019t. Striving to act on these ideals has helped me stay true to myself, regardless of what\u2019s considered \"conformist.\" Perhaps I\u2019ve failed the punk movement. We\u2019ll have to wait and see. In the meantime, I\u2019ll do what makes me happy and change what doesn\u2019t. I\u2019ll wear Doc Martens instead of Uggs; I\u2019ll partake in a grande pumpkin spice latte; I\u2019ll watch Gossip Girl; I\u2019ll blare my favorite guitar solo over the speakers in my room. And that\u2019s as punk as it gets.", "human_reference": "Several years ago, my mother told me I listen to \u201cwhite people music.\u201d And I suppose that\u2019s true\u2014rock 'n' roll tends to spring from the middle-class basements of young, white men. Though I did point out that its origins trace back to jazz musicians of the Harlem Renaissance. Also that one of the greatest guitarists of all time\u2014dear Mr.Hendrix; may he rest in peace\u2014was black. My devotion to punk rock began in seventh grade, when Green Day\u2019s \u201cBoulevard of Broken Dreams\u201d came up on my iTunes shuffle. I started to look into their other releases, eventually immersing myself into the complete punk discography. My mother, having grown up in a racially segregated New York, was more likely to listen to Stevie Wonder than Stevie Nicks. But, she must have figured, to each her own. So while my compatriots indulged in the music of Taylor Swift, One Direction, and Lady Gaga, my tacky Hot Topic headphones blasted Green Day, Ramones, and The Clash. My young adolescent ears drank in the raw, chaotic beauty, an echo of the pain of the past. The thrashing, pulsating vitality of the instruments painted a picture, connecting me to the disillusioned kids who launched an epic movement of liberation some 40 years ago. Punkers question authority. Aggressively contrarian, they advocate for the other side\u2014the side that seemed smothered silent during the post-Vietnam era. They rejected the established norms. They spoke out and weren\u2019t afraid. I had always felt different from my peers. In my girls\u2019s prep school, the goal was to be blond and good at soccer. I was neither, which automatically deemed me \u201cuncool\u201d. I had a few close friends but never felt like I was part of a whole. Then came the punk philosophy, for the outliers, for those who were different. That was something I could be part of. Instead of trying to conform to my peers, I adopted an anti-conformist attitude. Much like the prematurely gray anti-hero of my favorite book, I sneered at all the \u201cphonies\u201d around me. I resented anything popular. Uggs? Wouldn\u2019t buy them. Yoga pants? Never. Starbucks?Well, I could make a few concessions. But I felt more cynical than liberated. I wasted so much energy on being different than I lost track of what actually made me happy. I insisted I didn\u2019t care what people thought of me, which was true. Yet if I based my actions almost solely on their behavior, how could I deny their influence? Luckily, as I transitioned from a private school to a brand new public high school, I got to clean the slate. I bought yoga pants and found they were comfortable. I listened to a wide variety of music, even the eh kind that wasn\u2019t 100% hardcore punk. And I was happier. I revised my punk philosophy: Do as you like\u2014whether it fits into the \u201csystem\u201d or not. The Beatles\u2019s \u201cRevolution\u201d lyrics sum it up well: You tell me it\u2019s the institution Well, you know You\u2019d better free your mind instead What I think Lennon was getting at is questioning everything does not entail opposing everything. Defiance for the sake of defiance is unproductive at best, destructive at worst. I believe in life\u2019s greater Truths, like Love and Justice. These Truths are what should govern my actions\u2014not what\u2019s popular and what isn\u2019t. Striving to act on these ideals has helped me stay true to myself, regardless of what\u2019s considered \"conformist.\" Perhaps I\u2019ve failed the punk movement. We\u2019ll have to wait and see. In the meantime, I\u2019ll do what makes me happy and change what doesn\u2019t. I\u2019ll wear Doc Martens instead of Uggs; I\u2019ll partake in a grande pumpkin spice latte; I\u2019ll watch Gossip Girl; I\u2019ll blare my favorite guitar solo over the speakers in my room. And that\u2019s as punk as it gets.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0020", "source": "CollegeEssay"}}
{"ai_text": "Meditation over a flaxen sunset with a friend and parmesan-topped spaghetti for dinner \u2014 \u201c14.\u201d Assignments piling up on my desk as a high fever keeps me sick at home \u2014 \u201c3.\u201d Taking a photo excursion through downtown Seattle for a Spanish project \u2014 \u201c15.\u201d For the past 700 days and counting, the Happiness Spreadsheet has been my digital collection for documenting numerical, descriptive, and graphical representations of my happiness. Its instructions are simple: Open the Google Sheet, enter a number between 1 and 20 that best represents my level of happiness, and write a short comment describing the day. But the practical aspect of the spreadsheet is only a piece of what it has represented in my life. A \u201c14\u201d etched on November 15, 2018, marked the first Lakeside Cooking on the Stove Club meeting. What had started as a farcical proposition of mine transformed into a playground where high school classmates and I convene every two weeks to prepare a savory afternoon snack for ourselves. A few months later, a \u201c16\u201d scribbled on February 27, 2019, marked the completion of a fence my Spanish class and I constructed for the dusty soccer field at a small Colombian village. Hard-fought days of mixing cement and transporting supplies had paid off for the affectionate community we had immediately come to love. The Happiness Spreadsheet doesn\u2019t only reflect my own thoughts and emotions; it is an illustration of the fulfillment I get from gifting happiness to others. If happiness paves the roads of my life, my family is the city intertwined by those roads \u2014 each member a distinct neighborhood, a distinct story. In times of stress, whether it be studying for an upcoming derivatives test or presenting my research at an international conference, I dash to my father for help. Coming from the dusty, people-packed backstreets of Thiruvananthapuram, India, he guides me in looking past the chaos and noticing the hidden accomplishments that lie in the corners. When in need of confidence, I find my mother, who taps her experiences living in her tranquil and sturdy tatami-covered home in Hiroshima, Japan, helping me prepare for my first high school dance or my final match in a tennis tournament. Whenever my Happiness Spreadsheet numbers touch lows, my family is always there to level me out to \u201c10.\u201d The Happiness Spreadsheet is also a battery monitor for enthusiasm. On occasion, it is on full charge, like when I touched the last chord on the piano for my composition's winner recital or when, one frosty Friday morning, I convinced a teacher to play over the school speakers a holiday medley I\u2019d recorded with a friend. Other times, the battery is depleted, and I am frustrated by writer's block, when not a single melody, chord, or musical construct crosses my mind. The Happiness Spreadsheet can be a hall of fame, but it can likewise be a catalog of mistakes, burdens, and grueling challenges. The spreadsheet began on a typical school day when I left my physics class following the most confusing test I\u2019d taken. The idea was born spontaneously at lunch, and I asked two of my friends if they were interested in pursuing this exercise with me. We thought the practice would last only a couple of weeks or months at most, but after reaching 700 days, we now wonder if we\u2019ll ever stop. To this day, I ponder its full importance in my life. With every new number I enter, I recognize that each entry is not what defines me; rather, it is the ever-growing line connecting all the data points that reflects who I am today. With every valley, I force myself onward and with every mountain's peak, I recognize the valleys I\u2019ve crossed to reach the summit. Where will the Happiness Spreadsheet take me next?", "human_reference": "Meditation over a flaxen sunset with a friend and parmesan-topped spaghetti for dinner \u2014 \u201c14.\u201d Assignments piling up on my desk as a high fever keeps me sick at home \u2014 \u201c3.\u201d Taking a photo excursion through downtown Seattle for a Spanish project \u2014 \u201c15.\u201d For the past 700 days and counting, the Happiness Spreadsheet has been my digital collection for documenting numerical, descriptive, and graphical representations of my happiness. Its instructions are simple: Open the Google Sheet, enter a number between 1 and 20 that best represents my level of happiness, and write a short comment describing the day. But the practical aspect of the spreadsheet is only a piece of what it has represented in my life. A \u201c14\u201d etched on November 15, 2018, marked the first Lakeside Cooking on the Stove Club meeting. What had started as a farcical proposition of mine transformed into a playground where high school classmates and I convene every two weeks to prepare a savory afternoon snack for ourselves. A few months later, a \u201c16\u201d scribbled on February 27, 2019, marked the completion of a fence my Spanish class and I constructed for the dusty soccer field at a small Colombian village. Hard-fought days of mixing cement and transporting supplies had paid off for the affectionate community we had immediately come to love. The Happiness Spreadsheet doesn\u2019t only reflect my own thoughts and emotions; it is an illustration of the fulfillment I get from gifting happiness to others. If happiness paves the roads of my life, my family is the city intertwined by those roads \u2014 each member a distinct neighborhood, a distinct story. In times of stress, whether it be studying for an upcoming derivatives test or presenting my research at an international conference, I dash to my father for help. Coming from the dusty, people-packed backstreets of Thiruvananthapuram, India, he guides me in looking past the chaos and noticing the hidden accomplishments that lie in the corners. When in need of confidence, I find my mother, who taps her experiences living in her tranquil and sturdy tatami-covered home in Hiroshima, Japan, helping me prepare for my first high school dance or my final match in a tennis tournament. Whenever my Happiness Spreadsheet numbers touch lows, my family is always there to level me out to \u201c10.\u201d The Happiness Spreadsheet is also a battery monitor for enthusiasm. On occasion, it is on full charge, like when I touched the last chord on the piano for my composition's winner recital or when, one frosty Friday morning, I convinced a teacher to play over the school speakers a holiday medley I\u2019d recorded with a friend. Other times, the battery is depleted, and I am frustrated by writer's block, when not a single melody, chord, or musical construct crosses my mind. The Happiness Spreadsheet can be a hall of fame, but it can likewise be a catalog of mistakes, burdens, and grueling challenges. The spreadsheet began on a typical school day when I left my physics class following the most confusing test I\u2019d taken. The idea was born spontaneously at lunch, and I asked two of my friends if they were interested in pursuing this exercise with me. We thought the practice would last only a couple of weeks or months at most, but after reaching 700 days, we now wonder if we\u2019ll ever stop. To this day, I ponder its full importance in my life. With every new number I enter, I recognize that each entry is not what defines me; rather, it is the ever-growing line connecting all the data points that reflects who I am today. With every valley, I force myself onward and with every mountain's peak, I recognize the valleys I\u2019ve crossed to reach the summit. Where will the Happiness Spreadsheet take me next?", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0057", "source": "CollegeEssay"}}
{"ai_text": "Title: QA System Using Feature Engineering and Self-Attention (IID SQuAD track)\nAbstract: Machine reading comprehension is an exceedingly important task in NLP and is a desired feature in many of the latest consumer and research projects. Therefore, using this task as motivation, we set out to build a reading comprehension model that performed well on the SQuAD 2.0 question answering dataset. To do this, we built upon the existing BiDAF  machine comprehension model given to us through the CS224n staff. Our contributions to this model are a character embedding layer on top of the existing word embedding layer, a self attention layer, and added features to the character and word embeddings which include Part of Speech tags (POS), named entity recognition (NER) tags, and dependency tags. As a result of implementing these layers we found that character embedding with additional input features performed the best with an F1 dev score of 64.38 and an EM dev score 61.29. On the test set we achieved F1 and EM scores 62.17 and 59.04 respectively.", "human_reference": "Title: QA System Using Feature Engineering and Self-Attention (IID SQuAD track)\nAbstract: Machine reading comprehension is an exceedingly important task in NLP and is a desired feature in many of the latest consumer and research projects. Therefore, using this task as motivation, we set out to build a reading comprehension model that performed well on the SQuAD 2.0 question answering dataset. To do this, we built upon the existing BiDAF  machine comprehension model given to us through the CS224n staff. Our contributions to this model are a character embedding layer on top of the existing word embedding layer, a self attention layer, and added features to the character and word embeddings which include Part of Speech tags (POS), named entity recognition (NER) tags, and dependency tags. As a result of implementing these layers we found that character embedding with additional input features performed the best with an F1 dev score of 64.38 and an EM dev score 61.29. On the test set we achieved F1 and EM scores 62.17 and 59.04 respectively.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0091", "source": "CS224N"}}
{"ai_text": "I had never broken into a car before. We were in Laredo, having just finished our first day at a Habitat for Humanity work site. The Hotchkiss volunteers had already left, off to enjoy some Texas BBQ, leaving me behind with the college kids to clean up. Not until we were stranded did we realize we were locked out of the van. Someone picked a coat hanger out of the dumpster, handed it to me, and took a few steps back. \"Can you do that thing with a coat hanger to unlock it?\" \"Why me?\" I thought. More out of amusement than optimism, I gave it a try. I slid the hanger into the window's seal like I'd seen on crime shows, and spent a few minutes jiggling the apparatus around the inside of the frame. Suddenly, two things simultaneously clicked. One was the lock on the door. (I actually succeeded in springing it.) The other was the realization that I'd been in this type of situation before. In fact, I'd been born into this type of situation. My upbringing has numbed me to unpredictability and chaos. With a family of seven, my home was loud, messy, and spottily supervised. My siblings arguing, the dog barking, the phone ringing\u2014all meant my house was functioning normally. My Dad, a retired Navy pilot, was away half the time. When he was home, he had a parenting style something like a drill sergeant. At the age of nine, I learned how to clear burning oil from the surface of water. My Dad considered this a critical life skill\u2014you know, in case my aircraft carrier should ever get torpedoed. \"The water's on fire! Clear a hole!\" he shouted, tossing me in the lake without warning. While I'm still unconvinced about that particular lesson's practicality, my Dad's overarching message is unequivocally true: much of life is unexpected, and you have to deal with the twists and turns. Living in my family, days rarely unfolded as planned. A bit overlooked, a little pushed around, I learned to roll with reality, negotiate a quick deal, and give the improbable a try. I don't sweat the small stuff, and I definitely don't expect perfect fairness. So what if our dining room table only has six chairs for seven people? Someone learns the importance of punctuality every night. But more than punctuality and a special affinity for musical chairs, my family life has taught me to thrive in situations over which I have no power. Growing up, I never controlled my older siblings, but I learned how to thwart their attempts to control me. I forged alliances, and realigned them as necessary. Sometimes, I was the poor, defenseless little brother; sometimes I was the omniscient elder. Different things to different people, as the situation demanded. I learned to adapt. Back then, these techniques were merely reactions undertaken to ensure my survival. But one day this fall, Dr. Hicks, our Head of School, asked me a question that he hoped all seniors would reflect on throughout the year: \"How can I participate in a thing I do not govern, in the company of people I did not choose?\" The question caught me off guard, much like the question posed to me in Laredo. Then, I realized I knew the answer. I knew why the coat hanger had been handed to me. Growing up as the middle child in my family, I was a vital participant in a thing I did not govern, in the company of people I did not choose. It's family. It's society. And often, it's chaos. You participate by letting go of the small stuff, not expecting order and perfection, and facing the unexpected with confidence, optimism, and preparedness. My family experience taught me to face a serendipitous world with confidence.", "human_reference": "I had never broken into a car before. We were in Laredo, having just finished our first day at a Habitat for Humanity work site. The Hotchkiss volunteers had already left, off to enjoy some Texas BBQ, leaving me behind with the college kids to clean up. Not until we were stranded did we realize we were locked out of the van. Someone picked a coat hanger out of the dumpster, handed it to me, and took a few steps back. \"Can you do that thing with a coat hanger to unlock it?\" \"Why me?\" I thought. More out of amusement than optimism, I gave it a try. I slid the hanger into the window's seal like I'd seen on crime shows, and spent a few minutes jiggling the apparatus around the inside of the frame. Suddenly, two things simultaneously clicked. One was the lock on the door. (I actually succeeded in springing it.) The other was the realization that I'd been in this type of situation before. In fact, I'd been born into this type of situation. My upbringing has numbed me to unpredictability and chaos. With a family of seven, my home was loud, messy, and spottily supervised. My siblings arguing, the dog barking, the phone ringing\u2014all meant my house was functioning normally. My Dad, a retired Navy pilot, was away half the time. When he was home, he had a parenting style something like a drill sergeant. At the age of nine, I learned how to clear burning oil from the surface of water. My Dad considered this a critical life skill\u2014you know, in case my aircraft carrier should ever get torpedoed. \"The water's on fire! Clear a hole!\" he shouted, tossing me in the lake without warning. While I'm still unconvinced about that particular lesson's practicality, my Dad's overarching message is unequivocally true: much of life is unexpected, and you have to deal with the twists and turns. Living in my family, days rarely unfolded as planned. A bit overlooked, a little pushed around, I learned to roll with reality, negotiate a quick deal, and give the improbable a try. I don't sweat the small stuff, and I definitely don't expect perfect fairness. So what if our dining room table only has six chairs for seven people? Someone learns the importance of punctuality every night. But more than punctuality and a special affinity for musical chairs, my family life has taught me to thrive in situations over which I have no power. Growing up, I never controlled my older siblings, but I learned how to thwart their attempts to control me. I forged alliances, and realigned them as necessary. Sometimes, I was the poor, defenseless little brother; sometimes I was the omniscient elder. Different things to different people, as the situation demanded. I learned to adapt. Back then, these techniques were merely reactions undertaken to ensure my survival. But one day this fall, Dr. Hicks, our Head of School, asked me a question that he hoped all seniors would reflect on throughout the year: \"How can I participate in a thing I do not govern, in the company of people I did not choose?\" The question caught me off guard, much like the question posed to me in Laredo. Then, I realized I knew the answer. I knew why the coat hanger had been handed to me. Growing up as the middle child in my family, I was a vital participant in a thing I did not govern, in the company of people I did not choose. It's family. It's society. And often, it's chaos. You participate by letting go of the small stuff, not expecting order and perfection, and facing the unexpected with confidence, optimism, and preparedness. My family experience taught me to face a serendipitous world with confidence.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0046", "source": "CollegeEssay"}}
{"ai_text": "Title: Improving Out-of-Domain Question Answering Performance with Adversarial Training\nAbstract: In this project, we aim to investigate the effectiveness of adversarial training on improving out-of-domain performance of question answering tasks. We show that finetuning a pretrained transformer with adversarial examples generated with Fast Gradient Method (FGM) using in-domain training data consistently improves the out-of-domain performance of the model. We also analyze the performance difference in terms of computation cost, memory cost and accuracy between a variety of hyperparameter configurations for adversarial training.", "human_reference": "Title: Improving Out-of-Domain Question Answering Performance with Adversarial Training\nAbstract: In this project, we aim to investigate the effectiveness of adversarial training on improving out-of-domain performance of question answering tasks. We show that finetuning a pretrained transformer with adversarial examples generated with Fast Gradient Method (FGM) using in-domain training data consistently improves the out-of-domain performance of the model. We also analyze the performance difference in terms of computation cost, memory cost and accuracy between a variety of hyperparameter configurations for adversarial training.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0041", "source": "CS224N"}}
{"ai_text": "Title: Context Demonstrations and Backtranslation Augmentation Techniques For a More Robust QA System\nAbstract: Because many real-world NLP tasks rely on user data that is not necessarily guaranteed to be in-distribution, it is critical to build robust question answering systems that can generalize to out-of-domain data. We aim to build a question answering system using context demonstrations and dataset augmentation via backtranslation on top of DistilBERT that is robust to domain shifts. Our method replicates one of the two approaches described in Gao et al. (2020), sampling and appending out-of-domain demonstrations to each training example when finetuning the model. Our method also augments the out-of-domain dataset from which demonstrations are sampled using backtranslation to generate in-distribution training examples. We find that the basic approach of simply appending randomly sampled out-of-domain demonstrations to in-domain contexts does not improve model F1 and EM score performance, but supplementing this approach by adding separator tokens between each demonstration and augmenting the out-of-domain training dataset using backtranslation improves model performance.", "human_reference": "Title: Context Demonstrations and Backtranslation Augmentation Techniques For a More Robust QA System\nAbstract: Because many real-world NLP tasks rely on user data that is not necessarily guaranteed to be in-distribution, it is critical to build robust question answering systems that can generalize to out-of-domain data. We aim to build a question answering system using context demonstrations and dataset augmentation via backtranslation on top of DistilBERT that is robust to domain shifts. Our method replicates one of the two approaches described in Gao et al. (2020), sampling and appending out-of-domain demonstrations to each training example when finetuning the model. Our method also augments the out-of-domain dataset from which demonstrations are sampled using backtranslation to generate in-distribution training examples. We find that the basic approach of simply appending randomly sampled out-of-domain demonstrations to in-domain contexts does not improve model F1 and EM score performance, but supplementing this approach by adding separator tokens between each demonstration and augmenting the out-of-domain training dataset using backtranslation improves model performance.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0054", "source": "CS224N"}}
{"ai_text": "Title: QANet for Question Answering on SQuAD2.0\nAbstract: In this project, we study the application of a QANet architecture to question answering on the SQuAD2.0 dataset. Question answering consists in training models to answer questions provided in natural language from either prodided or general context. The QANet architecture, originally presented in 2018, was a top performer on the original SQuAD dataset before the advent of pre-training. While the original SQuAD dataset only contained answerable questions, the creators of the dataset published the updated SQuAD2.0 dataset that contains unanswerable question and demonstrated that while it had little effect on human performance, it greatly reduced the effectiveness of existing models. We study how the QANet model fair on this dataset compared with a BiDAF baseline model, another high-performing model. We show that QANet's effectiveness drops, but that simple modifications to the original architecture allow significant improvements in overall performance. We also study the benefits of ensembling different architectures to improve final performance. We achieve EM and F1 scores of 63.415 and 66.734 on the test dataset.", "human_reference": "Title: QANet for Question Answering on SQuAD2.0\nAbstract: In this project, we study the application of a QANet architecture to question answering on the SQuAD2.0 dataset. Question answering consists in training models to answer questions provided in natural language from either prodided or general context. The QANet architecture, originally presented in 2018, was a top performer on the original SQuAD dataset before the advent of pre-training. While the original SQuAD dataset only contained answerable questions, the creators of the dataset published the updated SQuAD2.0 dataset that contains unanswerable question and demonstrated that while it had little effect on human performance, it greatly reduced the effectiveness of existing models. We study how the QANet model fair on this dataset compared with a BiDAF baseline model, another high-performing model. We show that QANet's effectiveness drops, but that simple modifications to the original architecture allow significant improvements in overall performance. We also study the benefits of ensembling different architectures to improve final performance. We achieve EM and F1 scores of 63.415 and 66.734 on the test dataset.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0083", "source": "CS224N"}}
{"ai_text": "Title: Improving the Robustness of QA Systems through Data Augmentation and Mixture of Experts\nAbstract: Despite the stunning achievements of question answering (QA) systems in recent years, existing neural models tend to fail when they generalize beyond the in-domain distributions. This project seeks to improve the robustness of these QA systems to unseen domains through a combination of Easy Data Augmentation (EDA) and Mixture of Experts (MoE) techniques.  As baseline, we finetuned a pre-trained DistilBERT model with Natural Questions, NewsQA and SQuAD datasets using the default configurations and evaluated the model performance on the out-of-domain datasets, including RelationExtraction, DuoRC, and RACE. After obtaining our second baseline by including a small number of training examples from our out-of-domain datasets, we ran two rounds of hyperparameters tuning through random search. Based on the best performing set of hyperparameters, we then augmented our out-of-domain datasets using the EDA techniques and analyzed the effects of each technique through a series of experiments. Finally, we implemented an MoE model with three experts and a two-layer bi-directional LSTM followed by a linear layer as the gating function.  Both the data augmentation technique and the mixture-of-expert approach demonstrated capability to improve the robustness of DistilBERT-based QA systems, and a combination of the two methods brings even further improvement. The combined approach increased the F1 and EM scores on the dev set by 15.03% and 14.87%, respectively, compared to the baseline, and achieved an F1 score of 62.062 and an EM score of 42.317 on the test leaderboard.", "human_reference": "Title: Improving the Robustness of QA Systems through Data Augmentation and Mixture of Experts\nAbstract: Despite the stunning achievements of question answering (QA) systems in recent years, existing neural models tend to fail when they generalize beyond the in-domain distributions. This project seeks to improve the robustness of these QA systems to unseen domains through a combination of Easy Data Augmentation (EDA) and Mixture of Experts (MoE) techniques.  As baseline, we finetuned a pre-trained DistilBERT model with Natural Questions, NewsQA and SQuAD datasets using the default configurations and evaluated the model performance on the out-of-domain datasets, including RelationExtraction, DuoRC, and RACE. After obtaining our second baseline by including a small number of training examples from our out-of-domain datasets, we ran two rounds of hyperparameters tuning through random search. Based on the best performing set of hyperparameters, we then augmented our out-of-domain datasets using the EDA techniques and analyzed the effects of each technique through a series of experiments. Finally, we implemented an MoE model with three experts and a two-layer bi-directional LSTM followed by a linear layer as the gating function.  Both the data augmentation technique and the mixture-of-expert approach demonstrated capability to improve the robustness of DistilBERT-based QA systems, and a combination of the two methods brings even further improvement. The combined approach increased the F1 and EM scores on the dev set by 15.03% and 14.87%, respectively, compared to the baseline, and achieved an F1 score of 62.062 and an EM score of 42.317 on the test leaderboard.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0089", "source": "CS224N"}}
{"ai_text": "Title: Embedding and Attending: Two Hearts that Beat as One\nAbstract: Neural attention mechanisms have proven to be effective at leveraging relevant tokens of the input data to more accurately predict output words. Moreover, incorporating additional embedding information significantly boosts performance and provides greater granularity of tokens at the character and word level. For these reasons, we focused on implementing various models that concern primarily the embedding layer and attention layers. In our project, we implemented three different attention mechanisms (co-attention from Dynamic Coattention Networks, key-query-value self-attention, and R-Net self-attention) in the domain of the Question-Answering (QA) paradigm. Our goal was to produce a model that is highly performant compared to the baseline BiDAF model on the Stanford Questioning Answering Dataset (SQuAD 2.0). We combined these attention mechanisms with character-level embeddings to provide more local contextual information, and finally enhanced these embeddings by including additional input features (part-of-speech and lemmatized forms of words). Lastly, we conducted a series of hyperparameter tuning experiments to determine the ideal hyperparameters that result in the greatest F1/EM scores. Augmenting the baseline with these techniques produced a significant improvement compared to the baseline. Our most performant model obtained an F1 score of 65.27 and EM score of 61.77 (an increase of 5.6% and 5.5%, respectively).", "human_reference": "Title: Embedding and Attending: Two Hearts that Beat as One\nAbstract: Neural attention mechanisms have proven to be effective at leveraging relevant tokens of the input data to more accurately predict output words. Moreover, incorporating additional embedding information significantly boosts performance and provides greater granularity of tokens at the character and word level. For these reasons, we focused on implementing various models that concern primarily the embedding layer and attention layers. In our project, we implemented three different attention mechanisms (co-attention from Dynamic Coattention Networks, key-query-value self-attention, and R-Net self-attention) in the domain of the Question-Answering (QA) paradigm. Our goal was to produce a model that is highly performant compared to the baseline BiDAF model on the Stanford Questioning Answering Dataset (SQuAD 2.0). We combined these attention mechanisms with character-level embeddings to provide more local contextual information, and finally enhanced these embeddings by including additional input features (part-of-speech and lemmatized forms of words). Lastly, we conducted a series of hyperparameter tuning experiments to determine the ideal hyperparameters that result in the greatest F1/EM scores. Augmenting the baseline with these techniques produced a significant improvement compared to the baseline. Our most performant model obtained an F1 score of 65.27 and EM score of 61.77 (an increase of 5.6% and 5.5%, respectively).", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0076", "source": "CS224N"}}
{"ai_text": "I think I will go with the latter one. I'm not saying that studying a subject for job opportunities is wrong, it's just that I'm not that kind of person. Me myself want to be a scientist in the future, and following my own interests are rather important, because doing research can be tedious or frustrating in many situations, and my interests may be the only thing to keep me going on and on. If you are only driven by profit, it's likely that you will abandon your current subject once it seems not so profitable, and that's clearly not good for the development of science.", "human_reference": "I think I will go with the latter one. I'm not saying that studying a subject for job opportunities is wrong, it's just that I'm not that kind of person. Me myself want to be a scientist in the future, and following my own interests are rather important, because doing research can be tedious or frustrating in many situations, and my interests may be the only thing to keep me going on and on. If you are only driven by profit, it's likely that you will abandon your current subject once it seems not so profitable, and that's clearly not good for the development of science.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0057", "source": "TOEFL11"}}
{"ai_text": "Hurricane Maria devastated Puerto Rico, and my Massachusetts city had also been torn apart. In a city where nearly half of the population is Puerto Rican, the destination of people fleeing the island had immediately come into question, \u201cWhy are they coming here? Our schools already don\u2019t have enough books for our students, never mind Puerto Rican refugees.\u201d These are words out of my French and Irish uncle\u2019s mouth. As he looked in my brown eyes and proclaimed his son\u2019s lack of an AP English book was more important than the life and well-being of a child that looks like me. It is enlightening to begin to take notice of the ignorance that surrounds your identity. It is eye-opening to hear words of hate and intolerance spew from the mouths of people you love, people who claim to love you. I have heard people express how they really feel when they forget about my dark complexion and let a joke slip, to follow up with, \"Well not you, you're not really Puerto Rican.\" To be seven years old and shrouded in a feeling of discomfort for who you are; making an effort to sound and act \u201cwhite\u201d among my white family and friends. Thanksgiving with my blue-eyed and freckled cousins was an event that displaced me. My Abuela\u2019s house was where my Puerto Rican cousins flourished. They spoke fluent Spanish and shook their heads when I asked what they were saying. I \u201cdidn\u2019t care\u201d about my culture to them. It is in this limbo that I find myself more aware of the dubious eyes on me when I\u2019m asked if I am Muslim or Italian (as if Muslim is an ethnicity). When they compliment my \u201cdifferent\u201d name, their eyes widen when they learn that I am from the \u201cwhiter\u201d side of the city, but nod in understanding when I clarify that my Mother is white. I notice that these glances are consonant with the fact that the grocery store I work at in the neighboring town made thousands of dollars in their donation cups for Hurricane Harvey victims, but not one mention of Puerto Rico\u2019s disastrous conditions. It is from these glances that I realize both these adversities are not of equal importance to the store where I was one of four Hispanic employees. I am Puerto Rican and Irish and French and Polish and all these backgrounds have allowed me to see unique perspectives, but they are not a single definition of me. I am a daughter, a student, a friend, a sister. I am everything I love and every book I've read and all the people I've helped and all the places I've traveled. I am all of my passions and the closed minds I intend on opening and the thirst for life I intend on quenching. I have grown up with a feeling of exclusion from both sides of my heritage, yet in the process of fighting for a sense of belonging I have embraced myself for more than the color of my skin or the accent of my father. My identity is so much more than an uncomfortable glance from a person who can't place my nose with a nation. I am more than a prejudice comment. What I have truly come to understand by living at the intersection of two very different situations is how ignorance develops so easily from not being able to empathize. My white uncle will never know what it is like to be a minority. He will never feel the stares I have felt, he will never be called a spic, he will never be disadvantaged for his light complexion. It is only when people place themselves as close as possible to the reality of others do they begin to rid themselves of the subconscious prejudices our society places upon us.", "human_reference": "Hurricane Maria devastated Puerto Rico, and my Massachusetts city had also been torn apart. In a city where nearly half of the population is Puerto Rican, the destination of people fleeing the island had immediately come into question, \u201cWhy are they coming here? Our schools already don\u2019t have enough books for our students, never mind Puerto Rican refugees.\u201d These are words out of my French and Irish uncle\u2019s mouth. As he looked in my brown eyes and proclaimed his son\u2019s lack of an AP English book was more important than the life and well-being of a child that looks like me. It is enlightening to begin to take notice of the ignorance that surrounds your identity. It is eye-opening to hear words of hate and intolerance spew from the mouths of people you love, people who claim to love you. I have heard people express how they really feel when they forget about my dark complexion and let a joke slip, to follow up with, \"Well not you, you're not really Puerto Rican.\" To be seven years old and shrouded in a feeling of discomfort for who you are; making an effort to sound and act \u201cwhite\u201d among my white family and friends. Thanksgiving with my blue-eyed and freckled cousins was an event that displaced me. My Abuela\u2019s house was where my Puerto Rican cousins flourished. They spoke fluent Spanish and shook their heads when I asked what they were saying. I \u201cdidn\u2019t care\u201d about my culture to them. It is in this limbo that I find myself more aware of the dubious eyes on me when I\u2019m asked if I am Muslim or Italian (as if Muslim is an ethnicity). When they compliment my \u201cdifferent\u201d name, their eyes widen when they learn that I am from the \u201cwhiter\u201d side of the city, but nod in understanding when I clarify that my Mother is white. I notice that these glances are consonant with the fact that the grocery store I work at in the neighboring town made thousands of dollars in their donation cups for Hurricane Harvey victims, but not one mention of Puerto Rico\u2019s disastrous conditions. It is from these glances that I realize both these adversities are not of equal importance to the store where I was one of four Hispanic employees. I am Puerto Rican and Irish and French and Polish and all these backgrounds have allowed me to see unique perspectives, but they are not a single definition of me. I am a daughter, a student, a friend, a sister. I am everything I love and every book I've read and all the people I've helped and all the places I've traveled. I am all of my passions and the closed minds I intend on opening and the thirst for life I intend on quenching. I have grown up with a feeling of exclusion from both sides of my heritage, yet in the process of fighting for a sense of belonging I have embraced myself for more than the color of my skin or the accent of my father. My identity is so much more than an uncomfortable glance from a person who can't place my nose with a nation. I am more than a prejudice comment. What I have truly come to understand by living at the intersection of two very different situations is how ignorance develops so easily from not being able to empathize. My white uncle will never know what it is like to be a minority. He will never feel the stares I have felt, he will never be called a spic, he will never be disadvantaged for his light complexion. It is only when people place themselves as close as possible to the reality of others do they begin to rid themselves of the subconscious prejudices our society places upon us.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0009", "source": "CollegeEssay"}}
{"ai_text": "I'd like to talk to Alan Shore from the TV series Boston Legal. He is the kind of person I admire. He is decent, a man of his word, one of the very few that I regard as having a strong sense of justice. Yet he is not bound up by the rules and knows when to break them to achieve the ultimate good. And he is interesting and eloquent, all the things that I desire to have. I want to talk to him about how to balance when you want to have principles and yet not bound up by them at the same time.", "human_reference": "I'd like to talk to Alan Shore from the TV series Boston Legal. He is the kind of person I admire. He is decent, a man of his word, one of the very few that I regard as having a strong sense of justice. Yet he is not bound up by the rules and knows when to break them to achieve the ultimate good. And he is interesting and eloquent, all the things that I desire to have. I want to talk to him about how to balance when you want to have principles and yet not bound up by them at the same time.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0031", "source": "TOEFL11"}}
{"ai_text": "I have a fetish for writing. I\u2019m not talking about crafting prose or verses, or even sentences out of words. But simply constructing letters and characters from strokes of ink gives me immense satisfaction. It\u2019s not quite calligraphy, as I don\u2019t use calligraphic pens or Chinese writing brushes; I prefer it simple, spontaneous, and subconscious. I often find myself crafting characters in the margins of notebooks with a fifty-cent pencil, or tracing letters out of thin air with anything from chopsticks to fingertips. The art of handwriting is a relic in the information era. Why write when one can type? Perhaps the Chinese had an answer before the advent of keyboards. \u201cOne\u2019s handwriting,\u201d said the ancient Chinese, \u201cis a painting of one\u2019s mind.\u201d After all, when I practice my handwriting, I am crafting characters. My character. I particularly enjoy meticulously designing a character, stroke by stroke, and eventually building up, letter by letter, to a quote person\u00adalized in my own voice. Every movement of the pen and every drop\u00adlet of ink all lead to something profound, as if the arches of every \"m\" are doorways to revelations. After all, characters are the build\u00ading blocks of language, and language is the only vehicle through which knowledge unfolds. Thus, in a way, these letters under my pen are themselves representations of knowledge, and the delicate beauty of every letter proves, visually, the intrinsic beauty of know\u00ading. I suppose handwriting reminds me of my conviction in this vi\u00adsual manner: through learning answers are found, lives enriched, and societies bettered. Moreover, perhaps this strange passion in polishing every single character of a word delineates my dedication to learning, testifies my zeal for my conviction, and sketches a crucial stroke of my character. \"We--must--know ... \" the mathematician David Hilbert's voice echoes in resolute cursive at the tip of my pen, as he, addressing German scientists in 1930, propounds the goal of modern intellectu\u00adals. My pen firmly nods in agreement with Hilbert, while my mind again fumbles for the path to knowledge. The versatility of handwriting enthralls me. The Chinese devel\u00adoped many styles -- called hands -- of writing. Fittingly, each hand seems to parallel one of my many academic interests. Characters of the Regular Hand (kai shu), a legible script, serve me well during many long hours when I scratch my head and try to prove a mathematical statement rigorously, as the legibility illuminates my logic on paper. Words of the Running Hand (xing shu), a semi-cursive script, are like the passionate words that I speak before a committee of Model United Nations delegates, propounding a decisive course of action: the words, both spoken and written, are swift and coherent but resolute and emphatic. And strokes of the Cursive Hand (cao shu) resemble those sudden artistic sparks when I deliver a line on stage: free spontaneous, but emphatic syllables travel through the lights like rivers of ink flowing on the page. Yet the fact that the three distinctive hands cooperate so seamlessly, fusing together the glorious culture of writing, is perhaps a fable of learning, a testament that the many talents of the Renaissance Man could all be worthwhile for enriching human society. Such is my methodology: just like I organize my different hands into a neat personal style with my fetish for writing, I can unify my broad interests with my passion for learning. \u201c...We -- will -- know!\u201d Hilbert finishes his adage, as I frantically slice an exclamation mark as the final stroke of this painting of my mind. I must know: for knowing, like well-crafted letters, has an inherent beauty and an intrinsic value. I will know: for my versatile interests in academics will flow like my versatile styles of writing. I must know and I will know: for my fetish for writing is a fetish for learning.", "human_reference": "I have a fetish for writing. I\u2019m not talking about crafting prose or verses, or even sentences out of words. But simply constructing letters and characters from strokes of ink gives me immense satisfaction. It\u2019s not quite calligraphy, as I don\u2019t use calligraphic pens or Chinese writing brushes; I prefer it simple, spontaneous, and subconscious. I often find myself crafting characters in the margins of notebooks with a fifty-cent pencil, or tracing letters out of thin air with anything from chopsticks to fingertips. The art of handwriting is a relic in the information era. Why write when one can type? Perhaps the Chinese had an answer before the advent of keyboards. \u201cOne\u2019s handwriting,\u201d said the ancient Chinese, \u201cis a painting of one\u2019s mind.\u201d After all, when I practice my handwriting, I am crafting characters. My character. I particularly enjoy meticulously designing a character, stroke by stroke, and eventually building up, letter by letter, to a quote person\u00adalized in my own voice. Every movement of the pen and every drop\u00adlet of ink all lead to something profound, as if the arches of every \"m\" are doorways to revelations. After all, characters are the build\u00ading blocks of language, and language is the only vehicle through which knowledge unfolds. Thus, in a way, these letters under my pen are themselves representations of knowledge, and the delicate beauty of every letter proves, visually, the intrinsic beauty of know\u00ading. I suppose handwriting reminds me of my conviction in this vi\u00adsual manner: through learning answers are found, lives enriched, and societies bettered. Moreover, perhaps this strange passion in polishing every single character of a word delineates my dedication to learning, testifies my zeal for my conviction, and sketches a crucial stroke of my character. \"We--must--know ... \" the mathematician David Hilbert's voice echoes in resolute cursive at the tip of my pen, as he, addressing German scientists in 1930, propounds the goal of modern intellectu\u00adals. My pen firmly nods in agreement with Hilbert, while my mind again fumbles for the path to knowledge. The versatility of handwriting enthralls me. The Chinese devel\u00adoped many styles -- called hands -- of writing. Fittingly, each hand seems to parallel one of my many academic interests. Characters of the Regular Hand (kai shu), a legible script, serve me well during many long hours when I scratch my head and try to prove a mathematical statement rigorously, as the legibility illuminates my logic on paper. Words of the Running Hand (xing shu), a semi-cursive script, are like the passionate words that I speak before a committee of Model United Nations delegates, propounding a decisive course of action: the words, both spoken and written, are swift and coherent but resolute and emphatic. And strokes of the Cursive Hand (cao shu) resemble those sudden artistic sparks when I deliver a line on stage: free spontaneous, but emphatic syllables travel through the lights like rivers of ink flowing on the page. Yet the fact that the three distinctive hands cooperate so seamlessly, fusing together the glorious culture of writing, is perhaps a fable of learning, a testament that the many talents of the Renaissance Man could all be worthwhile for enriching human society. Such is my methodology: just like I organize my different hands into a neat personal style with my fetish for writing, I can unify my broad interests with my passion for learning. \u201c...We -- will -- know!\u201d Hilbert finishes his adage, as I frantically slice an exclamation mark as the final stroke of this painting of my mind. I must know: for knowing, like well-crafted letters, has an inherent beauty and an intrinsic value. I will know: for my versatile interests in academics will flow like my versatile styles of writing. I must know and I will know: for my fetish for writing is a fetish for learning.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0047", "source": "CollegeEssay"}}
{"ai_text": "When I was 16, I lived with the Watkins family in Wichita, Kansas. Mrs. Watkins was the coordinator of the foreign exchange student program I was enrolled in. She had a nine year old son named Cody. I would babysit Cody every day after school for at least two to three hours. We would play Scrabble or he would read to me from Charlotte\u2019s Web or The Ugly Duckling. He would talk a lot about his friends and school life, and I would listen to him and ask him the meanings of certain words. He was my first friend in the New World. My second family was the Martinez family, who were friends of the Watkins\u2019s. The host dad Michael was a high school English teacher and the host mom Jennifer (who had me call her \u201cJen\u201d) taught elementary school. She had recently delivered a baby, so she was still in the hospital when I moved into their house. The Martinez family did almost everything together. We made pizza together, watched Shrek on their cozy couch together, and went fishing on Sunday together. On rainy days, Michael, Jen and I would sit on the porch and listen to the rain, talking about our dreams and thoughts. Within two months I was calling them mom and dad. After I finished the exchange student program, I had the option of returning to Korea but I decided to stay in America. I wanted to see new places and meet different people. Since I wasn\u2019t an exchange student anymore, I had the freedom--and burden--of finding a new school and host family on my own. After a few days of thorough investigation, I found the Struiksma family in California. They were a unique group. The host mom Shellie was a single mom who had two of her own sons and two Russian daughters that she had adopted. The kids always had something warm to eat, and were always on their best behavior at home and in school. It would be fair to say that this was all due to Shellie\u2019s upbringing. My room was on the first floor, right in front of Shellie\u2019s hair salon, a small business that she ran out of her home. In the living room were six or seven huge amplifiers and a gigantic chandelier hung from the high ceiling. The kitchen had a bar. At first, the non-stop visits from strangers made me nervous, but soon I got used to them. I remember one night, a couple barged into my room while I was sleeping. It was awkward. After a few months I realized we weren\u2019t the best fit. In the nicest way possible, I told them I had to leave. They understood. The Ortiz family was my fourth family. Kimberly, the host mom, treated me the same way she treated her own son. She made me do chores: I fixed dinner, fed their two dogs Sassy and Lady, and once a week I cleaned the bathroom. I also had to follow some rules: No food in my room, no using the family computer, no lights on after midnight, and no ride unless it was an emergency. The first couple of months were really hard to get used to, but eventually I adjusted. I lived with the Ortiz family for seven months like a monk in the deep forest. However, the host dad Greg\u2019s asthma got worse after winter, so he wanted to move to the countryside. It was unexpected and I only had a week to find a new host family. I asked my friend Danielle if I could live with her until I found a new home. That\u2019s how I met the Dirksen family, my fifth family. The Dirksen family had three kids. They were all different. Danielle liked bitter black coffee, Christian liked energy drinks, and Becca liked sweet lemon tea. Dawn, the host mom didn\u2019t like winter, and Mark, the host dad, didn\u2019t like summer. After dinner, we would all play Wii Sports together. I was the king of bowling, and Dawn was the queen of tennis. I don\u2019t remember a single time that they argued about the games. Afterward, we would gather in the living room and Danielle would play the piano while the rest of us sang hymns. Of course, those 28 months were too short to fully understand all five families, but I learned from and was shaped by each of them. By teaching me English, nine year-old Cody taught me the importance of being able to learn from anyone; the Martinez family showed me the value of spending time together as a family; the Struiksma family taught me to reserve judgment about divorced women and adopted children; Mrs. Ortiz taught me the value of discipline and the Dirksen family taught me the importance of appreciating one another\u2019s different qualities. Getting along with other people is necessary for anyone and living with five families has made me more sensitive to others\u2019 needs: I have learned how to recognize when someone needs to talk, when I should give advice and when to simply listen, and when someone needs to be left alone; in the process, I have become much more adaptable. I\u2019m ready to change, learn, and be shaped by my future families.", "human_reference": "When I was 16, I lived with the Watkins family in Wichita, Kansas. Mrs. Watkins was the coordinator of the foreign exchange student program I was enrolled in. She had a nine year old son named Cody. I would babysit Cody every day after school for at least two to three hours. We would play Scrabble or he would read to me from Charlotte\u2019s Web or The Ugly Duckling. He would talk a lot about his friends and school life, and I would listen to him and ask him the meanings of certain words. He was my first friend in the New World. My second family was the Martinez family, who were friends of the Watkins\u2019s. The host dad Michael was a high school English teacher and the host mom Jennifer (who had me call her \u201cJen\u201d) taught elementary school. She had recently delivered a baby, so she was still in the hospital when I moved into their house. The Martinez family did almost everything together. We made pizza together, watched Shrek on their cozy couch together, and went fishing on Sunday together. On rainy days, Michael, Jen and I would sit on the porch and listen to the rain, talking about our dreams and thoughts. Within two months I was calling them mom and dad. After I finished the exchange student program, I had the option of returning to Korea but I decided to stay in America. I wanted to see new places and meet different people. Since I wasn\u2019t an exchange student anymore, I had the freedom--and burden--of finding a new school and host family on my own. After a few days of thorough investigation, I found the Struiksma family in California. They were a unique group. The host mom Shellie was a single mom who had two of her own sons and two Russian daughters that she had adopted. The kids always had something warm to eat, and were always on their best behavior at home and in school. It would be fair to say that this was all due to Shellie\u2019s upbringing. My room was on the first floor, right in front of Shellie\u2019s hair salon, a small business that she ran out of her home. In the living room were six or seven huge amplifiers and a gigantic chandelier hung from the high ceiling. The kitchen had a bar. At first, the non-stop visits from strangers made me nervous, but soon I got used to them. I remember one night, a couple barged into my room while I was sleeping. It was awkward. After a few months I realized we weren\u2019t the best fit. In the nicest way possible, I told them I had to leave. They understood. The Ortiz family was my fourth family. Kimberly, the host mom, treated me the same way she treated her own son. She made me do chores: I fixed dinner, fed their two dogs Sassy and Lady, and once a week I cleaned the bathroom. I also had to follow some rules: No food in my room, no using the family computer, no lights on after midnight, and no ride unless it was an emergency. The first couple of months were really hard to get used to, but eventually I adjusted. I lived with the Ortiz family for seven months like a monk in the deep forest. However, the host dad Greg\u2019s asthma got worse after winter, so he wanted to move to the countryside. It was unexpected and I only had a week to find a new host family. I asked my friend Danielle if I could live with her until I found a new home. That\u2019s how I met the Dirksen family, my fifth family. The Dirksen family had three kids. They were all different. Danielle liked bitter black coffee, Christian liked energy drinks, and Becca liked sweet lemon tea. Dawn, the host mom didn\u2019t like winter, and Mark, the host dad, didn\u2019t like summer. After dinner, we would all play Wii Sports together. I was the king of bowling, and Dawn was the queen of tennis. I don\u2019t remember a single time that they argued about the games. Afterward, we would gather in the living room and Danielle would play the piano while the rest of us sang hymns. Of course, those 28 months were too short to fully understand all five families, but I learned from and was shaped by each of them. By teaching me English, nine year-old Cody taught me the importance of being able to learn from anyone; the Martinez family showed me the value of spending time together as a family; the Struiksma family taught me to reserve judgment about divorced women and adopted children; Mrs. Ortiz taught me the value of discipline and the Dirksen family taught me the importance of appreciating one another\u2019s different qualities. Getting along with other people is necessary for anyone and living with five families has made me more sensitive to others\u2019 needs: I have learned how to recognize when someone needs to talk, when I should give advice and when to simply listen, and when someone needs to be left alone; in the process, I have become much more adaptable. I\u2019m ready to change, learn, and be shaped by my future families.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0055", "source": "CollegeEssay"}}
{"ai_text": "Studying in big cities definitely works with me. I went to college in Beijing, which is one of the biggest cities of China. To be honest I didn't care for this city in the beginning, but its charm grows on me. We know that going to college is not all about gaining knowledge from textbooks, getting prepared for entering the society and meeting people are also important. Beijing is an excellent city for these things. It's full of opportunities and passionate people. Plus, most of the best universities are in big cities, so studying here is good for your academic life as well.", "human_reference": "Studying in big cities definitely works with me. I went to college in Beijing, which is one of the biggest cities of China. To be honest I didn't care for this city in the beginning, but its charm grows on me. We know that going to college is not all about gaining knowledge from textbooks, getting prepared for entering the society and meeting people are also important. Beijing is an excellent city for these things. It's full of opportunities and passionate people. Plus, most of the best universities are in big cities, so studying here is good for your academic life as well.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0083", "source": "TOEFL11"}}
{"ai_text": "Title: Improving the Performance of Previous QA Models\nAbstract: Question answering is a challenging problem that tests language processing models the ability to comprehend natural languages. In this project, we implemented two models, BiDAF and QANet, to solve the Stanford question answering dataset (SQuAD) 2.0. We experienced different methods to improve the performance of these models, including adding character embedding layers, data augmentation, and ensemble modeling. Finally, we compared the result across different experiments and gave an analysis of our models. In the end, our best model achieved F1/EM score of 68.71/65.38 in the test leaderboard.", "human_reference": "Title: Improving the Performance of Previous QA Models\nAbstract: Question answering is a challenging problem that tests language processing models the ability to comprehend natural languages. In this project, we implemented two models, BiDAF and QANet, to solve the Stanford question answering dataset (SQuAD) 2.0. We experienced different methods to improve the performance of these models, including adding character embedding layers, data augmentation, and ensemble modeling. Finally, we compared the result across different experiments and gave an analysis of our models. In the end, our best model achieved F1/EM score of 68.71/65.38 in the test leaderboard.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0100", "source": "CS224N"}}
{"ai_text": "Title: Robust Question Answering via In-domain Adversarial Training and Out-domain Data Augmentation\nAbstract: How can a Question Answering model trained on Wikipedia solve examination questions correctly? The cross-domain Question Answering is challenging since QA models are usually not robust to generalize well on out-of-domain datasets. We would like to explore the effectiveness of domain-related information on QA model robustness. We leverage potential domain information, both domain-specific and domain-invariant, from the text data. During training on the in-domain training set, we explore the adversarial training by experimenting on three adversarial functions. We add a domain classifier to distinguish different domains. Meanwhile, the QA model fools the domain discriminator to learn domain-invariant feature representations from the in-domain training set. In addition to the domain-invariant learning from the in-domain training, we also propose a data augmentation method that can retain high-level domain information by using named entity recognition and synonyms replacement. Out -of-domain datasets are insufficient and we want to utilize them most. This augmentation method is applied on the oo-domain training set and we suppose that it will let the model learn domain specific information from the out-of-domain datasets. To give better insights on our adversarial training and augmentation methods, we conducted several experiments and provide our analysis in this report.", "human_reference": "Title: Robust Question Answering via In-domain Adversarial Training and Out-domain Data Augmentation\nAbstract: How can a Question Answering model trained on Wikipedia solve examination questions correctly? The cross-domain Question Answering is challenging since QA models are usually not robust to generalize well on out-of-domain datasets. We would like to explore the effectiveness of domain-related information on QA model robustness. We leverage potential domain information, both domain-specific and domain-invariant, from the text data. During training on the in-domain training set, we explore the adversarial training by experimenting on three adversarial functions. We add a domain classifier to distinguish different domains. Meanwhile, the QA model fools the domain discriminator to learn domain-invariant feature representations from the in-domain training set. In addition to the domain-invariant learning from the in-domain training, we also propose a data augmentation method that can retain high-level domain information by using named entity recognition and synonyms replacement. Out -of-domain datasets are insufficient and we want to utilize them most. This augmentation method is applied on the oo-domain training set and we suppose that it will let the model learn domain specific information from the out-of-domain datasets. To give better insights on our adversarial training and augmentation methods, we conducted several experiments and provide our analysis in this report.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0124", "source": "CS224N"}}
{"ai_text": "Garishly lined with a pearlescent lavender, my eyes idly scanned the haphazard desk in front of me, settling on a small kohl. I packed the ebony powder into my waterline with a shaky hand, wincing at the fine specks making their way into my eyes. The palette's colors bore in, the breadth of my imagination interwoven into now-brittle brushes. The girl in the mirror seemed sharper, older, somehow. At only 12, I was relatively new to the powders and blushes that lined my birthday makeup kit, but I was determined to decipher the deep splashes of color that had for so long been an enigma to me. After school involved self-inflicted solitary confinement, as I shut myself in my bedroom to hone my skills. The palette\u2019s colors bore in, the breadth of my imagination interwoven into now-brittle brushes. Much to my chagrin, my mom walked in one day, amused at my smudged lipstick, which congealed on the wispy hairs that lined my upper lip. \u201cHalloween already?\u201d she asked playfully. I flushed in embarrassment as she got to work, smoothing my skin with a brush and filling the gaps in my squiggly liner. Becoming a makeup aficionado was going to take some help. \u201cWhat\u2019s this even made of?\u201d I asked, transfixed by the bright powder she was smattering on my cheeks. \u201cYou know, I\u2019m not sure,\u201d she murmured. \u201cMaybe you should find out.\u201d I did. Hours down the internet rabbit hole, I learned that the shimmery powder was made of mica, a mineral commonly used in cosmetics. While the substance was dazzling, its production process was steeped in humanitarian violations and environmental damage. Determined to reconcile my burgeoning love for makeup with my core values, I flung the kit into the corner of my drawer, vowing to find a more sustainable alternative. Yes, I was every bit as dramatic as you imagine it. Now 17, I approach ethical makeup with assured deliberation. As I glance at my dusty kit, which still sits where I left it, I harken back on the journey it has taken me on. Without the reckoning that it spurred, makeup would still simply be a tool of physical transformation, rather than a catalyst of personal growth. Now, each swipe of eyeliner is a stroke of my pen across paper as I write a children\u2019s book about conscious consumerism. My flitting fingers programmatically place sparkles, mattes, and tints across my face in the same way that they feverishly move across a keyboard, watching algorithms and graphs integrate into models of supply chain transparency. Makeup has taught me to be unflinching, both in self expression and my expectations for the future. I coat my lips with a bold sheen, preparing them to form words of unequivocal urgency at global conferences and casual discussions. I see my passion take flight, emboldening others to approach their own reckonings, uncomfortable as they may be. I embark on a two-year journey of not buying new clothes in a statement against mass consumption and rally youth into a unified organization. We stand together, picking at the gritty knots of makeup, corporate accountability, and sustainability as they slowly unravel. Deep rooted journeys of triumph and tribulation are plastered across the surface of my skin \u2014 this paradox excites me. I\u2019m not sure why makeup transfixes me. Perhaps it\u2019s because I enjoy seeing my reveries take shape. Yukta, the wannabe Wicked Witch of the West, has lids coated with emerald luster and lips of coal. Yukta, the Indian classical dancer, wields thick eyeliner and bright crimson lipstick that allow her expressions to be amplified across a stage. Deep rooted journeys of triumph and tribulation are plastered across the surface of my skin \u2014 this paradox excites me. Perhaps I am also drawn to makeup because as I peel back the layers, I am still wholly me. I am still the young girl staring wide-eyed at her reflection, earnestly questioning in an attempt to learn more about the world. Most importantly, I still carry an unflagging vigor to coalesce creativity and activism into palpable change, one brushstroke at a time.", "human_reference": "Garishly lined with a pearlescent lavender, my eyes idly scanned the haphazard desk in front of me, settling on a small kohl. I packed the ebony powder into my waterline with a shaky hand, wincing at the fine specks making their way into my eyes. The palette's colors bore in, the breadth of my imagination interwoven into now-brittle brushes. The girl in the mirror seemed sharper, older, somehow. At only 12, I was relatively new to the powders and blushes that lined my birthday makeup kit, but I was determined to decipher the deep splashes of color that had for so long been an enigma to me. After school involved self-inflicted solitary confinement, as I shut myself in my bedroom to hone my skills. The palette\u2019s colors bore in, the breadth of my imagination interwoven into now-brittle brushes. Much to my chagrin, my mom walked in one day, amused at my smudged lipstick, which congealed on the wispy hairs that lined my upper lip. \u201cHalloween already?\u201d she asked playfully. I flushed in embarrassment as she got to work, smoothing my skin with a brush and filling the gaps in my squiggly liner. Becoming a makeup aficionado was going to take some help. \u201cWhat\u2019s this even made of?\u201d I asked, transfixed by the bright powder she was smattering on my cheeks. \u201cYou know, I\u2019m not sure,\u201d she murmured. \u201cMaybe you should find out.\u201d I did. Hours down the internet rabbit hole, I learned that the shimmery powder was made of mica, a mineral commonly used in cosmetics. While the substance was dazzling, its production process was steeped in humanitarian violations and environmental damage. Determined to reconcile my burgeoning love for makeup with my core values, I flung the kit into the corner of my drawer, vowing to find a more sustainable alternative. Yes, I was every bit as dramatic as you imagine it. Now 17, I approach ethical makeup with assured deliberation. As I glance at my dusty kit, which still sits where I left it, I harken back on the journey it has taken me on. Without the reckoning that it spurred, makeup would still simply be a tool of physical transformation, rather than a catalyst of personal growth. Now, each swipe of eyeliner is a stroke of my pen across paper as I write a children\u2019s book about conscious consumerism. My flitting fingers programmatically place sparkles, mattes, and tints across my face in the same way that they feverishly move across a keyboard, watching algorithms and graphs integrate into models of supply chain transparency. Makeup has taught me to be unflinching, both in self expression and my expectations for the future. I coat my lips with a bold sheen, preparing them to form words of unequivocal urgency at global conferences and casual discussions. I see my passion take flight, emboldening others to approach their own reckonings, uncomfortable as they may be. I embark on a two-year journey of not buying new clothes in a statement against mass consumption and rally youth into a unified organization. We stand together, picking at the gritty knots of makeup, corporate accountability, and sustainability as they slowly unravel. Deep rooted journeys of triumph and tribulation are plastered across the surface of my skin \u2014 this paradox excites me. I\u2019m not sure why makeup transfixes me. Perhaps it\u2019s because I enjoy seeing my reveries take shape. Yukta, the wannabe Wicked Witch of the West, has lids coated with emerald luster and lips of coal. Yukta, the Indian classical dancer, wields thick eyeliner and bright crimson lipstick that allow her expressions to be amplified across a stage. Deep rooted journeys of triumph and tribulation are plastered across the surface of my skin \u2014 this paradox excites me. Perhaps I am also drawn to makeup because as I peel back the layers, I am still wholly me. I am still the young girl staring wide-eyed at her reflection, earnestly questioning in an attempt to learn more about the world. Most importantly, I still carry an unflagging vigor to coalesce creativity and activism into palpable change, one brushstroke at a time.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0021", "source": "CollegeEssay"}}
{"ai_text": "I know that I had prepared well for this moment. For two arduous months, I readied my fingers for an exciting concert. No anxiety could undermine my confidence in my preparation, and my piano recital\u2019s success was \u201cin the bag.\u201d I selected three pieces for my repertoire: the ambience of Erik Satie\u2019s Gymnopedie No. 1 as the opener, a somber contemplation of Beethoven\u2019s First Movement of the Moonlight Sonata, and Bach\u2019s light and surreal Prelude in C Major for the conclusion. My shining moment arrived, and I strode purposefully toward the piano. The building in which my performance was held was new, but its dwellers were old. Respect and prestige permeated the atmosphere as I took each stride to my seat. As I sat down, the chair creaked and moaned as if in sympathy with the audience\u2019s aching desire to hear me play. I prepared my sheet music and commenced my epic moment. Never was such an exhilarating performance heard. All of the little techniques and tricks that I practiced were executed perfectly. I captured the dynamics I wanted to express in Satie\u2019s phonological experiment with each chord to which I applied varying pressure. Moving onto one of Beethoven\u2019s most famous works, I crafted the cascading arpeggios of each new chord, which resonated unity uninterrupted in me and in the audience. When I concluded with the airy prelude from Bach\u2019s Well-Tempered Clavier, the room swelled with bliss. Having poured my heart and soul into each piece, I beamed with pride. As customary for a stellar show, I rose to bow to the audience to thank them for their eruption of applause. Flowers were thrown, cheers elicited, and standing ovations bestowed. From the subsiding din came a faint question to rain on my parade: \u201cCould you play something more lively, darling, say, a Neil Diamond song?\u201d I work on weekends at a long-term-care facility, and my geriatric audience, although a pleasure with whom to interact, can be brutally honest. Begrudgingly, I thanked Mrs. Hersch for her request, promised her better next time, and stewed in my own irrelevance. Going home that day, my feathers were ruffled. How could any civilized listener, after such a superb medley, disregard such time-honored compositions? The notion was absurd. Yet perhaps more outlandish, as I later acknowledged, was my visceral reaction to the events that had transpired. Why did I react hesitantly to a simple request made in earnestness? It would have been easier, in fact, to practice \u201cSweet Caroline\u201d than to break my fingers over Beethoven\u2019s work. Then, in my moments of introspection, I concluded that my choice of musical pieces mattered little as long as my audience enjoyed them. Whether it meant recreating the most tortured and heinously composed pop song or a masterfully crafted Romantic concerto, I vowed to play them all. Throughout my life, my adult mentors have succored me with platitudes when most needed, which laid the foundation for my confidence. Yet, while working with people who have lived five times longer than I have, experiencing so much more than I can imagine, I know that the world does not revolve around my tastes and interests. I\u2019m okay with that. Thus, for a couple of hours each day in the living room, unlucky family members passing by are subjected to the torment of my tenth run-through of \u201cSweet Caroline\u201d as I prepare for my next recital for an audience that has taught me more about personal preferences, and myself, than I anticipated.", "human_reference": "I know that I had prepared well for this moment. For two arduous months, I readied my fingers for an exciting concert. No anxiety could undermine my confidence in my preparation, and my piano recital\u2019s success was \u201cin the bag.\u201d I selected three pieces for my repertoire: the ambience of Erik Satie\u2019s Gymnopedie No. 1 as the opener, a somber contemplation of Beethoven\u2019s First Movement of the Moonlight Sonata, and Bach\u2019s light and surreal Prelude in C Major for the conclusion. My shining moment arrived, and I strode purposefully toward the piano. The building in which my performance was held was new, but its dwellers were old. Respect and prestige permeated the atmosphere as I took each stride to my seat. As I sat down, the chair creaked and moaned as if in sympathy with the audience\u2019s aching desire to hear me play. I prepared my sheet music and commenced my epic moment. Never was such an exhilarating performance heard. All of the little techniques and tricks that I practiced were executed perfectly. I captured the dynamics I wanted to express in Satie\u2019s phonological experiment with each chord to which I applied varying pressure. Moving onto one of Beethoven\u2019s most famous works, I crafted the cascading arpeggios of each new chord, which resonated unity uninterrupted in me and in the audience. When I concluded with the airy prelude from Bach\u2019s Well-Tempered Clavier, the room swelled with bliss. Having poured my heart and soul into each piece, I beamed with pride. As customary for a stellar show, I rose to bow to the audience to thank them for their eruption of applause. Flowers were thrown, cheers elicited, and standing ovations bestowed. From the subsiding din came a faint question to rain on my parade: \u201cCould you play something more lively, darling, say, a Neil Diamond song?\u201d I work on weekends at a long-term-care facility, and my geriatric audience, although a pleasure with whom to interact, can be brutally honest. Begrudgingly, I thanked Mrs. Hersch for her request, promised her better next time, and stewed in my own irrelevance. Going home that day, my feathers were ruffled. How could any civilized listener, after such a superb medley, disregard such time-honored compositions? The notion was absurd. Yet perhaps more outlandish, as I later acknowledged, was my visceral reaction to the events that had transpired. Why did I react hesitantly to a simple request made in earnestness? It would have been easier, in fact, to practice \u201cSweet Caroline\u201d than to break my fingers over Beethoven\u2019s work. Then, in my moments of introspection, I concluded that my choice of musical pieces mattered little as long as my audience enjoyed them. Whether it meant recreating the most tortured and heinously composed pop song or a masterfully crafted Romantic concerto, I vowed to play them all. Throughout my life, my adult mentors have succored me with platitudes when most needed, which laid the foundation for my confidence. Yet, while working with people who have lived five times longer than I have, experiencing so much more than I can imagine, I know that the world does not revolve around my tastes and interests. I\u2019m okay with that. Thus, for a couple of hours each day in the living room, unlucky family members passing by are subjected to the torment of my tenth run-through of \u201cSweet Caroline\u201d as I prepare for my next recital for an audience that has taught me more about personal preferences, and myself, than I anticipated.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0014", "source": "CollegeEssay"}}
{"ai_text": "Title: Building a QA System (IID SQuAD track)\nAbstract: In this project, we explored different techniques in the encoding layer, the attention layer and the output layer of an end-to-end neural network architecture for question answering. Experiment results show that better performance can be achieved with different enhancements on top of the baseline model. Especially, with extra character embedding and deep residual coattention, we can achieve EM of 61.17 and F1 of 64.97 in comparison to EM of 58.32 and F1 of 61.78 of the baseline BiDAF model. To better understand the behavior of the best performed model, we broke down the F1 score distribution for the development set and examined the performance across different context lengths, answer lengths, and question types. Furthermore, by inspecting some of the error examples, we found that the model performs poorly mainly when it involves reasoning or advanced/complicated sentence structures.", "human_reference": "Title: Building a QA System (IID SQuAD track)\nAbstract: In this project, we explored different techniques in the encoding layer, the attention layer and the output layer of an end-to-end neural network architecture for question answering. Experiment results show that better performance can be achieved with different enhancements on top of the baseline model. Especially, with extra character embedding and deep residual coattention, we can achieve EM of 61.17 and F1 of 64.97 in comparison to EM of 58.32 and F1 of 61.78 of the baseline BiDAF model. To better understand the behavior of the best performed model, we broke down the F1 score distribution for the development set and examined the performance across different context lengths, answer lengths, and question types. Furthermore, by inspecting some of the error examples, we found that the model performs poorly mainly when it involves reasoning or advanced/complicated sentence structures.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0055", "source": "CS224N"}}
{"ai_text": "Title: Robust Question Answering using Domain Adversarial Training\nAbstract: While recent developments in deep learning and natural language understanding have produced models that perform very well on question answering tasks, they often learn superficial correlations specific to their training data and fail to generalize to unseen domains. We aim to create a more robust, generalized model by forcing it to create domain-invariant representations of the input using an adversarial discriminator system that attempts to classify the outputs of the QA model by domain. Our results show improvements over the baseline on average, although the model exhibited worse performance on certain datasets. We hypothesize that this is caused by differences in the kind of reasoning required for those datasets, differences which actually end up being erased by the discriminator.", "human_reference": "Title: Robust Question Answering using Domain Adversarial Training\nAbstract: While recent developments in deep learning and natural language understanding have produced models that perform very well on question answering tasks, they often learn superficial correlations specific to their training data and fail to generalize to unseen domains. We aim to create a more robust, generalized model by forcing it to create domain-invariant representations of the input using an adversarial discriminator system that attempts to classify the outputs of the QA model by domain. Our results show improvements over the baseline on average, although the model exhibited worse performance on certain datasets. We hypothesize that this is caused by differences in the kind of reasoning required for those datasets, differences which actually end up being erased by the discriminator.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0009", "source": "CS224N"}}
{"ai_text": "Piece by Piece: Building My Reality At this point in my life, I am used to the chuckles I receive upon telling my friends that I, in fact, love Legos. Growing up in a house of four children was a hectic environment to say the least; an escape from the chaos of siblings was much needed. As a kid, sitting down and concentrating on one task was never my intention, rather I was constantly energetic, chasing and being chased by my siblings. Building Lego sets had always been a way to minimize any stressors that were going on at the time, or to simply relax and enjoy the challenge. My first Lego set was given to me at a very young age, my seventh birthday, and although excited, I was puzzled with what I was supposed to accomplish. I knew that Luke Skywalker was going to need a little more assistance than I could offer at that age, so after countless hours of struggling and persisting, I inevitably succumbed to the numerous offers of help. Each birthday and holiday moving forward, I requested Legos in order to perfect my ability, and each time I gained expertise. Finally, I encountered my own \u201cEureka!\u201d moment, individually completing my first kit, a miniature replica of the Seattle Space Needle, solely on willpower and sheer excitement. My worn, but comfortable bedroom floor had become my safe haven for letting my mind wander and to create sculptures I would have never thought of if it hadn\u2019t been for my obsession with those miniscule, plastic blocks. I hadn\u2019t usually been the most creative, artistic person; however, when I sat down in my room next to my collection and freed my mind, I suddenly become an artist of my own definition. Soon, as I got older, more unique ideas for pieces flooded my mind rather than following strict instructions. These ideas had resulted in the possibility of designing and constructing certain buildings and entities, of course without any real-world consequences. My bedroom floor eventually turned into a skyline resembling that of New York City, skyscrapers grazing the top of my bed and Rockefeller Center spanning from my desk to my closet. Arriving home late from school or a strenuous practice, I was relieved to lay down next to my meaningful, personalized city. I rarely construct Lego structures nowadays; however, my obsession with those tiny bricks embedded a passion in me that will never cease to follow me. Arriving to a boarding school as a first-year student, I was extremely hesitant and nervous. Though I would soon be a part of a team, I sought an escape from my anxiety of being away from home and especially my bedroom. Though I hadn\u2019t brought along any of my Legos, (I\u2019m sure you can imagine why), I signed up for a new class which taught the basics of ceramics and sculpting figures. Ceramics was an entire new entity to me and I enjoyed every second of it. I had been constructing simple bowls and plates to ease myself into the new medium I was using. Soon, however, I became more confident and adventurous with my designs. After hours in the studio at school, I ultimately transferred my projects back to my personal studio, my bedroom, to join the company of my surrounding Lego projects. Not only providing me with entertainment, Legos left an everlasting mark on my capacity to experiment with new endeavors I would rarely attempt. Legos hold a special place in my mind and my heart due to the effect they have had on my curiosity, creativity and overall optimism. I will continue to design my sculptures, my essays, and my future, which is certainly guided by my imagination. Having constructed those guided, age appropriate sets and eventually designing unique pieces, I developed a knack for sculpting and imagining brand new ideas I transfer into my everyday life.", "human_reference": "Piece by Piece: Building My Reality At this point in my life, I am used to the chuckles I receive upon telling my friends that I, in fact, love Legos. Growing up in a house of four children was a hectic environment to say the least; an escape from the chaos of siblings was much needed. As a kid, sitting down and concentrating on one task was never my intention, rather I was constantly energetic, chasing and being chased by my siblings. Building Lego sets had always been a way to minimize any stressors that were going on at the time, or to simply relax and enjoy the challenge. My first Lego set was given to me at a very young age, my seventh birthday, and although excited, I was puzzled with what I was supposed to accomplish. I knew that Luke Skywalker was going to need a little more assistance than I could offer at that age, so after countless hours of struggling and persisting, I inevitably succumbed to the numerous offers of help. Each birthday and holiday moving forward, I requested Legos in order to perfect my ability, and each time I gained expertise. Finally, I encountered my own \u201cEureka!\u201d moment, individually completing my first kit, a miniature replica of the Seattle Space Needle, solely on willpower and sheer excitement. My worn, but comfortable bedroom floor had become my safe haven for letting my mind wander and to create sculptures I would have never thought of if it hadn\u2019t been for my obsession with those miniscule, plastic blocks. I hadn\u2019t usually been the most creative, artistic person; however, when I sat down in my room next to my collection and freed my mind, I suddenly become an artist of my own definition. Soon, as I got older, more unique ideas for pieces flooded my mind rather than following strict instructions. These ideas had resulted in the possibility of designing and constructing certain buildings and entities, of course without any real-world consequences. My bedroom floor eventually turned into a skyline resembling that of New York City, skyscrapers grazing the top of my bed and Rockefeller Center spanning from my desk to my closet. Arriving home late from school or a strenuous practice, I was relieved to lay down next to my meaningful, personalized city. I rarely construct Lego structures nowadays; however, my obsession with those tiny bricks embedded a passion in me that will never cease to follow me. Arriving to a boarding school as a first-year student, I was extremely hesitant and nervous. Though I would soon be a part of a team, I sought an escape from my anxiety of being away from home and especially my bedroom. Though I hadn\u2019t brought along any of my Legos, (I\u2019m sure you can imagine why), I signed up for a new class which taught the basics of ceramics and sculpting figures. Ceramics was an entire new entity to me and I enjoyed every second of it. I had been constructing simple bowls and plates to ease myself into the new medium I was using. Soon, however, I became more confident and adventurous with my designs. After hours in the studio at school, I ultimately transferred my projects back to my personal studio, my bedroom, to join the company of my surrounding Lego projects. Not only providing me with entertainment, Legos left an everlasting mark on my capacity to experiment with new endeavors I would rarely attempt. Legos hold a special place in my mind and my heart due to the effect they have had on my curiosity, creativity and overall optimism. I will continue to design my sculptures, my essays, and my future, which is certainly guided by my imagination. Having constructed those guided, age appropriate sets and eventually designing unique pieces, I developed a knack for sculpting and imagining brand new ideas I transfer into my everyday life.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0008", "source": "CollegeEssay"}}
{"ai_text": "Being honest is always a principal part in any relationships, it's the only way to make people feel comfortable around each other. When things go wrong, even with good reasons, I'd like to hear about the truth of it no matter how frustrating it might be. Because if someone lies to me and I find out somehow, how can I trust him anymore? How can I not be wondering if he's tell the truth every time he talks to me? There's no way I can keep a healthy relationship with this guy.", "human_reference": "Being honest is always a principal part in any relationships, it's the only way to make people feel comfortable around each other. When things go wrong, even with good reasons, I'd like to hear about the truth of it no matter how frustrating it might be. Because if someone lies to me and I find out somehow, how can I trust him anymore? How can I not be wondering if he's tell the truth every time he talks to me? There's no way I can keep a healthy relationship with this guy.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0020", "source": "TOEFL11"}}
{"ai_text": "Title: Building QA Robustness Through Data Augmentation\nAbstract: While question and answering (QA) models have achieved tremendous results on in-domain queries, recent research has brought into question the ability of these Q&A models to generalize well to unseen data in other domains. To address this,  we aim to build a robust question answering system, which trained on a set of in-domain data can then be adapted to unseen domains given few training samples. Our main approach is the field of data augmentation. In this work, we conduct a survey of existing data augmentation methods, including backtranslation, synonym replacement, and synonym insertion, as well as introduce a mixed data augmentation method (MDA) combining the previous three. For examples of backtranslation, synonym replacement, and synonym insertion, please see the displayed figure. The figure displays three examples for how one sentence might be augmented using each data method. In particular, we explore the efficacy of data augmentation in the task of question answering. We find that data augmentation provides moderate gains on our out of domain validation and test sets and that certain methods such as backtranslation and synonym replacement provide larger improvements compared to others. Overall, we confirm that data augmentation is a simple, generalizable technique with a wide variety of different methods that can effectively aid in improving the robustness of Q&A models in the face of unseen domains with few training examples.", "human_reference": "Title: Building QA Robustness Through Data Augmentation\nAbstract: While question and answering (QA) models have achieved tremendous results on in-domain queries, recent research has brought into question the ability of these Q&A models to generalize well to unseen data in other domains. To address this,  we aim to build a robust question answering system, which trained on a set of in-domain data can then be adapted to unseen domains given few training samples. Our main approach is the field of data augmentation. In this work, we conduct a survey of existing data augmentation methods, including backtranslation, synonym replacement, and synonym insertion, as well as introduce a mixed data augmentation method (MDA) combining the previous three. For examples of backtranslation, synonym replacement, and synonym insertion, please see the displayed figure. The figure displays three examples for how one sentence might be augmented using each data method. In particular, we explore the efficacy of data augmentation in the task of question answering. We find that data augmentation provides moderate gains on our out of domain validation and test sets and that certain methods such as backtranslation and synonym replacement provide larger improvements compared to others. Overall, we confirm that data augmentation is a simple, generalizable technique with a wide variety of different methods that can effectively aid in improving the robustness of Q&A models in the face of unseen domains with few training examples.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0074", "source": "CS224N"}}
{"ai_text": "Title: Question Answering on SQuAD2.0\nAbstract: We chose the default project to build a Question Answering system on the SQuAD 2.0 dataset. Our initial approach to solve this problem focused on implementing the default baseline model that is based on a variant of Bidirectional Attention Flow (BiDAF) with attention. We explored performance after adding character level embeddings to the baseline along with exploring various attention mechanisms. Additionally, we also explored the impact of tuning the hyper-parameters used to train the model. Finally, we studied the effect of using multiple variants of RNN as building blocks in the neural architecture. We improved the model performance on both dev and test sets by at least 4 points. The baseline F1 and EM scores without character embedding were 60.65 and 57.13 while our best improvements with BiDAF, Character Embedding, Self-attention with LSTM were 65.80 and 62.99 respectively. The scores would have been better with pre-trained models however, for our track it was prohibited. Even if we could improve the performance by a bit, question answering remains a challenging problem with a lot of scope of improvement. Also, we need to make sure that the current model generalizes beyond SQuAD dataset.\nThis course was our first foray in the field of NLP and we have developed a deeper understanding about the advances and challenges in Natural Language Understanding and processing and hope to keep improving it with time.", "human_reference": "Title: Question Answering on SQuAD2.0\nAbstract: We chose the default project to build a Question Answering system on the SQuAD 2.0 dataset. Our initial approach to solve this problem focused on implementing the default baseline model that is based on a variant of Bidirectional Attention Flow (BiDAF) with attention. We explored performance after adding character level embeddings to the baseline along with exploring various attention mechanisms. Additionally, we also explored the impact of tuning the hyper-parameters used to train the model. Finally, we studied the effect of using multiple variants of RNN as building blocks in the neural architecture. We improved the model performance on both dev and test sets by at least 4 points. The baseline F1 and EM scores without character embedding were 60.65 and 57.13 while our best improvements with BiDAF, Character Embedding, Self-attention with LSTM were 65.80 and 62.99 respectively. The scores would have been better with pre-trained models however, for our track it was prohibited. Even if we could improve the performance by a bit, question answering remains a challenging problem with a lot of scope of improvement. Also, we need to make sure that the current model generalizes beyond SQuAD dataset.\nThis course was our first foray in the field of NLP and we have developed a deeper understanding about the advances and challenges in Natural Language Understanding and processing and hope to keep improving it with time.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0031", "source": "CS224N"}}
{"ai_text": "Title: Self-Attention in Question Answering\nAbstract: For the default final project, our task was to build a model that performs question answering over the Stanford Question Answering Dataset (SQuAD). Our goal was to improve on the baseline BiDAF model's F1 and EM scores on the task. To do so, we made two additions to the model: character embeddings and a self-attention layer, both which were used in R-Net. We found that while these additions improved the F1 and EM scores, it also required significantly more memory and training time.", "human_reference": "Title: Self-Attention in Question Answering\nAbstract: For the default final project, our task was to build a model that performs question answering over the Stanford Question Answering Dataset (SQuAD). Our goal was to improve on the baseline BiDAF model's F1 and EM scores on the task. To do so, we made two additions to the model: character embeddings and a self-attention layer, both which were used in R-Net. We found that while these additions improved the F1 and EM scores, it also required significantly more memory and training time.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0028", "source": "CS224N"}}
{"ai_text": "There's no doubt that my favorite type of movie is drama. I've always loved movies since I was a little kid, but unlike most of the kids, I've never found action movies attractive. Exciting as they are, they are usually empty in the inside. It's likely that you'll remember nothing about them after a month. While I like to seek the wisdom in those dramas, to find out what the movies really want to say behind the scene. I grow so emotional attached to the people in the stories. I began to share their joy and sorrow. And I've learned so much from them.", "human_reference": "There's no doubt that my favorite type of movie is drama. I've always loved movies since I was a little kid, but unlike most of the kids, I've never found action movies attractive. Exciting as they are, they are usually empty in the inside. It's likely that you'll remember nothing about them after a month. While I like to seek the wisdom in those dramas, to find out what the movies really want to say behind the scene. I grow so emotional attached to the people in the stories. I began to share their joy and sorrow. And I've learned so much from them.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0074", "source": "TOEFL11"}}
{"ai_text": "Title: Task-Adaptive Pretraining, Domain Sampling, and Data Augmentation Improve Generalized Question Answering\nAbstract: To create a deep-learning question answering (QA) system that generalizes to unseen domains, we investigate the use of three techniques: task-adaptive pretraining (TAPT), domain sampling, and data augmentation. We train a single DistilBERT model in three phases (shown in the flowchart). First, during TAPT, we pretrain with masked-language modeling (MLM) on our QA datasets. Second, we fine-tune on our QA data. We employ domain sampling during both pretraining and fine-tuning, which preferably samples data that lead to better downstream performance. Finally, for our data augmentations, we use synonym replacement and random deletion to increase the size and variety of our out-domain data, before additionally fine-tuning on these augmented data. During evaluation, we found significant EM/F1 performance improvements by fine-tuning on augmented out-domain data. We found modest, yet non-trivial, performance improvements with TAPT and domain sampling. Using these three techniques, our model achieved EM/F1 scores of 37.44/51.37 on the development set and 40.12/58.05 on the test set.", "human_reference": "Title: Task-Adaptive Pretraining, Domain Sampling, and Data Augmentation Improve Generalized Question Answering\nAbstract: To create a deep-learning question answering (QA) system that generalizes to unseen domains, we investigate the use of three techniques: task-adaptive pretraining (TAPT), domain sampling, and data augmentation. We train a single DistilBERT model in three phases (shown in the flowchart). First, during TAPT, we pretrain with masked-language modeling (MLM) on our QA datasets. Second, we fine-tune on our QA data. We employ domain sampling during both pretraining and fine-tuning, which preferably samples data that lead to better downstream performance. Finally, for our data augmentations, we use synonym replacement and random deletion to increase the size and variety of our out-domain data, before additionally fine-tuning on these augmented data. During evaluation, we found significant EM/F1 performance improvements by fine-tuning on augmented out-domain data. We found modest, yet non-trivial, performance improvements with TAPT and domain sampling. Using these three techniques, our model achieved EM/F1 scores of 37.44/51.37 on the development set and 40.12/58.05 on the test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0008", "source": "CS224N"}}
{"ai_text": "I prefer to cook at home. First of all, it is cheaper and safer than the restaurant. Because I can buy the things needed for cooking, and I can make sure that they are clean and fresh. Second, making a good dinner helps me obtain a sense of achievement. On every Spring festival, I always make dinner for my whole family, they always think that the meal is delicious and we can chat freely around the table. I am really proud of it and I think it can improve the relationship between my family and me.", "human_reference": "I prefer to cook at home. First of all, it is cheaper and safer than the restaurant. Because I can buy the things needed for cooking, and I can make sure that they are clean and fresh. Second, making a good dinner helps me obtain a sense of achievement. On every Spring festival, I always make dinner for my whole family, they always think that the meal is delicious and we can chat freely around the table. I am really proud of it and I think it can improve the relationship between my family and me.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0000", "source": "TOEFL11"}}
{"ai_text": "Title: Extended QA System on SQuAD 2.0\nAbstract: Our motivation is to build a Question Answering (QA) system that gives answers as specific and as accurate to queries, which is in itself an art but based on the science of Natural Language Processing (NLP). The main goal of our project is to produce a QA system that works well on SQuAD 2.0 dataset that performs better than the baseline Bidirectional Attention Flow (BiDAF) model. To better capture the context from a more expressive set of answers and understand the interactions between the question and the document, we utilized the coattention mechanism by encoding the two-way attention outputs together through a bidirectional reccurrent neural network (RNN). We experimented with enriching the embedding layer with concatenating character embeddings with existing word-level embedding, modifying the attention layer with coattention from Dynamic Coattention Networks (DCN), adding an Answer Pointer, which conditions the ending of the answer span on the starting position, to the output layer. Our best performing single model obtained F1/EM scores of 63.40/59.87, which both achieved better results than the baseline. Adding character embeddings and the answer pointer gave us a successful performance boost compared with the BiDAF baseline model. On the other hand, dynamic coattention from DCN did not beat the attention and modeling layer combined in the baseline BiDAF model but was worth trying. To further improve the performance of our model, we built ensemble models which finetune on the dropout rates, and the best one achieved F1/EM scores of 64.21/60.81.", "human_reference": "Title: Extended QA System on SQuAD 2.0\nAbstract: Our motivation is to build a Question Answering (QA) system that gives answers as specific and as accurate to queries, which is in itself an art but based on the science of Natural Language Processing (NLP). The main goal of our project is to produce a QA system that works well on SQuAD 2.0 dataset that performs better than the baseline Bidirectional Attention Flow (BiDAF) model. To better capture the context from a more expressive set of answers and understand the interactions between the question and the document, we utilized the coattention mechanism by encoding the two-way attention outputs together through a bidirectional reccurrent neural network (RNN). We experimented with enriching the embedding layer with concatenating character embeddings with existing word-level embedding, modifying the attention layer with coattention from Dynamic Coattention Networks (DCN), adding an Answer Pointer, which conditions the ending of the answer span on the starting position, to the output layer. Our best performing single model obtained F1/EM scores of 63.40/59.87, which both achieved better results than the baseline. Adding character embeddings and the answer pointer gave us a successful performance boost compared with the BiDAF baseline model. On the other hand, dynamic coattention from DCN did not beat the attention and modeling layer combined in the baseline BiDAF model but was worth trying. To further improve the performance of our model, we built ensemble models which finetune on the dropout rates, and the best one achieved F1/EM scores of 64.21/60.81.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0096", "source": "CS224N"}}
{"ai_text": "Title: Improve DistilIBERT-based Question Answering model performance on out-of-domain datasets by Mixing Right Experts\nAbstract: In this work, we built a MOE model by mixing 7 DistilBERT-based QA expert models that are task-fine-tuned on in-domain training datasets. We built data insight by carefully examining performance correlation across in-domain datasets and out-of-domain datasets and found out domain-fine-tuning on small target out-of-domain dataset that has quite different distribution than in-domain training dataset does not necessarily translate into out-of-domain performance on target dataset. We carefully select a set expert models for each out-of-domain set by leveraging data insights aforementioned. We achieved F1 score of 61.7} (ranked 6th out of 74 in test leaderboard) and EM score of 44.4 (ranked 2nd out of 74 in test leaderboard) in out-of-domain test datasets as of March 19, 2021.", "human_reference": "Title: Improve DistilIBERT-based Question Answering model performance on out-of-domain datasets by Mixing Right Experts\nAbstract: In this work, we built a MOE model by mixing 7 DistilBERT-based QA expert models that are task-fine-tuned on in-domain training datasets. We built data insight by carefully examining performance correlation across in-domain datasets and out-of-domain datasets and found out domain-fine-tuning on small target out-of-domain dataset that has quite different distribution than in-domain training dataset does not necessarily translate into out-of-domain performance on target dataset. We carefully select a set expert models for each out-of-domain set by leveraging data insights aforementioned. We achieved F1 score of 61.7} (ranked 6th out of 74 in test leaderboard) and EM score of 44.4 (ranked 2nd out of 74 in test leaderboard) in out-of-domain test datasets as of March 19, 2021.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0137", "source": "CS224N"}}
{"ai_text": "Title: The Efficient BiIDAF\nAbstract: In recent years, The massive pre-trained Language models have dominated the State-of-the-Art leaderboard across many NLP tasks including the Question Answering task on SQuAD 2.0. In this project, we travel back to a successful traditional approach known as Bi-Directional Attention Flow (BiDAF) which uses a sequence-to-sequence network. We identify the shortcomings of this model and implement a multi-stage hierarchical end-to-end network that solves the shortcomings of BiDAF.\n\nMore specifically, the original model uses a sequence-to-sequence network like RNN to encode information of query/context into a vector. Even though RNNs' are known to be quite effective, they have few huge bottlenecks, namely, non-parallelizability of the network due to its seq-to-seq/time-step based computation, lack of transfer learning support, and vulnerability to vanishing/exploding gradient. We handle these shortcomings of RNN by replacing them with transformer encoders.\n\nAdditionally, we implement few recent techniques to improve the vanilla encoder network, namely, Spatial Positional Encoding instead of traditional Absolute Positional Encoding, ScaleNorm instead of traditional LayerNorm, Feedforward Network with Gated Linear Unit instead of traditional Feedforward Network with RELU.\n\nLooking outside RNN, we replace the query-to-context and context-to-query Attention flow with Cross-Attention using a Multi-headed Attention mechanism. We show that multi-headed Cross-Attention works better than the traditional Attention Flow layer.\n\nFinally, we introduce pre-trained character embedding vectors that were extrapolated from the existing Glove pre-trained word embeddings. We also show that this improves the baseline BiDAF model by a considerable amount.\n\nLastly, we show the results of our final model on the validation set and compare its performance with the baseline BiDAF model. Evidently, we can observe that our model is performing better than the original BiDAF in terms of latency, and accuracy. Our Model is also highly extensible since we use encoders and multi-head attention and they don't suffer from traditional seq-to-seq bottlenecks and are available to the use of transfer learning.", "human_reference": "Title: The Efficient BiIDAF\nAbstract: In recent years, The massive pre-trained Language models have dominated the State-of-the-Art leaderboard across many NLP tasks including the Question Answering task on SQuAD 2.0. In this project, we travel back to a successful traditional approach known as Bi-Directional Attention Flow (BiDAF) which uses a sequence-to-sequence network. We identify the shortcomings of this model and implement a multi-stage hierarchical end-to-end network that solves the shortcomings of BiDAF.\n\nMore specifically, the original model uses a sequence-to-sequence network like RNN to encode information of query/context into a vector. Even though RNNs' are known to be quite effective, they have few huge bottlenecks, namely, non-parallelizability of the network due to its seq-to-seq/time-step based computation, lack of transfer learning support, and vulnerability to vanishing/exploding gradient. We handle these shortcomings of RNN by replacing them with transformer encoders.\n\nAdditionally, we implement few recent techniques to improve the vanilla encoder network, namely, Spatial Positional Encoding instead of traditional Absolute Positional Encoding, ScaleNorm instead of traditional LayerNorm, Feedforward Network with Gated Linear Unit instead of traditional Feedforward Network with RELU.\n\nLooking outside RNN, we replace the query-to-context and context-to-query Attention flow with Cross-Attention using a Multi-headed Attention mechanism. We show that multi-headed Cross-Attention works better than the traditional Attention Flow layer.\n\nFinally, we introduce pre-trained character embedding vectors that were extrapolated from the existing Glove pre-trained word embeddings. We also show that this improves the baseline BiDAF model by a considerable amount.\n\nLastly, we show the results of our final model on the validation set and compare its performance with the baseline BiDAF model. Evidently, we can observe that our model is performing better than the original BiDAF in terms of latency, and accuracy. Our Model is also highly extensible since we use encoders and multi-head attention and they don't suffer from traditional seq-to-seq bottlenecks and are available to the use of transfer learning.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0075", "source": "CS224N"}}
{"ai_text": "Title: Building a Robust QA System that Knows When it Doesn't Know\nAbstract: Machine Learning models have a hard time knowing when they shouldn't be confident\nabout their output. A robust QnA module should not only be able to do a good job at out of context data, but also be able to do a good job of knowing what data it can't handle. The goal of our project is to build a robust QnA model with an architecture that relies on a base of DistilBERT, improve on it through model fine-tuning, better optimization, and then augment the predictions of the model with a confidence score\n\nOur approach for this project was forked in two directions.\n1. Focus on fine-tuning the model through approaches like transfer learning, longer epochs, mix-out and re-initializing layers.\n2. Augment the model by providing a confidence score to enhance the model's reliability in real world usage.\n\nBERT models use the base weights from pre-training and then fine-tune on specific datasets. They are pre-trained on a variety of tasks making it easier to generalize but it needs to be further fine-tuned for specific task. Also, the fine tuning process is susceptible to the distribution of data in the smaller datasets.\n\nWe aim to improve on this by training on larger epochs, freezing all but the last layers of the BERT model, re-initializing the pre-trained model weights, using a regularization technique called mixout, use the bias correction and finally add additional layers to the model.\n\nThe learnings from the experiments were:\n1.    Bias correction doesn't have any significant impact on the performance\n2.    Freezing the initial layers of DistilBERT doesn't impact the performance but it does speed up the training time\n3.    Re-initializing the lower layers have a positive impact on the performance of the model\n4.    Applying regularization in form of mixout increases the overall accuracy of the model", "human_reference": "Title: Building a Robust QA System that Knows When it Doesn't Know\nAbstract: Machine Learning models have a hard time knowing when they shouldn't be confident\nabout their output. A robust QnA module should not only be able to do a good job at out of context data, but also be able to do a good job of knowing what data it can't handle. The goal of our project is to build a robust QnA model with an architecture that relies on a base of DistilBERT, improve on it through model fine-tuning, better optimization, and then augment the predictions of the model with a confidence score\n\nOur approach for this project was forked in two directions.\n1. Focus on fine-tuning the model through approaches like transfer learning, longer epochs, mix-out and re-initializing layers.\n2. Augment the model by providing a confidence score to enhance the model's reliability in real world usage.\n\nBERT models use the base weights from pre-training and then fine-tune on specific datasets. They are pre-trained on a variety of tasks making it easier to generalize but it needs to be further fine-tuned for specific task. Also, the fine tuning process is susceptible to the distribution of data in the smaller datasets.\n\nWe aim to improve on this by training on larger epochs, freezing all but the last layers of the BERT model, re-initializing the pre-trained model weights, using a regularization technique called mixout, use the bias correction and finally add additional layers to the model.\n\nThe learnings from the experiments were:\n1.    Bias correction doesn't have any significant impact on the performance\n2.    Freezing the initial layers of DistilBERT doesn't impact the performance but it does speed up the training time\n3.    Re-initializing the lower layers have a positive impact on the performance of the model\n4.    Applying regularization in form of mixout increases the overall accuracy of the model", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0039", "source": "CS224N"}}
{"ai_text": "Title: Default Final Project: RobustQA Track\nAbstract: Our goal is to build a question answering system that can adapt to unseen domains with only a few training samples from the domain.. We experimented with several approaches, including mixture of experts approach and various techniques to fine tune the pre-trained model better. Although we are able to to outperform the baseline, we found that model architecture is less important when it comes to improving performance. Relevant training data is by far the most important factor. Various fine tune techniques also help to some extend", "human_reference": "Title: Default Final Project: RobustQA Track\nAbstract: Our goal is to build a question answering system that can adapt to unseen domains with only a few training samples from the domain.. We experimented with several approaches, including mixture of experts approach and various techniques to fine tune the pre-trained model better. Although we are able to to outperform the baseline, we found that model architecture is less important when it comes to improving performance. Relevant training data is by far the most important factor. Various fine tune techniques also help to some extend", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0093", "source": "CS224N"}}
{"ai_text": "Title: Faster Attention for Question Answering\nAbstract: In this project (a default final project on the IID track), I built a question-answering system for SQuAD 2.0 by exploring both the BiDAF model through modifications of the default baseline as well as a from scratch implementation of QANet, a self-attention-based question-answering architecture. The BiDAF modifications which added character embeddings achieved a small, but significant improvement over the baseline model on the test set. However, the QANet models only nearly matched the baseline BiDAF scoring with character embeddings. Curiously, not only did my QANet under-perform the baseline in model performance, it also turned out to be significantly slower to train and at inference time on GPUs. Though profiling, I found that the QANet model is indeed faster on CPUs, however significantly under-performs the baseline BiDAF model on GPUs because the BiDAF model's slowest component, the RNN, is implemented as a highly optimized CuDNN routine on GPUs that the custom QANet encoder block did not benefit from. Finally, this profiling also shows that faster attention mechanisms, as explored in the literature, are unlikely to improve performance on this particular SQuAD 2.0 workload as additional instruction overhead would likely wash out any performance gains absent better operation compilation for GPUs or a custom GPU kernel.", "human_reference": "Title: Faster Attention for Question Answering\nAbstract: In this project (a default final project on the IID track), I built a question-answering system for SQuAD 2.0 by exploring both the BiDAF model through modifications of the default baseline as well as a from scratch implementation of QANet, a self-attention-based question-answering architecture. The BiDAF modifications which added character embeddings achieved a small, but significant improvement over the baseline model on the test set. However, the QANet models only nearly matched the baseline BiDAF scoring with character embeddings. Curiously, not only did my QANet under-perform the baseline in model performance, it also turned out to be significantly slower to train and at inference time on GPUs. Though profiling, I found that the QANet model is indeed faster on CPUs, however significantly under-performs the baseline BiDAF model on GPUs because the BiDAF model's slowest component, the RNN, is implemented as a highly optimized CuDNN routine on GPUs that the custom QANet encoder block did not benefit from. Finally, this profiling also shows that faster attention mechanisms, as explored in the literature, are unlikely to improve performance on this particular SQuAD 2.0 workload as additional instruction overhead would likely wash out any performance gains absent better operation compilation for GPUs or a custom GPU kernel.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0029", "source": "CS224N"}}
{"ai_text": "That would be the moment when I got the result of my college entrance test. We were under huge pressure that day. Finally the phone finally rang and it turned out that I actually did a good job, when my mind was still all blank, my dad said something that brought me back to reality:\u201d Son, I'm so proud of you.\u201d Suddenly I realized that all my hard work had paid off, I didn't let myself and anyone who loves me down, that's the moment I knew it is a brand new start of my life, and that I'll always cherish.", "human_reference": "That would be the moment when I got the result of my college entrance test. We were under huge pressure that day. Finally the phone finally rang and it turned out that I actually did a good job, when my mind was still all blank, my dad said something that brought me back to reality:\u201d Son, I'm so proud of you.\u201d Suddenly I realized that all my hard work had paid off, I didn't let myself and anyone who loves me down, that's the moment I knew it is a brand new start of my life, and that I'll always cherish.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0060", "source": "TOEFL11"}}
{"ai_text": "I would take them to a science museum, because I want my students to learn something from this experience and a science museum is the most motivating place to do that. Students are able to do a lot of cool things there like seeing the fossil that they've learnt about in class and what not, which is something they normally can't do.", "human_reference": "I would take them to a science museum, because I want my students to learn something from this experience and a science museum is the most motivating place to do that. Students are able to do a lot of cool things there like seeing the fossil that they've learnt about in class and what not, which is something they normally can't do.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0064", "source": "TOEFL11"}}
{"ai_text": "I could still hear her words, the words my teacher said as she handed me the packet, \u201cThis is a challenge. But I think you\u2019re up for it.\u201d I held the math packet in my hand. On the cover, the title \u2018Mission Possible!\u2019 screamed at me. I could feel my fingers tingling, and the goosebumps rolling up my arms. I stared at the black italicized letters of the title as I walked home. They seemed to stare back, alluding to the mysteries that lay underneath them. As soon as I got home, I ran to the top bunk where I slept, grabbed a pencil, and signed a mental contract with the packet: \u201cI, Zerubabel, promise to prioritize you, put you above all else in my life, not rest, and not eat until all the problems that lay in your pages are solved.\u201d I was a pretty dramatic 11-year-old. This is but one example of the many challenges I\u2019ve faced throughout my life. My love for challenges and the tenacity with which I approach them was instilled in me through observing my family and through my own experiences. Ten years ago, my family and I packed our belongings, sold everything we had, and flew across the Atlantic to our new home in America. During our first year in Minnesota, we were faced with the omnipresent challenge of money. My sister, rather than having the comfort of her crib, was forced to share a bed with my mom and I. My dad was forced to sleep on a makeshift bed my mom made for him every night, using cushions from a torn and scratchy old sofa. My mom was forced to wake up early and stay up late working, at home, and her minimum wage job. My parents never complained. To them, this was just another stage of life, another challenge to overcome. They worked tirelessly-my mom providing stability by maintaining one job while my dad, the creative one, was always switching between multiple in his pursuit for better pay. With each day, the consequences of their hard work showed; one bed became two, the second bed split into a bunk, and within that little room, each of us had a bed to sleep on. I now reflect on this, and many other challenges my family and I have faced during our ten years in America. I realize that it is through observing how my parents never slowed down that I learned the value of perseverance, through watching my mom\u2019s devotion to a single job that I learned the value of commitment, through my dad\u2019s consistent job switches that I learned the value of ambition, and through observing my sisters willingness to live with less that I learned the value of sacrifice. Through my own experiences, I learned I can apply these values and overcome any challenge that comes my way. My 11-year-old self figured this out after a grueling two months of working on the packet, finishing with all the questions answered. Throughout my time in middle and high school, my value of ambition has led me to take the most challenging courses available at my school. In my community, my value of commitment has allowed me to serve at my church for the past five years. These learned values have molded me into the person I am today and will continue to guide me as I pursue my goals in life. It is because of these values and the way they were instilled in me that I have decided to pursue a career as a surgeon; I know it is through the guidance of these values and the people who first showed them to me that I will be able to achieve this goal.", "human_reference": "I could still hear her words, the words my teacher said as she handed me the packet, \u201cThis is a challenge. But I think you\u2019re up for it.\u201d I held the math packet in my hand. On the cover, the title \u2018Mission Possible!\u2019 screamed at me. I could feel my fingers tingling, and the goosebumps rolling up my arms. I stared at the black italicized letters of the title as I walked home. They seemed to stare back, alluding to the mysteries that lay underneath them. As soon as I got home, I ran to the top bunk where I slept, grabbed a pencil, and signed a mental contract with the packet: \u201cI, Zerubabel, promise to prioritize you, put you above all else in my life, not rest, and not eat until all the problems that lay in your pages are solved.\u201d I was a pretty dramatic 11-year-old. This is but one example of the many challenges I\u2019ve faced throughout my life. My love for challenges and the tenacity with which I approach them was instilled in me through observing my family and through my own experiences. Ten years ago, my family and I packed our belongings, sold everything we had, and flew across the Atlantic to our new home in America. During our first year in Minnesota, we were faced with the omnipresent challenge of money. My sister, rather than having the comfort of her crib, was forced to share a bed with my mom and I. My dad was forced to sleep on a makeshift bed my mom made for him every night, using cushions from a torn and scratchy old sofa. My mom was forced to wake up early and stay up late working, at home, and her minimum wage job. My parents never complained. To them, this was just another stage of life, another challenge to overcome. They worked tirelessly-my mom providing stability by maintaining one job while my dad, the creative one, was always switching between multiple in his pursuit for better pay. With each day, the consequences of their hard work showed; one bed became two, the second bed split into a bunk, and within that little room, each of us had a bed to sleep on. I now reflect on this, and many other challenges my family and I have faced during our ten years in America. I realize that it is through observing how my parents never slowed down that I learned the value of perseverance, through watching my mom\u2019s devotion to a single job that I learned the value of commitment, through my dad\u2019s consistent job switches that I learned the value of ambition, and through observing my sisters willingness to live with less that I learned the value of sacrifice. Through my own experiences, I learned I can apply these values and overcome any challenge that comes my way. My 11-year-old self figured this out after a grueling two months of working on the packet, finishing with all the questions answered. Throughout my time in middle and high school, my value of ambition has led me to take the most challenging courses available at my school. In my community, my value of commitment has allowed me to serve at my church for the past five years. These learned values have molded me into the person I am today and will continue to guide me as I pursue my goals in life. It is because of these values and the way they were instilled in me that I have decided to pursue a career as a surgeon; I know it is through the guidance of these values and the people who first showed them to me that I will be able to achieve this goal.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0038", "source": "CollegeEssay"}}
{"ai_text": "I disagree with the idea that people should always be truthful. First of all, telling a white lie is better in many situations. For example, when parents lie to their kids that Santa Claus exists, their children will behave well for the whole year because they want to receive good presents from Santa. In addition, the other person may not really want to hear your honest answer anyway. For instance, when an overweight girl asks a guy if she is fat, she may want to hear the other person say \u201cNo, you are skinny.\u201d In situations like this, it is better to lie. Therefore, people should not always be truthful.", "human_reference": "I disagree with the idea that people should always be truthful. First of all, telling a white lie is better in many situations. For example, when parents lie to their kids that Santa Claus exists, their children will behave well for the whole year because they want to receive good presents from Santa. In addition, the other person may not really want to hear your honest answer anyway. For instance, when an overweight girl asks a guy if she is fat, she may want to hear the other person say \u201cNo, you are skinny.\u201d In situations like this, it is better to lie. Therefore, people should not always be truthful.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0035", "source": "TOEFL11"}}
{"ai_text": "The craft of storytelling, for me, is holy. Looking back on my life, I don\u2019t see one defining moment where I realized that my purpose is to study, compose, and teach story. It\u2019s more like a collection of snapshots whipping by, each one yellowed with age and fingerprints. I remember reading The Empty Space by Peter Brook on theatre theory long into the night, encountering the line, \u201cDrama was exposure, it was confrontation\u2026 it led to\u2026 an awakening of understanding.\u201d These words were what led me to the discovery of how storytelling is an emotional confrontation between the author and the writing and between the writing and the audience. It\u2019s collision. It\u2019s catharsis. Catharsis defines me as a playwright. The first play I wrote, The Rocket Man, adapted from a short story of the same name by Ray Bradbury, follows a teenage boy whose astronaut father spends much of his time in space. It\u2019s uncanny \u2013 that\u2019s my entire life. My own father travels from Denver to Los Angeles four days a week on business, and my family isn\u2019t whole unless he\u2019s with us. Drafting a scene of The Rocket Man, where the boy confronts his father before he leaves again, changed my life: Stay with me, the boy begged in the original scene I added. Please. I immediately began to cry, praying that my own father, a thousand miles away, was listening. I learned that day that catharsis is releasing my own story within a story. When the line between my soul and the soul of my story blurs, that\u2019s when the real work happens. The construct of The Rocket Man was like a cathedral, with my own emotion as stained glass and my memories as arches, but I realized after sharing it with my cast and crew for the first time that a cathedral is nothing without people to experience it. My 18-year-old male lead, Pierce, cried when he first read the script. So did my light designer and sound designer. What I want is to recreate this experience for an audience as a playwright with the intention of establishing the theatre as a safe place. You can grieve here. You can be seen here. You can hope here. This goal starts not just with craft, but with overwhelming love for that audience. At its core, storytelling is service. One of the defining challenges of my life presented itself as the opportunity to create and execute a free playwriting course for middle-school girls. When the pandemic hit, it forced me to reimagine my course for a virtual setting offered through the local school district; I realigned everything \u2013 my three-week curriculum, my downloadable course exercises, and my teaching strategies. Teaching playwriting to middle-school girls over Zoom meant listening to their struggle to make friends at school and their desire to participate in protest marches against the will of their parents. With each lesson, they experienced the transcendence of having their lives and emotions reflected through story, and they loved it. One student, Isabel, told me with ten exclamation points about how excited she was for class. She even filmed a one-woman version of a play she wrote, complete with costumes and accents. I came out of class every night feeling like I might burst from joy. Showing students how to release their own story within a story is the most purposeful thing I have ever done. Sitting in a theatre as the overture starts, hearing a thousand conversations stop in the span of a single heartbeat. A hand over my mouth in awe as I watch the finale and wonder at how a two-hour show can contain all the nuances of life. That\u2019s why I exist \u2013 to offer story. To teach it. That\u2019s my mission and my ministry.", "human_reference": "The craft of storytelling, for me, is holy. Looking back on my life, I don\u2019t see one defining moment where I realized that my purpose is to study, compose, and teach story. It\u2019s more like a collection of snapshots whipping by, each one yellowed with age and fingerprints. I remember reading The Empty Space by Peter Brook on theatre theory long into the night, encountering the line, \u201cDrama was exposure, it was confrontation\u2026 it led to\u2026 an awakening of understanding.\u201d These words were what led me to the discovery of how storytelling is an emotional confrontation between the author and the writing and between the writing and the audience. It\u2019s collision. It\u2019s catharsis. Catharsis defines me as a playwright. The first play I wrote, The Rocket Man, adapted from a short story of the same name by Ray Bradbury, follows a teenage boy whose astronaut father spends much of his time in space. It\u2019s uncanny \u2013 that\u2019s my entire life. My own father travels from Denver to Los Angeles four days a week on business, and my family isn\u2019t whole unless he\u2019s with us. Drafting a scene of The Rocket Man, where the boy confronts his father before he leaves again, changed my life: Stay with me, the boy begged in the original scene I added. Please. I immediately began to cry, praying that my own father, a thousand miles away, was listening. I learned that day that catharsis is releasing my own story within a story. When the line between my soul and the soul of my story blurs, that\u2019s when the real work happens. The construct of The Rocket Man was like a cathedral, with my own emotion as stained glass and my memories as arches, but I realized after sharing it with my cast and crew for the first time that a cathedral is nothing without people to experience it. My 18-year-old male lead, Pierce, cried when he first read the script. So did my light designer and sound designer. What I want is to recreate this experience for an audience as a playwright with the intention of establishing the theatre as a safe place. You can grieve here. You can be seen here. You can hope here. This goal starts not just with craft, but with overwhelming love for that audience. At its core, storytelling is service. One of the defining challenges of my life presented itself as the opportunity to create and execute a free playwriting course for middle-school girls. When the pandemic hit, it forced me to reimagine my course for a virtual setting offered through the local school district; I realigned everything \u2013 my three-week curriculum, my downloadable course exercises, and my teaching strategies. Teaching playwriting to middle-school girls over Zoom meant listening to their struggle to make friends at school and their desire to participate in protest marches against the will of their parents. With each lesson, they experienced the transcendence of having their lives and emotions reflected through story, and they loved it. One student, Isabel, told me with ten exclamation points about how excited she was for class. She even filmed a one-woman version of a play she wrote, complete with costumes and accents. I came out of class every night feeling like I might burst from joy. Showing students how to release their own story within a story is the most purposeful thing I have ever done. Sitting in a theatre as the overture starts, hearing a thousand conversations stop in the span of a single heartbeat. A hand over my mouth in awe as I watch the finale and wonder at how a two-hour show can contain all the nuances of life. That\u2019s why I exist \u2013 to offer story. To teach it. That\u2019s my mission and my ministry.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0069", "source": "CollegeEssay"}}
{"ai_text": "Personally, I would like to say that the school which is very impressive to me is Beijing University. It is one of the most famous universities in China. There are a couple of reasons to name. The first reason I wanna say is it\u2019s beautiful, when I\u2019m free, I can walk in the woods, smell the grass and flowers, listen to the birds singing, all of these make me feel relaxed. The second reason is there are a lot of foreigners, so I can make friends with them, we can have a get together, we discuss, we talk, we laugh, all of these make my life funny. So that\u2019s why Beijing University is very impressive to me.", "human_reference": "Personally, I would like to say that the school which is very impressive to me is Beijing University. It is one of the most famous universities in China. There are a couple of reasons to name. The first reason I wanna say is it\u2019s beautiful, when I\u2019m free, I can walk in the woods, smell the grass and flowers, listen to the birds singing, all of these make me feel relaxed. The second reason is there are a lot of foreigners, so I can make friends with them, we can have a get together, we discuss, we talk, we laugh, all of these make my life funny. So that\u2019s why Beijing University is very impressive to me.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0003", "source": "TOEFL11"}}
{"ai_text": "Title: Probability-Mixing: Semi-Supervised Learning in Question-Answering with Data Augmentation\nAbstract: The probability-Mixing method proposed in this project consists  label guessing and label interpolation.  : We need to prepare three types of data for this semi-supervised problem: labeled data, mixed data, and unlabeled data. If we have labeled data \"The project is too hard\". We first use GLoVE to find similar words and replace them with something like \"That work is super difficult\", which is our unlabeled data. Then for each word, we randomly select either from word from both data and have \"That work is too difficult\". Then we can linearly interpolate the labels for the mixed data for both mean square loss and cross-entropy loss. In this project, our experiments demonstrate that sequential order information does not necessarily help query-context matching, and excessive sequential order information in BiDAF's RNN can lead to overfitting. To alleviate overfitting and add more variety to the training samples, we propose four data augmentation methods without introducing non-negligible label noise, which improves the F1 scores of BiDAF and the QANet with 8 heads by at least 2 points. We also propose the Probability-Mixing method to prevent the model from memorizing the context, which significantly improves its ability in query-context matching. This method reduces the FPR from 0.3 to 0.18 and increases F1(TP) by 4 points for the QANet model, making it a much better model in preventing the generation of misleading information for the question-answering system.", "human_reference": "Title: Probability-Mixing: Semi-Supervised Learning in Question-Answering with Data Augmentation\nAbstract: The probability-Mixing method proposed in this project consists  label guessing and label interpolation.  : We need to prepare three types of data for this semi-supervised problem: labeled data, mixed data, and unlabeled data. If we have labeled data \"The project is too hard\". We first use GLoVE to find similar words and replace them with something like \"That work is super difficult\", which is our unlabeled data. Then for each word, we randomly select either from word from both data and have \"That work is too difficult\". Then we can linearly interpolate the labels for the mixed data for both mean square loss and cross-entropy loss. In this project, our experiments demonstrate that sequential order information does not necessarily help query-context matching, and excessive sequential order information in BiDAF's RNN can lead to overfitting. To alleviate overfitting and add more variety to the training samples, we propose four data augmentation methods without introducing non-negligible label noise, which improves the F1 scores of BiDAF and the QANet with 8 heads by at least 2 points. We also propose the Probability-Mixing method to prevent the model from memorizing the context, which significantly improves its ability in query-context matching. This method reduces the FPR from 0.3 to 0.18 and increases F1(TP) by 4 points for the QANet model, making it a much better model in preventing the generation of misleading information for the question-answering system.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0130", "source": "CS224N"}}
{"ai_text": "Title: CS224N Default Final Project Report: Building a QA System Using BiDAF and Subword Modeling Techniques\nAbstract: In our project, we attempted to answer the question: How can we best adapt a baseline Bi-Directional Attention Flow (BiDAF) network to answer questions in the SQuAD dataset? Our baseline model achieved 57.54 EM and 60.90 F1 in the dev set. Based on this, we experimented with concatenating character embeddings with word embeddings and other forms of subword modeling, such as manually constructing a subword vocabulary of size 10,000 by using the Byte-Pair Encoding algorithm and splitting words into subwords. We found that using our subword embedding layer actually decreased performance, likely to due confusion generated when encountering out of vocabulary words. Our final system and best-performing model is the BiDAF network with the character embedding layer, where character and word embeddings are concatenated in equal part (50/50). Our best results achieved 60.595 EM and 63.587 F1 on the dev set and 59.222 EM and 62.662 F1 on the test set.", "human_reference": "Title: CS224N Default Final Project Report: Building a QA System Using BiDAF and Subword Modeling Techniques\nAbstract: In our project, we attempted to answer the question: How can we best adapt a baseline Bi-Directional Attention Flow (BiDAF) network to answer questions in the SQuAD dataset? Our baseline model achieved 57.54 EM and 60.90 F1 in the dev set. Based on this, we experimented with concatenating character embeddings with word embeddings and other forms of subword modeling, such as manually constructing a subword vocabulary of size 10,000 by using the Byte-Pair Encoding algorithm and splitting words into subwords. We found that using our subword embedding layer actually decreased performance, likely to due confusion generated when encountering out of vocabulary words. Our final system and best-performing model is the BiDAF network with the character embedding layer, where character and word embeddings are concatenated in equal part (50/50). Our best results achieved 60.595 EM and 63.587 F1 on the dev set and 59.222 EM and 62.662 F1 on the test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0056", "source": "CS224N"}}
{"ai_text": "Title: Augmenting BiDAF with Per-Token Features\nAbstract: The DrQA document reader showed that adding per-token features (e.g. part-of speech and  named entity recognition tags) to a question answering model significantly improves performance on the SQuAD benchmark. I add six features to a baseline BiDAF model and explore the benefit of applying attention to not only LSTM hidden state, but also these per-token features. I verify the benefit of applying self-attention to these features and find that the augmented model significantly improves upon the baseline in terms of metrics and train time. My best model achieves a test score of (62.06 EM, 64.89 F1) compared to a baseline of (59.33, 62.09), reaching an optimal model in half the training steps.", "human_reference": "Title: Augmenting BiDAF with Per-Token Features\nAbstract: The DrQA document reader showed that adding per-token features (e.g. part-of speech and  named entity recognition tags) to a question answering model significantly improves performance on the SQuAD benchmark. I add six features to a baseline BiDAF model and explore the benefit of applying attention to not only LSTM hidden state, but also these per-token features. I verify the benefit of applying self-attention to these features and find that the augmented model significantly improves upon the baseline in terms of metrics and train time. My best model achieves a test score of (62.06 EM, 64.89 F1) compared to a baseline of (59.33, 62.09), reaching an optimal model in half the training steps.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0143", "source": "CS224N"}}
{"ai_text": "Title: Experimenting with BiDAF Embeddings and Coattention\nAbstract: We are motivated by the task of question answering, which is a natural application of language models and helps evaluate how well systems understand the meaning within text. Our primary goal is to improve upon the baseline BiDAF model provided to us on the SQuAD 2.0 dataset, namely by experimenting with character-level embeddings, conditional end pointer predictions (Answer-Pointer network), self-attention, and coattention. We think that each of them leads in some way to an intuitive representation of language, linking it to larger aims within the field. Surprisingly, the coattention and self-attention modified models each score comparatively to or below the baseline model. Perhaps this hints at the importance of multiple layers for self-attention and word-to-word token interactions, as we only used one layer and a vectorized form of the original RNet self-attention paper. Our character-level embeddings + Answer-Pointer modified BiDAF performs best, scoring EM: 60.23 and F1: 63.56 on the dev set and EM: 58.715 and F1: 62.283 on the test set (compared to the baseline model with EM: 56.61 and F1: 60.24 on the dev set). The improvement might be attributed to a better understanding of out-of-vocabulary words and patterns in the grammatical structure of subsequence phrases. Compared to the baseline, the final model better predicts \"No Answer\"s and outputs semantically more logical context subsequences. However, the model still struggles with \"why\" questions and questions that contain different keywords than the context but have synonymous meaning (ex. \"extremely short\" in the context, \"not long enough\" in the question). Based on this error analysis, in the future we would love to explore euclidean distance between words and better beam search approaches to improve performance, as well as further analyze the failure cases of our self-attention / coattention implementations.", "human_reference": "Title: Experimenting with BiDAF Embeddings and Coattention\nAbstract: We are motivated by the task of question answering, which is a natural application of language models and helps evaluate how well systems understand the meaning within text. Our primary goal is to improve upon the baseline BiDAF model provided to us on the SQuAD 2.0 dataset, namely by experimenting with character-level embeddings, conditional end pointer predictions (Answer-Pointer network), self-attention, and coattention. We think that each of them leads in some way to an intuitive representation of language, linking it to larger aims within the field. Surprisingly, the coattention and self-attention modified models each score comparatively to or below the baseline model. Perhaps this hints at the importance of multiple layers for self-attention and word-to-word token interactions, as we only used one layer and a vectorized form of the original RNet self-attention paper. Our character-level embeddings + Answer-Pointer modified BiDAF performs best, scoring EM: 60.23 and F1: 63.56 on the dev set and EM: 58.715 and F1: 62.283 on the test set (compared to the baseline model with EM: 56.61 and F1: 60.24 on the dev set). The improvement might be attributed to a better understanding of out-of-vocabulary words and patterns in the grammatical structure of subsequence phrases. Compared to the baseline, the final model better predicts \"No Answer\"s and outputs semantically more logical context subsequences. However, the model still struggles with \"why\" questions and questions that contain different keywords than the context but have synonymous meaning (ex. \"extremely short\" in the context, \"not long enough\" in the question). Based on this error analysis, in the future we would love to explore euclidean distance between words and better beam search approaches to improve performance, as well as further analyze the failure cases of our self-attention / coattention implementations.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0052", "source": "CS224N"}}
{"ai_text": "Title: Robust QA System with xEDA: Final Report\nAbstract: We present xEDA: extended easy data augmentation techniques for boosting the robustness of question answering systems to shifts in data domains. xEDA extends existing data augmentation techniques by drawing inspirations from techniques in computer vision. We evaluate its performance on out-of-domain question answering tasks and show that xEDA can improve performance and robustness to domain shifts when a small subset of the out-of-domain data is available at train time. xEDA consists of masking, extended random deletion, extended random insertion, and simple extended random insertion. We discovered that xEDA can help build a question answering system that is robust to shifts in domain distributions if few samples of out-of-domain datasets are available at train time. In particular, by applying xEDA to out-of-domain datasets during training, we were able to increase the performance of our question answering system by 6.1% in terms of F1 and by 14.9% in terms of EM when compared to the provided baseline on the dev set. Moreover, using 40% of the out-of-domain train datasets augmented via xEDA achieved the same performance as using 100% of the out-of-domain train datasets. Our analysis also suggests that an augmented data of smaller size may lead to better performance than non-augmented data of larger size in some cases. Given the simplicity and wide applicability of xEDA, we hope that this paper motivates researchers and practitioners to explore data augmentation techniques in complex NLP tasks.", "human_reference": "Title: Robust QA System with xEDA: Final Report\nAbstract: We present xEDA: extended easy data augmentation techniques for boosting the robustness of question answering systems to shifts in data domains. xEDA extends existing data augmentation techniques by drawing inspirations from techniques in computer vision. We evaluate its performance on out-of-domain question answering tasks and show that xEDA can improve performance and robustness to domain shifts when a small subset of the out-of-domain data is available at train time. xEDA consists of masking, extended random deletion, extended random insertion, and simple extended random insertion. We discovered that xEDA can help build a question answering system that is robust to shifts in domain distributions if few samples of out-of-domain datasets are available at train time. In particular, by applying xEDA to out-of-domain datasets during training, we were able to increase the performance of our question answering system by 6.1% in terms of F1 and by 14.9% in terms of EM when compared to the provided baseline on the dev set. Moreover, using 40% of the out-of-domain train datasets augmented via xEDA achieved the same performance as using 100% of the out-of-domain train datasets. Our analysis also suggests that an augmented data of smaller size may lead to better performance than non-augmented data of larger size in some cases. Given the simplicity and wide applicability of xEDA, we hope that this paper motivates researchers and practitioners to explore data augmentation techniques in complex NLP tasks.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0109", "source": "CS224N"}}
{"ai_text": "Title: Gaining More from Less Data in out-of-domain Question Answering Models\nAbstract: We propose text augmentation techniques for Question Answering task in NLP that involves using synonyms with stochasticity on out-of-domain datasets (DuoRC and RACE and RelationExtraction) that are set to be 400 times smaller than the in-domain datasets (SQuAD, NewsQA, NaturalQuestions). We illustrate QSR, SIBA, SIAA, CCS and CD augmentation strategies above, that help improve extraction of generalized information from out-of-domain or less available datasets from large pre-trained models BERT variant DistilBERT being able to benefit from producing QA applications across domains. It is found that augmenting less available QA datasets in a way described, indicate improvement in generalization, but not all augmentations strategies are equally good, a combination of 3x QSR, 3x SIBA, 3x SIAA, 3x CCS performed the best (as illustrated above) with exclusion of CD (this negatively impacted scores). We also define a metric EM+ (exact match plus) that is a binary measure if prediction is a superset of the answer, EM+ = 1, else 0; provides a less overfit-perspective as a performance metric than EM. We conjecture from analysis done in the paper that increasing unique words in OOD that aren't present in ID, help improve with performance.", "human_reference": "Title: Gaining More from Less Data in out-of-domain Question Answering Models\nAbstract: We propose text augmentation techniques for Question Answering task in NLP that involves using synonyms with stochasticity on out-of-domain datasets (DuoRC and RACE and RelationExtraction) that are set to be 400 times smaller than the in-domain datasets (SQuAD, NewsQA, NaturalQuestions). We illustrate QSR, SIBA, SIAA, CCS and CD augmentation strategies above, that help improve extraction of generalized information from out-of-domain or less available datasets from large pre-trained models BERT variant DistilBERT being able to benefit from producing QA applications across domains. It is found that augmenting less available QA datasets in a way described, indicate improvement in generalization, but not all augmentations strategies are equally good, a combination of 3x QSR, 3x SIBA, 3x SIAA, 3x CCS performed the best (as illustrated above) with exclusion of CD (this negatively impacted scores). We also define a metric EM+ (exact match plus) that is a binary measure if prediction is a superset of the answer, EM+ = 1, else 0; provides a less overfit-perspective as a performance metric than EM. We conjecture from analysis done in the paper that increasing unique words in OOD that aren't present in ID, help improve with performance.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0025", "source": "CS224N"}}
{"ai_text": "Growing up with a stutter was difficult. My thoughts were always fully formed, but the words would get stuck. The harder I tried to force them out, the more they would jam up, like train cars going off a track. Sometimes kids would laugh or make fun of my stutter, but mostly they just ignored me. They had better things to do than wait to hear my ideas, so they assumed I had none worth hearing. Eventually, I gave up talking to people entirely. They thought I was stupid, and, soon enough, I thought so too. My parents signed me up to play soccer in the hopes that being part of a team would help me make friends. I wasn\u2019t particularly athletic, and I could barely kick a ball. But soccer is a game anyone can play. On the field, my stutter didn\u2019t matter. I discovered that I could lead, quietly, through action, rather than words. Eventually, speech therapy resolved by stutter, but it was on the soccer field that I learned how to be me. One of my teammates had a brother with Down Syndrome. Every season, he came to our games to watch from the sidelines. He had cleats and a ball that he kicked around by himself, but because he also had an intellectual disability, there was no room for him on our team or anywhere else. I realized that although soccer is a sport for everyone, not everyone is included in the sport. I understood the pain of being excluded simply because of an inability to communicate. So, in February of 2015, I launched GOALS (Giving Opportunities to All who Love Soccer), a unified soccer program for kids with and without special needs. GOALS partners youth athletes who have intellectual disabilities with neurotypical peer buddies. The athletes and buddies play together, as unified pairs, in small-sided, non-competitive scrimmages. The first GOALS program had just nine participants. Today, we hold GOALS events twice a month for fifty players of all abilities. GOALS has impacted over 400 kids and is now an official partner of Special Olympics Arizona. But I don\u2019t measure the success of GOALS by the numbers. Success comes in the form of an athlete like Josh*, who came reluctantly to his first GOALS event, having never tried sports before, preferring instead to play video games by himself. Josh was surprised to find that he loves soccer, and he looks forward to playing with his friends at every GOALS event, where his diagnosis of autism doesn\u2019t define him. What makes GOALS special is that it is not a community service program for kids with intellectual disabilities. GOALS is a program that serves the entire community, understanding that our community includes people of all abilities. GOALS champions people for what they bring to the community, rather than defining them by what they take. GOALS breaks down the barriers that separate kids with special needs from their neurotypical peers, creating intentional connections that allow true friendships to develop. For many kids, GOALS is the first time they experience genuine acceptance. Through sports, we have the capacity to create something better than tolerance. Tolerance is for summer heat, visits to the doctor\u2019s office, and lines at the bank. People, though, deserve more than tolerance. People deserve acceptance. So often, kids with intellectual disabilities are isolated socially, for no other reason than that kids pass them by. But special needs kids can be incredibly smart and talented. We just need to slow down long enough to get to know them. Some need more time to turn their ideas into words, but we can afford to have those slower conversations. That is why, for me, unified sports will be a lifelong passion. Through sports, anyone can communicate that each of us is valued, each of us is a part of the time, and each of us is a friend worth having.", "human_reference": "Growing up with a stutter was difficult. My thoughts were always fully formed, but the words would get stuck. The harder I tried to force them out, the more they would jam up, like train cars going off a track. Sometimes kids would laugh or make fun of my stutter, but mostly they just ignored me. They had better things to do than wait to hear my ideas, so they assumed I had none worth hearing. Eventually, I gave up talking to people entirely. They thought I was stupid, and, soon enough, I thought so too. My parents signed me up to play soccer in the hopes that being part of a team would help me make friends. I wasn\u2019t particularly athletic, and I could barely kick a ball. But soccer is a game anyone can play. On the field, my stutter didn\u2019t matter. I discovered that I could lead, quietly, through action, rather than words. Eventually, speech therapy resolved by stutter, but it was on the soccer field that I learned how to be me. One of my teammates had a brother with Down Syndrome. Every season, he came to our games to watch from the sidelines. He had cleats and a ball that he kicked around by himself, but because he also had an intellectual disability, there was no room for him on our team or anywhere else. I realized that although soccer is a sport for everyone, not everyone is included in the sport. I understood the pain of being excluded simply because of an inability to communicate. So, in February of 2015, I launched GOALS (Giving Opportunities to All who Love Soccer), a unified soccer program for kids with and without special needs. GOALS partners youth athletes who have intellectual disabilities with neurotypical peer buddies. The athletes and buddies play together, as unified pairs, in small-sided, non-competitive scrimmages. The first GOALS program had just nine participants. Today, we hold GOALS events twice a month for fifty players of all abilities. GOALS has impacted over 400 kids and is now an official partner of Special Olympics Arizona. But I don\u2019t measure the success of GOALS by the numbers. Success comes in the form of an athlete like Josh*, who came reluctantly to his first GOALS event, having never tried sports before, preferring instead to play video games by himself. Josh was surprised to find that he loves soccer, and he looks forward to playing with his friends at every GOALS event, where his diagnosis of autism doesn\u2019t define him. What makes GOALS special is that it is not a community service program for kids with intellectual disabilities. GOALS is a program that serves the entire community, understanding that our community includes people of all abilities. GOALS champions people for what they bring to the community, rather than defining them by what they take. GOALS breaks down the barriers that separate kids with special needs from their neurotypical peers, creating intentional connections that allow true friendships to develop. For many kids, GOALS is the first time they experience genuine acceptance. Through sports, we have the capacity to create something better than tolerance. Tolerance is for summer heat, visits to the doctor\u2019s office, and lines at the bank. People, though, deserve more than tolerance. People deserve acceptance. So often, kids with intellectual disabilities are isolated socially, for no other reason than that kids pass them by. But special needs kids can be incredibly smart and talented. We just need to slow down long enough to get to know them. Some need more time to turn their ideas into words, but we can afford to have those slower conversations. That is why, for me, unified sports will be a lifelong passion. Through sports, anyone can communicate that each of us is valued, each of us is a part of the time, and each of us is a friend worth having.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0063", "source": "CollegeEssay"}}
{"ai_text": "Title: Better Learning with Lesser Data: Meta-Learning with DistiIBERT\nAbstract: While pre-trained transformer models have shown great success in recent years, it requires a large amount of task-specific data to finetune. In our project, we have experimented with the a variant of the MAML algorithm, namely Reptile, in a low resource QA program. In contrast to the normal training procedure, MAML algorithm trains the model with a double-loop structure. In the inner loop, the program goes through meta-batches, with T tasks in each. For each of the tasks in the inner-loop, a submodel is made and updates k times. After the k descents have been made for T submodels, they are collected and processed in the Metalearner Reptile, where the next descent on the meta-model is determined. From the first glance of this training protocol, it appears to be similar to the multi-task learning model, since they both expose the model to multiple tasks, which enables transfer learning. Aside from that, one major distinction of MAML is that it makes use of the k th gradients, which enables the SGD to access higher order terms in the loss function, thereby allowing the MAML algorithm to find a better initialization than the other methods and descends at much rapid rate in any tasks in the downstream, as shown in figure (1). Furthermore, the reason MAML can find better model initialization than multi-task learning is that it can avoid overfitting to any one task, which is known to be a tendency in multi-task learning. In the end of the study, we introduce a cost-to-improvement ratio, evaluating whether the additional accuracy gain in MAML can justify the increase in runtime. Despite there is a absolute gain in the accuracy by MAML, we express our reservation in regard to the comparative advantage of MAML, since this 1 point increase in the accuracy comes at large sacrifice of runtime.", "human_reference": "Title: Better Learning with Lesser Data: Meta-Learning with DistiIBERT\nAbstract: While pre-trained transformer models have shown great success in recent years, it requires a large amount of task-specific data to finetune. In our project, we have experimented with the a variant of the MAML algorithm, namely Reptile, in a low resource QA program. In contrast to the normal training procedure, MAML algorithm trains the model with a double-loop structure. In the inner loop, the program goes through meta-batches, with T tasks in each. For each of the tasks in the inner-loop, a submodel is made and updates k times. After the k descents have been made for T submodels, they are collected and processed in the Metalearner Reptile, where the next descent on the meta-model is determined. From the first glance of this training protocol, it appears to be similar to the multi-task learning model, since they both expose the model to multiple tasks, which enables transfer learning. Aside from that, one major distinction of MAML is that it makes use of the k th gradients, which enables the SGD to access higher order terms in the loss function, thereby allowing the MAML algorithm to find a better initialization than the other methods and descends at much rapid rate in any tasks in the downstream, as shown in figure (1). Furthermore, the reason MAML can find better model initialization than multi-task learning is that it can avoid overfitting to any one task, which is known to be a tendency in multi-task learning. In the end of the study, we introduce a cost-to-improvement ratio, evaluating whether the additional accuracy gain in MAML can justify the increase in runtime. Despite there is a absolute gain in the accuracy by MAML, we express our reservation in regard to the comparative advantage of MAML, since this 1 point increase in the accuracy comes at large sacrifice of runtime.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0117", "source": "CS224N"}}
{"ai_text": "I imagine my life ten years in the future to be free. My life will be different in one way from now that I don\u2019t have to be tied to a job. I can travel around the world, go anywhere, see different things, meet different people, broaden my horizon and thus feel the world. I don\u2019t have to confine myself to just one place. I can go to France, South Africa, South America and Islands in the Pacific. In addition, I can be a freelancer, live on my royalties. I will have plenty of time to write novels and read all the masterpieces. This is what I imagine my life will be in ten years.", "human_reference": "I imagine my life ten years in the future to be free. My life will be different in one way from now that I don\u2019t have to be tied to a job. I can travel around the world, go anywhere, see different things, meet different people, broaden my horizon and thus feel the world. I don\u2019t have to confine myself to just one place. I can go to France, South Africa, South America and Islands in the Pacific. In addition, I can be a freelancer, live on my royalties. I will have plenty of time to write novels and read all the masterpieces. This is what I imagine my life will be in ten years.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0002", "source": "TOEFL11"}}
{"ai_text": "Title: Question Answering with Co-attention and Transformer\nAbstract: in this project, we implemented several improvements of question answering system based on SQuAD database including: 1) QANet 2) coattention 3) RNet. We built the models from scratch and evaluated against the EM and F1 scores. Our main goal is to explore through various techniques in the Question Answering System. In this process, we were able to practice our skills of implementing complex models according to their descriptions  in literatures.\n\nWe first implemented the co-attention layer, which did not improve the model performance. We then added character-level embeddings to the baseline model which improved the EM score to 60.59 and F1 score to 64.17.\n\nAfter that we implemented QANet which used convolutions to capture the local structure of the context and self-attention mechanism to model the global interactions between text. We built the QANet incrementally and implemented several model components. We eventually saw major improvements in both EM and F1 scores (64.49 and 69.62) compared to the baseline BiDAF model and BiDAF with character-level embeddings.\n\nAt the same time, we implemented the Self Matching layer and the Pointer Network described in the RNet paper. The self-matching mechanism helps refine the attention representation by matching the passage against itself, which effectively encodes information from the whole passage.  This is implemented on the top of character-level embeddings and the baseline. We tested several modifications of the RNet architecture including different gate attention recurrent network and output layer. While Self Matching improved the performance, the Pointer Network caused vanishing gradients. The self-matching layer combined with character-level embeddings improved the performance to 62.06(EM) and 65.53(F1).\n\nAmong all techniques, QANet gives the best performance, and to our understanding, the reason is that the QANet can capture the local and global interaction at the same time with its complex model architecture containing both convolutions and attention-mechanism.", "human_reference": "Title: Question Answering with Co-attention and Transformer\nAbstract: in this project, we implemented several improvements of question answering system based on SQuAD database including: 1) QANet 2) coattention 3) RNet. We built the models from scratch and evaluated against the EM and F1 scores. Our main goal is to explore through various techniques in the Question Answering System. In this process, we were able to practice our skills of implementing complex models according to their descriptions  in literatures.\n\nWe first implemented the co-attention layer, which did not improve the model performance. We then added character-level embeddings to the baseline model which improved the EM score to 60.59 and F1 score to 64.17.\n\nAfter that we implemented QANet which used convolutions to capture the local structure of the context and self-attention mechanism to model the global interactions between text. We built the QANet incrementally and implemented several model components. We eventually saw major improvements in both EM and F1 scores (64.49 and 69.62) compared to the baseline BiDAF model and BiDAF with character-level embeddings.\n\nAt the same time, we implemented the Self Matching layer and the Pointer Network described in the RNet paper. The self-matching mechanism helps refine the attention representation by matching the passage against itself, which effectively encodes information from the whole passage.  This is implemented on the top of character-level embeddings and the baseline. We tested several modifications of the RNet architecture including different gate attention recurrent network and output layer. While Self Matching improved the performance, the Pointer Network caused vanishing gradients. The self-matching layer combined with character-level embeddings improved the performance to 62.06(EM) and 65.53(F1).\n\nAmong all techniques, QANet gives the best performance, and to our understanding, the reason is that the QANet can capture the local and global interaction at the same time with its complex model architecture containing both convolutions and attention-mechanism.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0080", "source": "CS224N"}}
{"ai_text": "I prefer studying in traditional classroom. We know that for many situations, studying is about cooperation and communication, which can be seriously affected if you are studying alone at home. And when you have problems, it's obviously more efficient to discuss with other people, they may provide another respect of thinking. Studies also show that people are likely to lose focus when working alone. And besides, studying in classroom can help make friends, which can make you love more about your work. People are social animals, spending too much time alone is not healthy for our minds.", "human_reference": "I prefer studying in traditional classroom. We know that for many situations, studying is about cooperation and communication, which can be seriously affected if you are studying alone at home. And when you have problems, it's obviously more efficient to discuss with other people, they may provide another respect of thinking. Studies also show that people are likely to lose focus when working alone. And besides, studying in classroom can help make friends, which can make you love more about your work. People are social animals, spending too much time alone is not healthy for our minds.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0080", "source": "TOEFL11"}}
{"ai_text": "Title: SQuAD 2.0: Improving Performance with Optimization and Feature Engineering\nAbstract: In this project, we significantly improved baseline performance on the SQuAD 2.0 question answering task through optimization and feature engineering. Instead of overhauling the original BiDAF network architecture, we focused on extracting as much information as possible from the input data, taking inspiration from the DrQA document reader. We first constructed character-level word embeddings via a 1D Convolutional Neural Network, and then added token and exact match features for both the context and question words. We also conducted thorough hyperparameter searches and experimented with various encoding methods, projection, and drop-out layers. Ensembling our best models by majority vote achieved validation set F1 and EM scores over 7 points higher than the baseline with comparable test set performance (F1=68.753, EM=65.714). Our findings suggest that feature engineering is a particularly effective approach to improve model performance in the absence of pretraining.", "human_reference": "Title: SQuAD 2.0: Improving Performance with Optimization and Feature Engineering\nAbstract: In this project, we significantly improved baseline performance on the SQuAD 2.0 question answering task through optimization and feature engineering. Instead of overhauling the original BiDAF network architecture, we focused on extracting as much information as possible from the input data, taking inspiration from the DrQA document reader. We first constructed character-level word embeddings via a 1D Convolutional Neural Network, and then added token and exact match features for both the context and question words. We also conducted thorough hyperparameter searches and experimented with various encoding methods, projection, and drop-out layers. Ensembling our best models by majority vote achieved validation set F1 and EM scores over 7 points higher than the baseline with comparable test set performance (F1=68.753, EM=65.714). Our findings suggest that feature engineering is a particularly effective approach to improve model performance in the absence of pretraining.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0057", "source": "CS224N"}}
{"ai_text": "If I had to choose between team sports and exercising alone, I\u2019d definitely play a team sport, like volleyball, my favorite. It\u2019s more enjoyable to spend time with people. There are a couple of reasons I say that. When you\u2019re, uh, with people, you can share the experience, for one. I mean, you can talk, or maybe joke, like, if you\u2019re playing volleyball, say, and you make a mistake\u2014you drop the ball, for instance\u2014your friends can reassure you. But besides that, friends can help motivate you. When you\u2019re exercising alone, on the other hand, you need to motivate yourself. So, my main point is that exercising with people makes the time spent more fun.", "human_reference": "If I had to choose between team sports and exercising alone, I\u2019d definitely play a team sport, like volleyball, my favorite. It\u2019s more enjoyable to spend time with people. There are a couple of reasons I say that. When you\u2019re, uh, with people, you can share the experience, for one. I mean, you can talk, or maybe joke, like, if you\u2019re playing volleyball, say, and you make a mistake\u2014you drop the ball, for instance\u2014your friends can reassure you. But besides that, friends can help motivate you. When you\u2019re exercising alone, on the other hand, you need to motivate yourself. So, my main point is that exercising with people makes the time spent more fun.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0079", "source": "TOEFL11"}}
{"ai_text": "Title: Robust QA System with Task-Adaptive Pretraining, Data Augmentation, and Hyperparameter Tuning\nAbstract: Despite their significant success, transformer-based models trained on massive amounts of text still lack robustness to out-of-distribution data. In this project, we aim to build a robust question answering system by improving the DistilBERT model. To accomplish this goal, we implement task-adaptive pretraining (TAPT), model tuning such as transformer block re-initialization and increasing the number of training epochs, and ensemble methods. We also use data augmentation techniques to enable the model to generalize well even with limited data in the domains of interest.", "human_reference": "Title: Robust QA System with Task-Adaptive Pretraining, Data Augmentation, and Hyperparameter Tuning\nAbstract: Despite their significant success, transformer-based models trained on massive amounts of text still lack robustness to out-of-distribution data. In this project, we aim to build a robust question answering system by improving the DistilBERT model. To accomplish this goal, we implement task-adaptive pretraining (TAPT), model tuning such as transformer block re-initialization and increasing the number of training epochs, and ensemble methods. We also use data augmentation techniques to enable the model to generalize well even with limited data in the domains of interest.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0102", "source": "CS224N"}}
{"ai_text": "My experiences in an internship was rather challenging. My major job was to work with some Nigerian students. My English was poor at that time, and you know Nigerian English is a lot different from American English. In the first two weeks I can hardly understand what they are talking about let alone working with them. So I walked with them every day after work, asking about their culture, and introducing China to them. And I was grateful that they are really nice people, they helped me a lot. At the end of the summer, we've become good friends and kept contact ever since.", "human_reference": "My experiences in an internship was rather challenging. My major job was to work with some Nigerian students. My English was poor at that time, and you know Nigerian English is a lot different from American English. In the first two weeks I can hardly understand what they are talking about let alone working with them. So I walked with them every day after work, asking about their culture, and introducing China to them. And I was grateful that they are really nice people, they helped me a lot. At the end of the summer, we've become good friends and kept contact ever since.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0007", "source": "TOEFL11"}}
{"ai_text": "I sat on my parents\u2019 bed weeping with my head resting on my knees. \u201cWhy did you have to do that to me? Why did you have to show me the house and then take it away from me?\u201d Hopelessly, I found myself praying to God realizing it was my last resort. For years, my family and I found ourselves moving from country to country in hopes of a better future. Factors, such as war and lack of academic opportunities, led my parents to pack their bags and embark on a new journey for our family around the world. Our arduous journey first began in Ku\u00e7ov\u00eb, Albania, then Athens, Greece, and then eventually, Boston, Massachusetts. Throughout those years, although my family always had a roof over our heads, I never had a place I could call \u201chome.\u201d That night that I prayed to God, my mind raced back to the night I was clicking the delete button on my e-mails, but suddenly stopped when I came upon a listing of the house. It was September 22, 2007 \u2014eight years exactly to the day that my family and I had moved to the United States. Instantly, I knew that it was fate that was bringing this house to me. I remembered visiting that yellow house the next day with my parents and falling in love with it. However, I also remembered the heartbreaking phone call I received later on that week saying that the owners had chosen another family\u2019s offer. A week after I had prayed to God, I had given up any hopes of my family buying the house. One day after school, I unlocked the door to our one-bedroom apartment and walked over to the telephone only to see it flashing a red light. I clicked PLAY and unexpectedly heard the voice of our real estate agent. \u201cEda!\u201d she said joyfully. \u201cThe deal fell through with the other family\u2014the house is yours! Call me back immediately to get started on the papers.\u201d For a moment, I stood agape and kept replaying the words in my head. Was this really happening to me? Was my dream of owning a home finally coming true? Over the month of November, I spent my days going to school and immediately rushing home to make phone calls. Although my parents were not fluent enough in English to communicate with the bank and real estate agent, I knew that I was not going to allow this obstacle to hinder my dream of helping to purchase a home for my family. Thus, unlike a typical thirteen-year-old girl\u2019s conversations, my phone calls did not involve the mention of makeup, shoes, or boys. Instead, my conversations were composed of terms, such as \u201cfixed-rate mortgages,\u201d \u201cpreapprovals,\u201d and \u201cdown payments.\u201d Nevertheless, I was determined to help purchase this home after thirteen years of feeling embarrassed from living in a one-bedroom apartment. No longer was I going to experience feelings of humiliation from not being able to host sleepovers with my friends or from not being able to gossip with girls in school about who had the prettiest room color. I had been homeless for the first thirteen years of my life. Although I will never be able to fully repay my parents for all of their sacrifices, the least I could do was to help find them a home that they could call their own\u2014and that year, I did. To me, a home means more than the general conception of \u201cfour walls and a roof.\u201d A home is a place filled with memories and laughter from my family. No matter where my future may lead me, I know that if at times I feel alone, I will always have a yellow home with my family inside waiting for me.", "human_reference": "I sat on my parents\u2019 bed weeping with my head resting on my knees. \u201cWhy did you have to do that to me? Why did you have to show me the house and then take it away from me?\u201d Hopelessly, I found myself praying to God realizing it was my last resort. For years, my family and I found ourselves moving from country to country in hopes of a better future. Factors, such as war and lack of academic opportunities, led my parents to pack their bags and embark on a new journey for our family around the world. Our arduous journey first began in Ku\u00e7ov\u00eb, Albania, then Athens, Greece, and then eventually, Boston, Massachusetts. Throughout those years, although my family always had a roof over our heads, I never had a place I could call \u201chome.\u201d That night that I prayed to God, my mind raced back to the night I was clicking the delete button on my e-mails, but suddenly stopped when I came upon a listing of the house. It was September 22, 2007 \u2014eight years exactly to the day that my family and I had moved to the United States. Instantly, I knew that it was fate that was bringing this house to me. I remembered visiting that yellow house the next day with my parents and falling in love with it. However, I also remembered the heartbreaking phone call I received later on that week saying that the owners had chosen another family\u2019s offer. A week after I had prayed to God, I had given up any hopes of my family buying the house. One day after school, I unlocked the door to our one-bedroom apartment and walked over to the telephone only to see it flashing a red light. I clicked PLAY and unexpectedly heard the voice of our real estate agent. \u201cEda!\u201d she said joyfully. \u201cThe deal fell through with the other family\u2014the house is yours! Call me back immediately to get started on the papers.\u201d For a moment, I stood agape and kept replaying the words in my head. Was this really happening to me? Was my dream of owning a home finally coming true? Over the month of November, I spent my days going to school and immediately rushing home to make phone calls. Although my parents were not fluent enough in English to communicate with the bank and real estate agent, I knew that I was not going to allow this obstacle to hinder my dream of helping to purchase a home for my family. Thus, unlike a typical thirteen-year-old girl\u2019s conversations, my phone calls did not involve the mention of makeup, shoes, or boys. Instead, my conversations were composed of terms, such as \u201cfixed-rate mortgages,\u201d \u201cpreapprovals,\u201d and \u201cdown payments.\u201d Nevertheless, I was determined to help purchase this home after thirteen years of feeling embarrassed from living in a one-bedroom apartment. No longer was I going to experience feelings of humiliation from not being able to host sleepovers with my friends or from not being able to gossip with girls in school about who had the prettiest room color. I had been homeless for the first thirteen years of my life. Although I will never be able to fully repay my parents for all of their sacrifices, the least I could do was to help find them a home that they could call their own\u2014and that year, I did. To me, a home means more than the general conception of \u201cfour walls and a roof.\u201d A home is a place filled with memories and laughter from my family. No matter where my future may lead me, I know that if at times I feel alone, I will always have a yellow home with my family inside waiting for me.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0028", "source": "CollegeEssay"}}
{"ai_text": "Title: Question-Answering with QANet for SQUAD 2.0\nAbstract: Our task for this project is to is to design a question-answering system for the SQuAD 2.0 dataset that improves upon the BiDAF baseline model. To do this, we experiment with QANet, a transformer-based architecture. We also reintroduce a character-level embeddings on top of the provided BiDAF model, as well as a self-attention layer. Our best QANet model achieved 61.47/64.81 EM/F1 scores on the test set.", "human_reference": "Title: Question-Answering with QANet for SQUAD 2.0\nAbstract: Our task for this project is to is to design a question-answering system for the SQuAD 2.0 dataset that improves upon the BiDAF baseline model. To do this, we experiment with QANet, a transformer-based architecture. We also reintroduce a character-level embeddings on top of the provided BiDAF model, as well as a self-attention layer. Our best QANet model achieved 61.47/64.81 EM/F1 scores on the test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0121", "source": "CS224N"}}
{"ai_text": "On Tuesdays and Thursdays, I sit in soil pulling crab grass and borage. I've been a farmer since sophomore year. The farm--managed by my school--is a one-acre plot more accurately described as a garden with chickens. My task today is to pick cherry tomatoes, most of which have ripened. I grab a tray from the shed and walk across pathways to the vine. I created these pathways during junior year, shoveling large heaps of wood-chips into a wheelbarrow, then raking these chips onto the pathways between beds. Our two tomato vines stand three feet tall and extend horizontally at least six feet; they are heavy with small red and orange glistening spheres. I fall into a rhythm, plucking and setting tomatoes in the container, eating several here and there. I recall when I was six, my Mom would send my twin brother and me to the backyard to weed dandelions. We would get distracted and play with our dog or climb the dogwood tree. I recall the awe I felt last week when I harvested a giant sunflower, discovering at least ten potatoes growing in its roots, or when I found a sweet potato the size of a football. I had planted the seed potato pieces last year. I think about jalapenos, how scratches on their skin indicate spiciness level. The satisfaction I felt the first time I ate a piece of food I grew at the farm, a raw green-bean. The pleasure I feel knowing friends and teachers also eat the food I grow; we donate the farm's produce to our school's dining hall and sell it at the weekly farmer's market in the parking lot. After farm, I will work a shift at the Farmer's Market. I will sit, perhaps eating Thai iced-tea-flavored ice cream from another stand, ready to explain where the farm is located, who works it, what we do with unsold food, and, finally, whether the price for a head of lettuce is negotiable (it is). Sometimes, I remember farmers I met during an exchange trip to Yangshuo, China, who were selling pomelos and bamboo shoots. I think about how to me, the difference between one-versus-two dollars for pomelos seems miniscule, but for those farmers, it means a lot. They rely solely on farming to feed their families; I farm for the pleasure of learning what they do out of necessity. As I carry my share of tomatoes to the shed - tomatoes I nurtured from seeds into sprouts into fruits \u2013 I contemplate how much farm has done for me. I can't sit down to a meal without imagining the plants on my plate as seeds and then sprouts, without wondering about the many hands that brought them to my table. Education, to me, means understanding the hidden processes that make up daily life. Playing with the farm chickens - Pablo, Claude, Vincent, Leonardo - and knowing how the coating around an egg works as a natural preservative makes me appreciate my omelet a tad more. Watching weeds that I pulled from various beds slowly decompose into fertilizer in the compost pile makes me consider the roles carbon and nitrogen cycles play in that process. Although I initially joined farm because I wanted to try something new, I quickly found that the work offers a balance with the intellectual work of the rest of my day. The farm connects education with experience; teaching me to see the application of my classroom learning in a real setting. Being able to see the relevance of what I am studying piques my curiosity. I aspire to maintain this connection between education and experience throughout my life, and will always find ways to contribute to my community, locally or globally. I will look for soil to cultivate, using my learning to see and understand more of the world, whether it be the natural environment or the way people live.", "human_reference": "On Tuesdays and Thursdays, I sit in soil pulling crab grass and borage. I've been a farmer since sophomore year. The farm--managed by my school--is a one-acre plot more accurately described as a garden with chickens. My task today is to pick cherry tomatoes, most of which have ripened. I grab a tray from the shed and walk across pathways to the vine. I created these pathways during junior year, shoveling large heaps of wood-chips into a wheelbarrow, then raking these chips onto the pathways between beds. Our two tomato vines stand three feet tall and extend horizontally at least six feet; they are heavy with small red and orange glistening spheres. I fall into a rhythm, plucking and setting tomatoes in the container, eating several here and there. I recall when I was six, my Mom would send my twin brother and me to the backyard to weed dandelions. We would get distracted and play with our dog or climb the dogwood tree. I recall the awe I felt last week when I harvested a giant sunflower, discovering at least ten potatoes growing in its roots, or when I found a sweet potato the size of a football. I had planted the seed potato pieces last year. I think about jalapenos, how scratches on their skin indicate spiciness level. The satisfaction I felt the first time I ate a piece of food I grew at the farm, a raw green-bean. The pleasure I feel knowing friends and teachers also eat the food I grow; we donate the farm's produce to our school's dining hall and sell it at the weekly farmer's market in the parking lot. After farm, I will work a shift at the Farmer's Market. I will sit, perhaps eating Thai iced-tea-flavored ice cream from another stand, ready to explain where the farm is located, who works it, what we do with unsold food, and, finally, whether the price for a head of lettuce is negotiable (it is). Sometimes, I remember farmers I met during an exchange trip to Yangshuo, China, who were selling pomelos and bamboo shoots. I think about how to me, the difference between one-versus-two dollars for pomelos seems miniscule, but for those farmers, it means a lot. They rely solely on farming to feed their families; I farm for the pleasure of learning what they do out of necessity. As I carry my share of tomatoes to the shed - tomatoes I nurtured from seeds into sprouts into fruits \u2013 I contemplate how much farm has done for me. I can't sit down to a meal without imagining the plants on my plate as seeds and then sprouts, without wondering about the many hands that brought them to my table. Education, to me, means understanding the hidden processes that make up daily life. Playing with the farm chickens - Pablo, Claude, Vincent, Leonardo - and knowing how the coating around an egg works as a natural preservative makes me appreciate my omelet a tad more. Watching weeds that I pulled from various beds slowly decompose into fertilizer in the compost pile makes me consider the roles carbon and nitrogen cycles play in that process. Although I initially joined farm because I wanted to try something new, I quickly found that the work offers a balance with the intellectual work of the rest of my day. The farm connects education with experience; teaching me to see the application of my classroom learning in a real setting. Being able to see the relevance of what I am studying piques my curiosity. I aspire to maintain this connection between education and experience throughout my life, and will always find ways to contribute to my community, locally or globally. I will look for soil to cultivate, using my learning to see and understand more of the world, whether it be the natural environment or the way people live.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0042", "source": "CollegeEssay"}}
{"ai_text": "Title: QANet on SQUAD 2.0\nAbstract: QANe\u00e9et achieved the state of the art prior to BERT on the SQUAD 2.0. The project aims to reimplemement QANet based on a model from the Attention is All You Need paper. We also revised the provided BiDAF model by adding a character embedding layer. We find that with the character embedding layer, BiDAF model is significantly improved, and we show that ensembling the QANet and BiDAF model can evidently improve the performance on the SQUAD 2.0 dataset.", "human_reference": "Title: QANet on SQUAD 2.0\nAbstract: QANe\u00e9et achieved the state of the art prior to BERT on the SQUAD 2.0. The project aims to reimplemement QANet based on a model from the Attention is All You Need paper. We also revised the provided BiDAF model by adding a character embedding layer. We find that with the character embedding layer, BiDAF model is significantly improved, and we show that ensembling the QANet and BiDAF model can evidently improve the performance on the SQUAD 2.0 dataset.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0139", "source": "CS224N"}}
{"ai_text": "Title: Meta Learning on Topics as Tasks for Robust QA Performance\nAbstract: A key pain point of current neural QA-focused NLP systems is the lack of generalization \u2014 often these systems learn parameters that fail to generalize to neverbefore-seen data domains, unlike how humans can take previous knowledge and build accurate inferences beyond \"training\" data distributions. Clearly, advances in meta-learning have shown promise in improving model resiliency and adaptability across many AI domains, and thus we hope to modify our given Transformer QA model to improve performance on out-of-domain QA tasks and data. Specifically, we hope to use the Reptile meta-learning algorithm applied to multiple prelearning tasks \u2014 which we interpret to be topics from within a single dataset \u2014 to create a metalearner on which we test out-of-domain QA, in order to hopefully show that this model would be more robust than baseline (higher EM and FI scores).", "human_reference": "Title: Meta Learning on Topics as Tasks for Robust QA Performance\nAbstract: A key pain point of current neural QA-focused NLP systems is the lack of generalization \u2014 often these systems learn parameters that fail to generalize to neverbefore-seen data domains, unlike how humans can take previous knowledge and build accurate inferences beyond \"training\" data distributions. Clearly, advances in meta-learning have shown promise in improving model resiliency and adaptability across many AI domains, and thus we hope to modify our given Transformer QA model to improve performance on out-of-domain QA tasks and data. Specifically, we hope to use the Reptile meta-learning algorithm applied to multiple prelearning tasks \u2014 which we interpret to be topics from within a single dataset \u2014 to create a metalearner on which we test out-of-domain QA, in order to hopefully show that this model would be more robust than baseline (higher EM and FI scores).", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0072", "source": "CS224N"}}
{"ai_text": "Title: BiDAF with Self-Attention for SQUAD 2.0\nAbstract: The primary goal of this work is to build a QA system that improves upon a baseline modified BiDAF model's performance on the SQuAD 2.0 dataset. To achieve this improvement, two approaches are explored. In the first one, the modified BiDAF model's embedding layer is extended with character-level embeddings. In the second approach, a self-attention layer is added on top of the existing BiDAF attention layer. The performance of these two approaches is evaluated separately and also when combined together into a single model. The model with character embeddings yielded the best performance on the test set, achieving an EM score of 56.872 and a F1 score of 60.652. The self-attention model performed below expectations overall, though it was the best model when it came to performance on unanswerable questions.", "human_reference": "Title: BiDAF with Self-Attention for SQUAD 2.0\nAbstract: The primary goal of this work is to build a QA system that improves upon a baseline modified BiDAF model's performance on the SQuAD 2.0 dataset. To achieve this improvement, two approaches are explored. In the first one, the modified BiDAF model's embedding layer is extended with character-level embeddings. In the second approach, a self-attention layer is added on top of the existing BiDAF attention layer. The performance of these two approaches is evaluated separately and also when combined together into a single model. The model with character embeddings yielded the best performance on the test set, achieving an EM score of 56.872 and a F1 score of 60.652. The self-attention model performed below expectations overall, though it was the best model when it came to performance on unanswerable questions.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0116", "source": "CS224N"}}
{"ai_text": "Title: Coattention, Dynamic Pointing Decoders & QANet for Question Answering\nAbstract: The task of question answering (QA) requires language comprehension and modeling the complex interaction between the context and the query. Recurrent models achieved good results using RNNs to process sequential inputs and attention components to cope with long term interactions. However, recurrent QA models have two main weaknesses. First, due to the single-pass nature of the decoder step, models have issues recovering from incorrect local maxima. Second, due to the sequential nature of RNNs these models are often too slow for both training and inference. To address the first problems, we implemented a model based on Dynamic Coattention Network (DCN) that incorporates a dynamic decoder that iteratively predicts the answer span. To improve the model efficiency, we also implemented a transformer based recurrency-free model (QANet), which consists of a stack of encoder blocks including self-attention and convolutional layers. On the Stanford Question Answering Dataset (SQuAD 2.0), our best QANet based model achieves 68.76 F1 score and 65.081 Exact Match(EM) on dev set and 66.00 F1 and 62.67 EM on the test set. A high level model comparison of DCN and QANet is illustrated in the image.", "human_reference": "Title: Coattention, Dynamic Pointing Decoders & QANet for Question Answering\nAbstract: The task of question answering (QA) requires language comprehension and modeling the complex interaction between the context and the query. Recurrent models achieved good results using RNNs to process sequential inputs and attention components to cope with long term interactions. However, recurrent QA models have two main weaknesses. First, due to the single-pass nature of the decoder step, models have issues recovering from incorrect local maxima. Second, due to the sequential nature of RNNs these models are often too slow for both training and inference. To address the first problems, we implemented a model based on Dynamic Coattention Network (DCN) that incorporates a dynamic decoder that iteratively predicts the answer span. To improve the model efficiency, we also implemented a transformer based recurrency-free model (QANet), which consists of a stack of encoder blocks including self-attention and convolutional layers. On the Stanford Question Answering Dataset (SQuAD 2.0), our best QANet based model achieves 68.76 F1 score and 65.081 Exact Match(EM) on dev set and 66.00 F1 and 62.67 EM on the test set. A high level model comparison of DCN and QANet is illustrated in the image.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0092", "source": "CS224N"}}
{"ai_text": "Title: DAM-Net: Robust QA System with Data Augmentation and Multitask Learning\nAbstract: If the machine can comprehend a passage and answer questions based on the context, how to upgrade a QA system to generalize to unseen domains outside the training data? In this project, we propose DAM-Net, a robust QA model that can achieve strong performance even on test examples drawn beyond their training distributions. Specifically, we perform data augmentation on our training data, expand training with the auxiliary task (i.e. fill-in-the-blank), and utilize multi-domain training with additional fine-tuning. DAM-Net has shown strong performance on the robust QA benchmark and sometimes it even outperforms humans in terms of the comprehensiveness and accuracy of the answers!", "human_reference": "Title: DAM-Net: Robust QA System with Data Augmentation and Multitask Learning\nAbstract: If the machine can comprehend a passage and answer questions based on the context, how to upgrade a QA system to generalize to unseen domains outside the training data? In this project, we propose DAM-Net, a robust QA model that can achieve strong performance even on test examples drawn beyond their training distributions. Specifically, we perform data augmentation on our training data, expand training with the auxiliary task (i.e. fill-in-the-blank), and utilize multi-domain training with additional fine-tuning. DAM-Net has shown strong performance on the robust QA benchmark and sometimes it even outperforms humans in terms of the comprehensiveness and accuracy of the answers!", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0020", "source": "CS224N"}}
{"ai_text": "Title: Extended BiDAF with Character-Level Embedding\nAbstract: With the rise of NLP and ML, we've seen much progress in regards to the task of machine comprehension and building robust question answering systems. we want to focus on investigating and improving the BiDAF model, starting from extending the baseline model by including character-level word embeddings. We then ran experiments using the improvements recommended in section 5.11 of the default project handout. Two major goals were accomplished: we implemented character-level embeddings and adjusted dropout rate and learning rate in addition to other hyper-parameters in order to improve our model. On our best model, we were able to achieve an F1 score of 65.106 and a EM score of 61.369 in the non-PCE division.", "human_reference": "Title: Extended BiDAF with Character-Level Embedding\nAbstract: With the rise of NLP and ML, we've seen much progress in regards to the task of machine comprehension and building robust question answering systems. we want to focus on investigating and improving the BiDAF model, starting from extending the baseline model by including character-level word embeddings. We then ran experiments using the improvements recommended in section 5.11 of the default project handout. Two major goals were accomplished: we implemented character-level embeddings and adjusted dropout rate and learning rate in addition to other hyper-parameters in order to improve our model. On our best model, we were able to achieve an F1 score of 65.106 and a EM score of 61.369 in the non-PCE division.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0060", "source": "CS224N"}}
{"ai_text": "That would be the moment when I got the result of my college entrance test. We were under huge pressure that day. Finally the phone finally rang and it turned out that I actually did a good job. when my mind was still all blank, my dad said something that brought me back to reality:\u201d Son, I'm so proud of you.\u201d Suddenly I realized that all my hard work had paid off, I didn't let myself and anyone who loves me down, that's the moment I knew it is a brand new start of my life, and that I'll always cherish.", "human_reference": "That would be the moment when I got the result of my college entrance test. We were under huge pressure that day. Finally the phone finally rang and it turned out that I actually did a good job. when my mind was still all blank, my dad said something that brought me back to reality:\u201d Son, I'm so proud of you.\u201d Suddenly I realized that all my hard work had paid off, I didn't let myself and anyone who loves me down, that's the moment I knew it is a brand new start of my life, and that I'll always cherish.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0009", "source": "TOEFL11"}}
{"ai_text": "Title: QA System with QANet\nAbstract: Question answering system has always been an active field in the Natural Language Processing (NLP) researches. In the past few years, the most successful models are primarily based on Recurrent Neural Networks (RNNs) with attention. Though a lot of progress has been made, due to its sequential nature, RNN's operations are unparallelizable, which makes both training and inference slow. In addition, with linear interaction distance, RNNs have difficulty in learning long dependencies. This is a severe problem in QA system, since the context are usually long paragraphs.\n\nBased on these problems, in this project, we implemented a QA model based on Transformer, hoping to achieve both accurate and fast reading comprehension. We focused on reading comprehension among all QA problems, which is to select a part of text from the given context to answer some certain question. Instead of LSTM, this model used convolution layers and self-attention to form encoders. Given a paragraph of context and a question, it will output the probability of each context word being the start or end of the answer. However, against our expectation, this model did not perform very well. The speed is low due to its large amount of parameters, and the accuracy cannot match that of BiDAF because of overfitting.", "human_reference": "Title: QA System with QANet\nAbstract: Question answering system has always been an active field in the Natural Language Processing (NLP) researches. In the past few years, the most successful models are primarily based on Recurrent Neural Networks (RNNs) with attention. Though a lot of progress has been made, due to its sequential nature, RNN's operations are unparallelizable, which makes both training and inference slow. In addition, with linear interaction distance, RNNs have difficulty in learning long dependencies. This is a severe problem in QA system, since the context are usually long paragraphs.\n\nBased on these problems, in this project, we implemented a QA model based on Transformer, hoping to achieve both accurate and fast reading comprehension. We focused on reading comprehension among all QA problems, which is to select a part of text from the given context to answer some certain question. Instead of LSTM, this model used convolution layers and self-attention to form encoders. Given a paragraph of context and a question, it will output the probability of each context word being the start or end of the answer. However, against our expectation, this model did not perform very well. The speed is low due to its large amount of parameters, and the accuracy cannot match that of BiDAF because of overfitting.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0088", "source": "CS224N"}}
{"ai_text": "The thing I often take up in my leisure time is surfing the internet. The information on the internet can enable me to understand the world better. And it helps me to become a more successful person. When I was selecting which university and which major to take after I graduated from the high school, the internet gave me a lot of useful information about the future of some of my prospective professions. I even talked with several people in those particular professions and got their opinions about it. And I think it is really helpful.", "human_reference": "The thing I often take up in my leisure time is surfing the internet. The information on the internet can enable me to understand the world better. And it helps me to become a more successful person. When I was selecting which university and which major to take after I graduated from the high school, the internet gave me a lot of useful information about the future of some of my prospective professions. I even talked with several people in those particular professions and got their opinions about it. And I think it is really helpful.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0017", "source": "TOEFL11"}}
{"ai_text": "In my point of view, government should provide fund to build museums and theaters because they serve as exhibition centers for people to know about the history and culture of the country. The range of museums is fantastic\u2014there are museums of ancient history and archaeology, of natural history and even museums for such things as transportation and crime! And because the museums often hold new exhibitions, there is always something different to see. Theaters offer people a big place to enjoy a variety of operas and plays of different regions. Meanwhile, the building of the museums and theaters will enhance the cultural exchange between countries. They are also one of the contributing factors that promote the national economy. That's why I think it's a good idea for government to help with the building of museums and theaters.", "human_reference": "In my point of view, government should provide fund to build museums and theaters because they serve as exhibition centers for people to know about the history and culture of the country. The range of museums is fantastic\u2014there are museums of ancient history and archaeology, of natural history and even museums for such things as transportation and crime! And because the museums often hold new exhibitions, there is always something different to see. Theaters offer people a big place to enjoy a variety of operas and plays of different regions. Meanwhile, the building of the museums and theaters will enhance the cultural exchange between countries. They are also one of the contributing factors that promote the national economy. That's why I think it's a good idea for government to help with the building of museums and theaters.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0052", "source": "TOEFL11"}}
{"ai_text": "Years ago on my journey to Australia, I lived in a house right on the edge of a forest. At night when the heat starts to go away, I loved to light a lamp, sit in a cane chair in the balcony and read a book. It was so peaceful at that time that the cool breeze is the only thing to remind you that time is still running. I love it when moonlight slanted through be branches down the floor, all birds stopped tweeting and the only sound you can hear is occasional chirping from some crickets. Everything was so perfect.", "human_reference": "Years ago on my journey to Australia, I lived in a house right on the edge of a forest. At night when the heat starts to go away, I loved to light a lamp, sit in a cane chair in the balcony and read a book. It was so peaceful at that time that the cool breeze is the only thing to remind you that time is still running. I love it when moonlight slanted through be branches down the floor, all birds stopped tweeting and the only sound you can hear is occasional chirping from some crickets. Everything was so perfect.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0016", "source": "TOEFL11"}}
{"ai_text": "If I am choosing among computer science, business, and photography classes, I will choose computer science for two main reasons. First of all, I prefer computer science because programming is a skill that is applicable to all industries. For example, even if I want to work in the fashion industry one day, I can still use my programming skills to help my company build a beautiful website, write a useful app for its customers, and so on. I addition, I prefer computer science because computer science classes can teach me how to solve problems. For instance, I can learn how to analyze problems and solve them systematically through logic. This problem-solving ability helps me become a smarter candidate, so I have a higher chance of finding a job. Therefore, I prefer computer science.", "human_reference": "If I am choosing among computer science, business, and photography classes, I will choose computer science for two main reasons. First of all, I prefer computer science because programming is a skill that is applicable to all industries. For example, even if I want to work in the fashion industry one day, I can still use my programming skills to help my company build a beautiful website, write a useful app for its customers, and so on. I addition, I prefer computer science because computer science classes can teach me how to solve problems. For instance, I can learn how to analyze problems and solve them systematically through logic. This problem-solving ability helps me become a smarter candidate, so I have a higher chance of finding a job. Therefore, I prefer computer science.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0088", "source": "TOEFL11"}}
{"ai_text": "I began measuring my life in flipped pages, packed boxes, and school maps when I was 6. As my family and I flitted between states and coasts for my father\u2019s job over the last decade, I shielded myself with fantasy novels. With my head propped on the baseboard near my nightlight and a book held up in front of me by aching arms, I would dance in whimsical forests, fight daring battles, and rule dangerous courts long after dark. In my fantastic universe, I could take turns being the queen, the knight, the hero, and even the villain. These books helped me express the happiness, anger, sadness, and queerness I could not have even begun to imagine alone. The characters I discovered in novels as I toured libraries and Barnes & Noble stores in strip malls around the country taught me resilience and empowered me to nourish my strengths. Mare Barrow showed me the power of determined women, and I unapologetically strove for academic excellence and obtained a GPA of 4.4. Tane, from The Priory of the Orange Tree, inspired me to push the limits of my own body, so I\u2019ve traversed approximately 1,544 miles in cross-country races and practices. Evelyn Hugo\u2019s unapologetic character compelled me to want to embrace and feel free with my queerness rather than shelter it away in a shameful corner. Even further, this year I am adding a third dimension to my love of fantasy by interpreting Mrs. White in my school\u2019s production of Shuddersome and The Monkey\u2019s Paw with assistance from Anne of Green Gables, my first fictional idol, who massively influenced my personality and tendency for dramatics. But above all, Leigh Bardugu, my favorite author, gave me permission to even dare to write and to dream that I can. What began as a safety net in my adolescence has grown to something more, a true passion for English and all that it can express. Language is power and I wish to wield it like a mighty sword. I want to be the puppetmaster, the speaker, and the leader in a world that is crafted in ink. I want to be a New York Times bestseller and to know that whatever I do is impactful and that it creates a difference, no matter how small. I want to walk down a crowded street and see \u201cmy book\u201d spread open in a passing person\u2019s hands, as they refuse to put it down, just like I did so many times in the hallways of my middle school. A writer, a college professor, a publishing lawyer: I want it all, the riots of failure, and the pride of success. Without the assistance of literature, I wouldn\u2019t be who I am today. If I hadn\u2019t grown up fueled on library hauls I wouldn\u2019t have discovered that I love English. I wouldn\u2019t get shivers when I fret for a favorite character or celebrate their triumphs, be as ready to face obstacles, or be as adventurous as I am. Without the moves around the country and back, I wouldn\u2019t have become so resilient and open to change, so adaptable to life, but most importantly I wouldn\u2019t have become so in love with language. With every move I burrowed in books, and with every book I became me. Literature has made me in every way, and the only way I can repay it is to become the penman.", "human_reference": "I began measuring my life in flipped pages, packed boxes, and school maps when I was 6. As my family and I flitted between states and coasts for my father\u2019s job over the last decade, I shielded myself with fantasy novels. With my head propped on the baseboard near my nightlight and a book held up in front of me by aching arms, I would dance in whimsical forests, fight daring battles, and rule dangerous courts long after dark. In my fantastic universe, I could take turns being the queen, the knight, the hero, and even the villain. These books helped me express the happiness, anger, sadness, and queerness I could not have even begun to imagine alone. The characters I discovered in novels as I toured libraries and Barnes & Noble stores in strip malls around the country taught me resilience and empowered me to nourish my strengths. Mare Barrow showed me the power of determined women, and I unapologetically strove for academic excellence and obtained a GPA of 4.4. Tane, from The Priory of the Orange Tree, inspired me to push the limits of my own body, so I\u2019ve traversed approximately 1,544 miles in cross-country races and practices. Evelyn Hugo\u2019s unapologetic character compelled me to want to embrace and feel free with my queerness rather than shelter it away in a shameful corner. Even further, this year I am adding a third dimension to my love of fantasy by interpreting Mrs. White in my school\u2019s production of Shuddersome and The Monkey\u2019s Paw with assistance from Anne of Green Gables, my first fictional idol, who massively influenced my personality and tendency for dramatics. But above all, Leigh Bardugu, my favorite author, gave me permission to even dare to write and to dream that I can. What began as a safety net in my adolescence has grown to something more, a true passion for English and all that it can express. Language is power and I wish to wield it like a mighty sword. I want to be the puppetmaster, the speaker, and the leader in a world that is crafted in ink. I want to be a New York Times bestseller and to know that whatever I do is impactful and that it creates a difference, no matter how small. I want to walk down a crowded street and see \u201cmy book\u201d spread open in a passing person\u2019s hands, as they refuse to put it down, just like I did so many times in the hallways of my middle school. A writer, a college professor, a publishing lawyer: I want it all, the riots of failure, and the pride of success. Without the assistance of literature, I wouldn\u2019t be who I am today. If I hadn\u2019t grown up fueled on library hauls I wouldn\u2019t have discovered that I love English. I wouldn\u2019t get shivers when I fret for a favorite character or celebrate their triumphs, be as ready to face obstacles, or be as adventurous as I am. Without the moves around the country and back, I wouldn\u2019t have become so resilient and open to change, so adaptable to life, but most importantly I wouldn\u2019t have become so in love with language. With every move I burrowed in books, and with every book I became me. Literature has made me in every way, and the only way I can repay it is to become the penman.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0010", "source": "CollegeEssay"}}
{"ai_text": "They covered the precious mahogany coffin with a brown amalgam of rocks, decomposed organisms, and weeds. It was my turn to take the shovel, but I felt too ashamed to dutifully send her off when I had not properly said goodbye. I refused to throw dirt on her. I refused to let go of my grandmother, to accept a death I had not seen coming, to believe that an illness could not only interrupt, but steal a beloved life. When my parents finally revealed to me that my grandmother had been battling liver cancer, I was twelve and I was angry--mostly with myself. They had wanted to protect me--only six years old at the time--from the complex and morose concept of death. However, when the end inevitably arrived, I wasn\u2019t trying to comprehend what dying was; I was trying to understand how I had been able to abandon my sick grandmother in favor of playing with friends and watching TV. Hurt that my parents had deceived me and resentful of my own oblivion, I committed myself to preventing such blindness from resurfacing. I became desperately devoted to my education because I saw knowledge as the key to freeing myself from the chains of ignorance. While learning about cancer in school I promised myself that I would memorize every fact and absorb every detail in textbooks and online medical journals. And as I began to consider my future, I realized that what I learned in school would allow me to silence that which had silenced my grandmother. However, I was focused not with learning itself, but with good grades and high test scores. I started to believe that academic perfection would be the only way to redeem myself in her eyes--to make up for what I had not done as a granddaughter. However, a simple walk on a hiking trail behind my house made me open my own eyes to the truth. Over the years, everything--even honoring my grandmother--had become second to school and grades. As my shoes humbly tapped against the Earth, the towering trees blackened by the forest fire a few years ago, the faintly colorful pebbles embedded in the sidewalk, and the wispy white clouds hanging in the sky reminded me of my small though nonetheless significant part in a larger whole that is humankind and this Earth. Before I could resolve my guilt, I had to broaden my perspective of the world as well as my responsibilities to my fellow humans. Volunteering at a cancer treatment center has helped me discover my path. When I see patients trapped in not only the hospital but also a moment in time by their diseases, I talk to them. For six hours a day, three times a week, Ivana is surrounded by IV stands, empty walls, and busy nurses that quietly yet constantly remind her of her breast cancer. Her face is pale and tired, yet kind--not unlike my grandmother\u2019s. I need only to smile and say hello to see her brighten up as life returns to her face. Upon our first meeting, she opened up about her two sons, her hometown, and her knitting group--no mention of her disease. Without even standing up, the three of us\u2014Ivana, me, and my grandmother--had taken a walk together. Cancer, as powerful and invincible as it may seem, is a mere fraction of a person\u2019s life. It\u2019s easy to forget when one\u2019s mind and body are so weak and vulnerable. I want to be there as an oncologist to remind them to take a walk once in a while, to remember that there\u2019s so much more to life than a disease. While I physically treat their cancer, I want to lend patients emotional support and mental strength to escape the interruption and continue living. Through my work, I can accept the shovel without burying my grandmother\u2019s memory.", "human_reference": "They covered the precious mahogany coffin with a brown amalgam of rocks, decomposed organisms, and weeds. It was my turn to take the shovel, but I felt too ashamed to dutifully send her off when I had not properly said goodbye. I refused to throw dirt on her. I refused to let go of my grandmother, to accept a death I had not seen coming, to believe that an illness could not only interrupt, but steal a beloved life. When my parents finally revealed to me that my grandmother had been battling liver cancer, I was twelve and I was angry--mostly with myself. They had wanted to protect me--only six years old at the time--from the complex and morose concept of death. However, when the end inevitably arrived, I wasn\u2019t trying to comprehend what dying was; I was trying to understand how I had been able to abandon my sick grandmother in favor of playing with friends and watching TV. Hurt that my parents had deceived me and resentful of my own oblivion, I committed myself to preventing such blindness from resurfacing. I became desperately devoted to my education because I saw knowledge as the key to freeing myself from the chains of ignorance. While learning about cancer in school I promised myself that I would memorize every fact and absorb every detail in textbooks and online medical journals. And as I began to consider my future, I realized that what I learned in school would allow me to silence that which had silenced my grandmother. However, I was focused not with learning itself, but with good grades and high test scores. I started to believe that academic perfection would be the only way to redeem myself in her eyes--to make up for what I had not done as a granddaughter. However, a simple walk on a hiking trail behind my house made me open my own eyes to the truth. Over the years, everything--even honoring my grandmother--had become second to school and grades. As my shoes humbly tapped against the Earth, the towering trees blackened by the forest fire a few years ago, the faintly colorful pebbles embedded in the sidewalk, and the wispy white clouds hanging in the sky reminded me of my small though nonetheless significant part in a larger whole that is humankind and this Earth. Before I could resolve my guilt, I had to broaden my perspective of the world as well as my responsibilities to my fellow humans. Volunteering at a cancer treatment center has helped me discover my path. When I see patients trapped in not only the hospital but also a moment in time by their diseases, I talk to them. For six hours a day, three times a week, Ivana is surrounded by IV stands, empty walls, and busy nurses that quietly yet constantly remind her of her breast cancer. Her face is pale and tired, yet kind--not unlike my grandmother\u2019s. I need only to smile and say hello to see her brighten up as life returns to her face. Upon our first meeting, she opened up about her two sons, her hometown, and her knitting group--no mention of her disease. Without even standing up, the three of us\u2014Ivana, me, and my grandmother--had taken a walk together. Cancer, as powerful and invincible as it may seem, is a mere fraction of a person\u2019s life. It\u2019s easy to forget when one\u2019s mind and body are so weak and vulnerable. I want to be there as an oncologist to remind them to take a walk once in a while, to remember that there\u2019s so much more to life than a disease. While I physically treat their cancer, I want to lend patients emotional support and mental strength to escape the interruption and continue living. Through my work, I can accept the shovel without burying my grandmother\u2019s memory.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0050", "source": "CollegeEssay"}}
{"ai_text": "Title: Extending QANet with Transformer-XL\nAbstract: This project tackles the machine reading comprehension (RC) problemon the SQuAD 2.0 dataset.  It involves inputting a context paragraph and aquestion into a model and outputting the span of the answer from the contextparagraph.  This project aims to extend the QANet, so that it can effectivelyperform RC on SQuAD 2.0. The segment-level recurrence with state reuse fromTransformer-XL is integrated into QANet to improve its ability of tacklinglong context paragraph (referred to as QANet-XL). In addition,  character embeddings and a fusion layer after context-query attention are used to extend BiDAF. Experiments show that QANet-XL underperforms the vanillaQANet and outperforms the extended BiDAF. The segment-level recurrence mech-anism from Transformer-XL is proven not a proper improvement for QANet on theSQuAD 2.0 dataset, since segmenting context paragraph is somewhat harmful. For the dev set, The extended BiDAF achieved EM/F1 = 62.16/65.98, the vanilla QANet achieved EM/F1=66.81/70.38, and the QANet-XL achieved EM/F1 = 63.12/66.67. A majority voting ensemble model based on previous mentioned models achieved EM/F1=66.85/69.97 on the test set.", "human_reference": "Title: Extending QANet with Transformer-XL\nAbstract: This project tackles the machine reading comprehension (RC) problemon the SQuAD 2.0 dataset.  It involves inputting a context paragraph and aquestion into a model and outputting the span of the answer from the contextparagraph.  This project aims to extend the QANet, so that it can effectivelyperform RC on SQuAD 2.0. The segment-level recurrence with state reuse fromTransformer-XL is integrated into QANet to improve its ability of tacklinglong context paragraph (referred to as QANet-XL). In addition,  character embeddings and a fusion layer after context-query attention are used to extend BiDAF. Experiments show that QANet-XL underperforms the vanillaQANet and outperforms the extended BiDAF. The segment-level recurrence mech-anism from Transformer-XL is proven not a proper improvement for QANet on theSQuAD 2.0 dataset, since segmenting context paragraph is somewhat harmful. For the dev set, The extended BiDAF achieved EM/F1 = 62.16/65.98, the vanilla QANet achieved EM/F1=66.81/70.38, and the QANet-XL achieved EM/F1 = 63.12/66.67. A majority voting ensemble model based on previous mentioned models achieved EM/F1=66.85/69.97 on the test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0014", "source": "CS224N"}}
{"ai_text": "I definitely believe that TV programs are bringing negative influences on our society, here are some of the reasons. First, families used to get together after dinner and talk about their day, to share their joy and sorrow, to play games and have fun. But now, the only thing they do now is sitting in the couch, watching TV for entertainment. And second, working out is a much better choice than watching TV. Obesity and heart attack are now very serious problems in this country, a major cause of it is that people like to sit in front of TV all day and eat junk food.", "human_reference": "I definitely believe that TV programs are bringing negative influences on our society, here are some of the reasons. First, families used to get together after dinner and talk about their day, to share their joy and sorrow, to play games and have fun. But now, the only thing they do now is sitting in the couch, watching TV for entertainment. And second, working out is a much better choice than watching TV. Obesity and heart attack are now very serious problems in this country, a major cause of it is that people like to sit in front of TV all day and eat junk food.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0005", "source": "TOEFL11"}}
{"ai_text": "When choosing a restaurant, the thing I consider the most important is its hygiene condition. First of all the restaurant should be clean, so that you can have your dinner in a good mood. And more importantly the food have to be safe because you don't want to get sick after dinner. And of course, the taste of food is very important too. Now there are excellent website where you can look up the restaurants and see other people's comment on them, they may even recommend some good food that you should try. Those are two features I care about most.", "human_reference": "When choosing a restaurant, the thing I consider the most important is its hygiene condition. First of all the restaurant should be clean, so that you can have your dinner in a good mood. And more importantly the food have to be safe because you don't want to get sick after dinner. And of course, the taste of food is very important too. Now there are excellent website where you can look up the restaurants and see other people's comment on them, they may even recommend some good food that you should try. Those are two features I care about most.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0038", "source": "TOEFL11"}}
{"ai_text": "Title: Gated Self-Attention for SQuAD Question Answering\nAbstract: Machine comprehension and question answering are central questions in natural\nlanguage processing, as they require modeling interactions between the passage\nand the question. In this paper, we build on the multi-stage hierarchical process\nBiDAF described in Seo et al. (2017)'s Bi-Directional Attention Flow for Machine Comprehension. We utilize tools from the R-Net model described in R-Net:\nMachine Reading Comprehension with Self-Matching Networks, testing different\ncombinations of model components. We experiment with different types of encoding, such as using a Gated Recurrent Unit (GRU) or a Convolutional Neural\nNetwork (CNN), and attention mechanisms, such as comparing context-query\nattention layers and contemplating the usage of gates. We ultimately introduce a\nmodified form of BiDAF which utilizes both an LSTM and a CNN in its encoding\nlayer, as well as BiDAF's context-query attention layer followed by R-Net's self-attention layer. We conduct various experiments on the SQuAD datasets, yielding\ncompetitive results on the CS224N SQuAD Leaderboard.", "human_reference": "Title: Gated Self-Attention for SQuAD Question Answering\nAbstract: Machine comprehension and question answering are central questions in natural\nlanguage processing, as they require modeling interactions between the passage\nand the question. In this paper, we build on the multi-stage hierarchical process\nBiDAF described in Seo et al. (2017)'s Bi-Directional Attention Flow for Machine Comprehension. We utilize tools from the R-Net model described in R-Net:\nMachine Reading Comprehension with Self-Matching Networks, testing different\ncombinations of model components. We experiment with different types of encoding, such as using a Gated Recurrent Unit (GRU) or a Convolutional Neural\nNetwork (CNN), and attention mechanisms, such as comparing context-query\nattention layers and contemplating the usage of gates. We ultimately introduce a\nmodified form of BiDAF which utilizes both an LSTM and a CNN in its encoding\nlayer, as well as BiDAF's context-query attention layer followed by R-Net's self-attention layer. We conduct various experiments on the SQuAD datasets, yielding\ncompetitive results on the CS224N SQuAD Leaderboard.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0131", "source": "CS224N"}}
{"ai_text": "An enjoyable event that took place in my childhood was when I got my first dog--- Little. I was ten and had been pestering my parents for a dog for years. One day I came home from school, my parents said they had a surprise for me and it was waiting for me in my room. I ran up to my room and threw open the door. I found a tiny puppy with a red bow around his neck and sleeping at the foot of my bed. We became good friends forever. And for the rest of his life, he slept on the exact same spot at the end of my bed every night. That was a great happy event in my childhood.", "human_reference": "An enjoyable event that took place in my childhood was when I got my first dog--- Little. I was ten and had been pestering my parents for a dog for years. One day I came home from school, my parents said they had a surprise for me and it was waiting for me in my room. I ran up to my room and threw open the door. I found a tiny puppy with a red bow around his neck and sleeping at the foot of my bed. We became good friends forever. And for the rest of his life, he slept on the exact same spot at the end of my bed every night. That was a great happy event in my childhood.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0029", "source": "TOEFL11"}}
{"ai_text": "Title: Stanford CS224N SQuAD IID Default Project\nAbstract: Being able to answer questions about a given passage marks a significant advancement in artificial intelligence. This task also has incredible practical utility, given the great need to have a personal assistant on our phones that can answer simple questions about world facts. In this project, we attempt to build a state-of-the-art model for question answering on the SQuAD 2.0 dataset via combining several different deep learning techniques. We iterated off of the baseline BiDAF model with various improvements such as feature engineering, character embeddings, co-attention, transformer models, and more. We had mixed success in getting all of these methodologies to fully run as anticipated and found many to not work as well as we had hoped. But we still managed to make significant improvements over the baseline by combining some of what we had implemented and performing a hyperparameter search. Our final model was quite successful on this front, achieving an F1 score of 63.517 and an EM score of 59.966 over the baseline's 58 F1 score and 55 EM score.", "human_reference": "Title: Stanford CS224N SQuAD IID Default Project\nAbstract: Being able to answer questions about a given passage marks a significant advancement in artificial intelligence. This task also has incredible practical utility, given the great need to have a personal assistant on our phones that can answer simple questions about world facts. In this project, we attempt to build a state-of-the-art model for question answering on the SQuAD 2.0 dataset via combining several different deep learning techniques. We iterated off of the baseline BiDAF model with various improvements such as feature engineering, character embeddings, co-attention, transformer models, and more. We had mixed success in getting all of these methodologies to fully run as anticipated and found many to not work as well as we had hoped. But we still managed to make significant improvements over the baseline by combining some of what we had implemented and performing a hyperparameter search. Our final model was quite successful on this front, achieving an F1 score of 63.517 and an EM score of 59.966 over the baseline's 58 F1 score and 55 EM score.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0064", "source": "CS224N"}}
{"ai_text": "Title: Building a Robust QA system with Data Augmentation\nAbstract: Pre-trained neural models such as our baseline model fine-tuned on a BERT based pre-trained transformer to perform nature language question and answering prob- lems usually show high levels of accuracy with in-context data, but often display a lack of robustness with out-of-context data. We hypothesize that this issue is not primarily caused by the pre-trained model's limitations, but rather by the lack of diverse training data that might convey important contextual information in the fine-tuning stage. We explore several methods to augment standard training data with syntactically informative data, generated by randomly replacing the grammatical tense of data, removing words associated with gender, race, or economic means, and only replacing question sentences with synonym words from a lexicon of words. We found that the augmentation method that performed the best was changing the grammar of more and one word in every question. Although it only made less than 1 point increase in the F1 and EM scores, we believe that if we also applied this method to the context and answers training data we would be able to see even more significant improvements. We were also surprised that the method of removing associations with gender, race, or economic status performed relatively well given that we removed a lot of words from the dataset.", "human_reference": "Title: Building a Robust QA system with Data Augmentation\nAbstract: Pre-trained neural models such as our baseline model fine-tuned on a BERT based pre-trained transformer to perform nature language question and answering prob- lems usually show high levels of accuracy with in-context data, but often display a lack of robustness with out-of-context data. We hypothesize that this issue is not primarily caused by the pre-trained model's limitations, but rather by the lack of diverse training data that might convey important contextual information in the fine-tuning stage. We explore several methods to augment standard training data with syntactically informative data, generated by randomly replacing the grammatical tense of data, removing words associated with gender, race, or economic means, and only replacing question sentences with synonym words from a lexicon of words. We found that the augmentation method that performed the best was changing the grammar of more and one word in every question. Although it only made less than 1 point increase in the F1 and EM scores, we believe that if we also applied this method to the context and answers training data we would be able to see even more significant improvements. We were also surprised that the method of removing associations with gender, race, or economic status performed relatively well given that we removed a lot of words from the dataset.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0062", "source": "CS224N"}}
{"ai_text": "out my tough transition. But instead of an answer, Ms. McVaugh offered me to join a girls\u2019 field hockey practice. I felt thrown off by the unusual opportunity at first, yet I quickly relished a warm rush of excitement surging through my veins as I imagined putting on field hockey cleats again. When I set foot on the turf the following day, however, my initial anxiety rejoined my exuberance. I felt more eyes turning towards me with each step I made. \u201cBoys do not play field hockey,\u201d I could hear the girls think. As I trailed behind the girls during the warm-up, the thought of quitting seemed more tempting with each second of silence that passed. But when the whistle blew and the ball was finally in play, I was surprised to see how quickly the gender barrier vanished. Where there was silence and separation at first, I could now see the shared fanaticism through our red faces and hear the emotion in our clamor. At the end of practice, I felt a burning glow of joy overtake my body as I caught my breath on the bench. In that moment, I gradually realized how I should not let obstacles, like gender boundaries in field hockey, hold me back from exploring new opportunities. Realizing the joy I had found in trying the unconventional, I took this experience to the soccer field to take on its new athletic challenges once again. Rather than agonizing over playing time or titles, I simply redirected my focus on the joy and beauty of the sport. Within days, I noticed the same atmosphere of sweat and screams from the turf take hold of the soccer field. Over time, this helped me take in feedback more readily, ask questions about tactics, and try out new skills. With each new improvement I made through this, I slowly began to grasp the value of my new approach to the sport. As a result, I decided to bring the same open, curious, and risk-taking mindset with me to the other opportunities that boarding school holds. In the classroom, I began asking deeper questions to fully comprehend new material. Back in the dorm, I turned the cultural differences between my peers into opportunities to learn from and contribute back to. From truly grasping nucleophile-electrophile reactions in organic chemistry to sharing Dutch \u2018stroopwafels\u2019 with my hall, such moments remind me of why I sacrificed my field hockey gear to go to Deerfield; even as my new mindset gradually led to the grades, friendships, and even athletic achievements I sought before, I realized that I value the exploration, growth and joy behind such successes far more. Now, before I put on my cleats, walk into the classroom or enter my dorm, I do not worry about the successes I might fail to reach or the obstacles that might hold me back. Rather, I pour my heart into such opportunities and take their experiences with me.", "human_reference": "out my tough transition. But instead of an answer, Ms. McVaugh offered me to join a girls\u2019 field hockey practice. I felt thrown off by the unusual opportunity at first, yet I quickly relished a warm rush of excitement surging through my veins as I imagined putting on field hockey cleats again. When I set foot on the turf the following day, however, my initial anxiety rejoined my exuberance. I felt more eyes turning towards me with each step I made. \u201cBoys do not play field hockey,\u201d I could hear the girls think. As I trailed behind the girls during the warm-up, the thought of quitting seemed more tempting with each second of silence that passed. But when the whistle blew and the ball was finally in play, I was surprised to see how quickly the gender barrier vanished. Where there was silence and separation at first, I could now see the shared fanaticism through our red faces and hear the emotion in our clamor. At the end of practice, I felt a burning glow of joy overtake my body as I caught my breath on the bench. In that moment, I gradually realized how I should not let obstacles, like gender boundaries in field hockey, hold me back from exploring new opportunities. Realizing the joy I had found in trying the unconventional, I took this experience to the soccer field to take on its new athletic challenges once again. Rather than agonizing over playing time or titles, I simply redirected my focus on the joy and beauty of the sport. Within days, I noticed the same atmosphere of sweat and screams from the turf take hold of the soccer field. Over time, this helped me take in feedback more readily, ask questions about tactics, and try out new skills. With each new improvement I made through this, I slowly began to grasp the value of my new approach to the sport. As a result, I decided to bring the same open, curious, and risk-taking mindset with me to the other opportunities that boarding school holds. In the classroom, I began asking deeper questions to fully comprehend new material. Back in the dorm, I turned the cultural differences between my peers into opportunities to learn from and contribute back to. From truly grasping nucleophile-electrophile reactions in organic chemistry to sharing Dutch \u2018stroopwafels\u2019 with my hall, such moments remind me of why I sacrificed my field hockey gear to go to Deerfield; even as my new mindset gradually led to the grades, friendships, and even athletic achievements I sought before, I realized that I value the exploration, growth and joy behind such successes far more. Now, before I put on my cleats, walk into the classroom or enter my dorm, I do not worry about the successes I might fail to reach or the obstacles that might hold me back. Rather, I pour my heart into such opportunities and take their experiences with me.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0036", "source": "CollegeEssay"}}
{"ai_text": "Title: BiDAF with Character and Subword Embeddings for SQuAD\nAbstract: In this paper, we have implemented subword embeddings and character-level embeddings on top of the word embeddings in the starter code.\nFor the character embeddings, we followed the approaches outlined in the BiDAF paper[1]. The character's representation vectors were randomly initiated and then passed through a convolutional neural network. We then applied the ReLu function, as well as downsampling it using the maxpool function to get the representation vector for every word.\nFor the subword embeddings, we utilized the implementation of the Byte Pair Encoding algorithm[2]. It segments the word by grouping character sequences that occur most frequently in its training data. We then looked up the representation vector for each subword, which is trained using the GloVe algorithm(The segmentation and vector representation are both implemented in the Python library bpemb)[3].  We utilized the maxpool function to get the representation vector of each word, and then used linear transformation to convert the input features to match the hidden layers. Finally, we concatenated the three types of embeddings and passed them through the Highway Networks.\nAmong the different types of models we have experimented with, the model with the concatenation of word embeddings and character-level embeddings performs the best on the SQuAD v2.0 dev set: EM=61.39, F1=65.05.\n\nReferences\n[1]Minjoon  Seo,  Aniruddha  Kembhavi,  Ali  Farhadi,  and  Hannaneh  Hajishirzi.   Bidirectionalattention flow for machine comprehension.arXiv preprint arXiv:1611.01603, 2016.\n[2]Benjamin Heinzerling and Michael Strube.   Bpemb:  Tokenization-free pre-trained subwordembeddings in 275 languages.arXiv preprint arXiv:1710.02187, 2017.\n[2]Jeffrey Pennington, Richard Socher, and Christopher D Manning.  Glove:  Global vectors forword representation.  InProceedings of the 2014 conference on empirical methods in naturallanguage processing (EMNLP), pages 1532-1543, 2014.", "human_reference": "Title: BiDAF with Character and Subword Embeddings for SQuAD\nAbstract: In this paper, we have implemented subword embeddings and character-level embeddings on top of the word embeddings in the starter code.\nFor the character embeddings, we followed the approaches outlined in the BiDAF paper[1]. The character's representation vectors were randomly initiated and then passed through a convolutional neural network. We then applied the ReLu function, as well as downsampling it using the maxpool function to get the representation vector for every word.\nFor the subword embeddings, we utilized the implementation of the Byte Pair Encoding algorithm[2]. It segments the word by grouping character sequences that occur most frequently in its training data. We then looked up the representation vector for each subword, which is trained using the GloVe algorithm(The segmentation and vector representation are both implemented in the Python library bpemb)[3].  We utilized the maxpool function to get the representation vector of each word, and then used linear transformation to convert the input features to match the hidden layers. Finally, we concatenated the three types of embeddings and passed them through the Highway Networks.\nAmong the different types of models we have experimented with, the model with the concatenation of word embeddings and character-level embeddings performs the best on the SQuAD v2.0 dev set: EM=61.39, F1=65.05.\n\nReferences\n[1]Minjoon  Seo,  Aniruddha  Kembhavi,  Ali  Farhadi,  and  Hannaneh  Hajishirzi.   Bidirectionalattention flow for machine comprehension.arXiv preprint arXiv:1611.01603, 2016.\n[2]Benjamin Heinzerling and Michael Strube.   Bpemb:  Tokenization-free pre-trained subwordembeddings in 275 languages.arXiv preprint arXiv:1710.02187, 2017.\n[2]Jeffrey Pennington, Richard Socher, and Christopher D Manning.  Glove:  Global vectors forword representation.  InProceedings of the 2014 conference on empirical methods in naturallanguage processing (EMNLP), pages 1532-1543, 2014.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0070", "source": "CS224N"}}
{"ai_text": "I cannot dance. This is not something I often admit willingly; in fact, it is quite baffling to me how horribly incapable I am at performing even the most basic movements on command. My grandmother often describes it as \u201ca tragedy\u201d as she is forced to watch her grandchild absolutely butcher our country\u2019s cultural dances, beautiful expressions of our unique West African roots turned into poor facsimiles by my robotic movements. And yet, year after year, I find myself taking the dance floor at my family\u2019s events, seemingly unaware of my objective lack of skill. Eventually, my display proves to be so amazingly unbearable that I am removed from the floor and shown the correct movements over and over again until I am able to replicate them well enough to come back. Bizarrely, despite my previous declaration that I cannot dance, for the past three years, I have found myself performing an entire choreographed routine at my school\u2019s yearly pep rallies. It is through looking back at these events that I realize that I have created a mischaracterization of my dancing abilities through my decisive first sentence. I can dance and am, in fact, very capable of doing so, but not when I act insularly. My ability to dance correlates directly with how willing I am to collaborate, the input and support of others turning the uncoordinated and unwieldy into the near-graceful. My attempts at dancing have led me to value community and collaboration greatly, and I find myself seeking and being drawn towards environments that will allow me to continue to develop both of these values as I learn and grow. Through my internship with the Johns Hopkins Applied Physics Lab, I was exposed to and became fascinated by the collaborative spirit that lies at the heart of Johns Hopkins. The idea that one cannot discover or innovate when working alone was affirmed during my research, and I have come to see that mutual collaboration and community are integral aspects of Johns Hopkins\u2019 unique culture. From the research initiatives that breach the boundaries between class levels, to the many organizations such as the Tutorial Project, relying on the shared initiatives of different students to directly make an impact on Baltimore and its many communities, and the distinctive access to especially interdisciplinary topics such as neuromorphic systems, I view that Johns Hopkins exemplifies the peak of collaborative achievement in education.", "human_reference": "I cannot dance. This is not something I often admit willingly; in fact, it is quite baffling to me how horribly incapable I am at performing even the most basic movements on command. My grandmother often describes it as \u201ca tragedy\u201d as she is forced to watch her grandchild absolutely butcher our country\u2019s cultural dances, beautiful expressions of our unique West African roots turned into poor facsimiles by my robotic movements. And yet, year after year, I find myself taking the dance floor at my family\u2019s events, seemingly unaware of my objective lack of skill. Eventually, my display proves to be so amazingly unbearable that I am removed from the floor and shown the correct movements over and over again until I am able to replicate them well enough to come back. Bizarrely, despite my previous declaration that I cannot dance, for the past three years, I have found myself performing an entire choreographed routine at my school\u2019s yearly pep rallies. It is through looking back at these events that I realize that I have created a mischaracterization of my dancing abilities through my decisive first sentence. I can dance and am, in fact, very capable of doing so, but not when I act insularly. My ability to dance correlates directly with how willing I am to collaborate, the input and support of others turning the uncoordinated and unwieldy into the near-graceful. My attempts at dancing have led me to value community and collaboration greatly, and I find myself seeking and being drawn towards environments that will allow me to continue to develop both of these values as I learn and grow. Through my internship with the Johns Hopkins Applied Physics Lab, I was exposed to and became fascinated by the collaborative spirit that lies at the heart of Johns Hopkins. The idea that one cannot discover or innovate when working alone was affirmed during my research, and I have come to see that mutual collaboration and community are integral aspects of Johns Hopkins\u2019 unique culture. From the research initiatives that breach the boundaries between class levels, to the many organizations such as the Tutorial Project, relying on the shared initiatives of different students to directly make an impact on Baltimore and its many communities, and the distinctive access to especially interdisciplinary topics such as neuromorphic systems, I view that Johns Hopkins exemplifies the peak of collaborative achievement in education.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0034", "source": "CollegeEssay"}}
{"ai_text": "Title: An Analysis on the Effect of Domain Representations in Question Answering Models\nAbstract: Studies of robust reading comprehension models have included both learning domain specific representations and domain invariant representations. This project analyzes the effectiveness of each of these approaches using Mixture-of-Experts (MoE) and adversarial models. In the domain specific approach, MoE's form a single expert model for each input domain (Guo et al., 2018, Takahashi et al., 2019). In contrast, domain invariant models learn a generalized hidden representation that cannot distinguish the domain of the input (Ma et al., 2019, Lee et al., 2019). Additionally, models are assessed to determine their level of understanding of natural language against learning simple linguistic bias heuristics.", "human_reference": "Title: An Analysis on the Effect of Domain Representations in Question Answering Models\nAbstract: Studies of robust reading comprehension models have included both learning domain specific representations and domain invariant representations. This project analyzes the effectiveness of each of these approaches using Mixture-of-Experts (MoE) and adversarial models. In the domain specific approach, MoE's form a single expert model for each input domain (Guo et al., 2018, Takahashi et al., 2019). In contrast, domain invariant models learn a generalized hidden representation that cannot distinguish the domain of the input (Ma et al., 2019, Lee et al., 2019). Additionally, models are assessed to determine their level of understanding of natural language against learning simple linguistic bias heuristics.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0015", "source": "CS224N"}}
{"ai_text": "Title: Building a QA system (IID SQuAD track)\nAbstract: In this project, we are dealing with building a Question Answering System that is expected to perform well on SQuAD. Our approaches to this task include the retraining of baseline model, improvement on embedding (BiDAF), modification of attention (Dynamic Coattention Model), replacement of LSTM with GRU and application of transformer (QANet). After experiments with different models and modifications, both BiDAF and QANet outperform the baseline model, with QANet being our best model. It takes some advantages of various features in other modifications mentioned before, and it consists of four layers: (1) Embedding layer where the combination of character-level and word-level embedding uses the pre-trained word embedding model to map the input into vector space. (2) Contextual embedding layer where the encoder block utilized contextual cues from surrounding words to refine the embedding of the words. (3) Attention flow layer where the coattention-like implementation produces a set of query-aware feature vectors for each word in the context. (4) Modeling and output layer where a stack of encoder blocks with fully-connected layers are sued to scan the context and provide an answer to the query. By submitting our best model to the test leaderboard, we have obtained satisfying results with F1 of 66.43 and EM of 62.45.", "human_reference": "Title: Building a QA system (IID SQuAD track)\nAbstract: In this project, we are dealing with building a Question Answering System that is expected to perform well on SQuAD. Our approaches to this task include the retraining of baseline model, improvement on embedding (BiDAF), modification of attention (Dynamic Coattention Model), replacement of LSTM with GRU and application of transformer (QANet). After experiments with different models and modifications, both BiDAF and QANet outperform the baseline model, with QANet being our best model. It takes some advantages of various features in other modifications mentioned before, and it consists of four layers: (1) Embedding layer where the combination of character-level and word-level embedding uses the pre-trained word embedding model to map the input into vector space. (2) Contextual embedding layer where the encoder block utilized contextual cues from surrounding words to refine the embedding of the words. (3) Attention flow layer where the coattention-like implementation produces a set of query-aware feature vectors for each word in the context. (4) Modeling and output layer where a stack of encoder blocks with fully-connected layers are sued to scan the context and provide an answer to the query. By submitting our best model to the test leaderboard, we have obtained satisfying results with F1 of 66.43 and EM of 62.45.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0004", "source": "CS224N"}}
{"ai_text": "There is a notebook that means a lot to me. I've always loved reading and I started writing my own stories years ago. One day a friend of mine found the pile of paper I kept my stories on and asked me to let him read them. I was kind of reluctant but still a little glad that he found them, so I agreed. After few days he handed me my stories along with this notebook, telling me to keep writing and one day I may become a good writer. For that I was really grateful, and that notebook encouraged me to keep on writing ever since.", "human_reference": "There is a notebook that means a lot to me. I've always loved reading and I started writing my own stories years ago. One day a friend of mine found the pile of paper I kept my stories on and asked me to let him read them. I was kind of reluctant but still a little glad that he found them, so I agreed. After few days he handed me my stories along with this notebook, telling me to keep writing and one day I may become a good writer. For that I was really grateful, and that notebook encouraged me to keep on writing ever since.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0028", "source": "TOEFL11"}}
{"ai_text": "Title: Improving Question Answering on SQuAD 2.0: Exploring the QANet Architecture\nAbstract: In this project, we investigated QANet - an end-to-end, non-recurrent model that is based on the use of convolutions and self-attention. Our first goal was to reimplement the QANet model from scratch and compare its performance to that of our baseline BiDAF - a model that relies on recurrent neural networks with attention. Both of the QA answering systems were tested on SQuAD 2.0 which includes both questions that are answerable given a context and questions that are not answerable given the context. Finally, after evaluation of our \"vanilla\" QANet and investigation of related work, we implemented an extended model called EQuANT. The model adds an additional output to explicitly predict the answerability of a question given the context. Our best model (QANet with tuned hyper-parameters) achieves F1 = 57.56 and EM = 54.66 on the developmental set, and F1 = 56.76 and EM = 53.34 on the test set.", "human_reference": "Title: Improving Question Answering on SQuAD 2.0: Exploring the QANet Architecture\nAbstract: In this project, we investigated QANet - an end-to-end, non-recurrent model that is based on the use of convolutions and self-attention. Our first goal was to reimplement the QANet model from scratch and compare its performance to that of our baseline BiDAF - a model that relies on recurrent neural networks with attention. Both of the QA answering systems were tested on SQuAD 2.0 which includes both questions that are answerable given a context and questions that are not answerable given the context. Finally, after evaluation of our \"vanilla\" QANet and investigation of related work, we implemented an extended model called EQuANT. The model adds an additional output to explicitly predict the answerability of a question given the context. Our best model (QANet with tuned hyper-parameters) achieves F1 = 57.56 and EM = 54.66 on the developmental set, and F1 = 56.76 and EM = 53.34 on the test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0047", "source": "CS224N"}}
{"ai_text": "Title: \"Pointed\" Question-Answering\nAbstract: Machine reading comprehension through question-answering is one of the most interesting and significant problems in Natural Language Processing because it not only measures how well the machine 'understands' a piece of text but also helps provide useful answers to humans. For this task, given a paragraph and a related question, the machine's model must select the span from the paragraph that corresponds to the answer using a start index prediction and end index prediction.  My baseline model for this task is a Bidirectional Attention Flow (BiDAF) end-to-end neural network, with embedding, encoder, attention, modeling and output layers. Significantly, the output layer involves the probability distribution of the start index token and end index token to be generated independently. However, in order for the model to learn how the end of an answer can depend on the start of an answer, I implement a boundary model of an Answer Pointer layer (introduced by Wang et al, 2017) based on the notion of a Pointer Network (Vinyals et al, 2015) as a replacement for the output layer of the baseline. This enables us to condition the prediction for the end token on the prediction for the start token of the answer in the input text. Further, since a Pointer Network outputs a probability distribution exclusively over locations in the input paragraph (context) at each step instead of outputting a probability distribution over the entire vocabulary, it allows us to improve the model's efficiency in addition to its accuracy. On testing this new model, I obtain an F1 score of 59.60 and an EM score of 55.01 on the development set, which is an improvement on the performance of the baseline - involving both F1 and EM scores of 52.19 on the development set.", "human_reference": "Title: \"Pointed\" Question-Answering\nAbstract: Machine reading comprehension through question-answering is one of the most interesting and significant problems in Natural Language Processing because it not only measures how well the machine 'understands' a piece of text but also helps provide useful answers to humans. For this task, given a paragraph and a related question, the machine's model must select the span from the paragraph that corresponds to the answer using a start index prediction and end index prediction.  My baseline model for this task is a Bidirectional Attention Flow (BiDAF) end-to-end neural network, with embedding, encoder, attention, modeling and output layers. Significantly, the output layer involves the probability distribution of the start index token and end index token to be generated independently. However, in order for the model to learn how the end of an answer can depend on the start of an answer, I implement a boundary model of an Answer Pointer layer (introduced by Wang et al, 2017) based on the notion of a Pointer Network (Vinyals et al, 2015) as a replacement for the output layer of the baseline. This enables us to condition the prediction for the end token on the prediction for the start token of the answer in the input text. Further, since a Pointer Network outputs a probability distribution exclusively over locations in the input paragraph (context) at each step instead of outputting a probability distribution over the entire vocabulary, it allows us to improve the model's efficiency in addition to its accuracy. On testing this new model, I obtain an F1 score of 59.60 and an EM score of 55.01 on the development set, which is an improvement on the performance of the baseline - involving both F1 and EM scores of 52.19 on the development set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0050", "source": "CS224N"}}
{"ai_text": "Title: More Explorations with Adversarial Training in Building Robust QA System\nAbstract: In real world Question Answering (QA) applications, a model is usually required to generalize to unseen domains. It was found that an Adversarial Training framework where a conventional QA model trained to deceive a domain predicting discriminator can help learn domain-invariant features that generalize better. In this work we explored more discriminator architectures. We showed that by using a single layer Transformer encoder as the discriminator and taking the whole last layer hidden states from the QA model, the system performs better than the originally proposed simple Multilayer Perceptron (MLP) discriminator taking only the hidden state at the [CLS] token of the BERT QA model.", "human_reference": "Title: More Explorations with Adversarial Training in Building Robust QA System\nAbstract: In real world Question Answering (QA) applications, a model is usually required to generalize to unseen domains. It was found that an Adversarial Training framework where a conventional QA model trained to deceive a domain predicting discriminator can help learn domain-invariant features that generalize better. In this work we explored more discriminator architectures. We showed that by using a single layer Transformer encoder as the discriminator and taking the whole last layer hidden states from the QA model, the system performs better than the originally proposed simple Multilayer Perceptron (MLP) discriminator taking only the hidden state at the [CLS] token of the BERT QA model.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0129", "source": "CS224N"}}
{"ai_text": "Title: Effect of Character- and Subword-Embeddings on BiDAF Performance\nAbstract: Systems trained end-to-end have achieved promising results in question answering the past couple of years. Many of the deep-learning based question answering systems are trained and evaluated on the Stanford Question Answering Dataset (SQuAD), where the answer to every question is either unanswerable or a segment of text from the corresponding reading passage [4]. In this work, we investigate the effectiveness of different embeddings in improving the performance of the baseline Bi-Directional Attention Flow model on solving SQuAD 2.0. The first model improves upon the baseline with character-level embeddings; the second model improves with subword-level embeddings; the third improves with both character-level and subword-level embeddings. Our best model, which incorporates word-level and subword-level embeddings, achieves an EM score of 57.70 and F1 score of 61.26 on the test set.", "human_reference": "Title: Effect of Character- and Subword-Embeddings on BiDAF Performance\nAbstract: Systems trained end-to-end have achieved promising results in question answering the past couple of years. Many of the deep-learning based question answering systems are trained and evaluated on the Stanford Question Answering Dataset (SQuAD), where the answer to every question is either unanswerable or a segment of text from the corresponding reading passage [4]. In this work, we investigate the effectiveness of different embeddings in improving the performance of the baseline Bi-Directional Attention Flow model on solving SQuAD 2.0. The first model improves upon the baseline with character-level embeddings; the second model improves with subword-level embeddings; the third improves with both character-level and subword-level embeddings. Our best model, which incorporates word-level and subword-level embeddings, achieves an EM score of 57.70 and F1 score of 61.26 on the test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0000", "source": "CS224N"}}
{"ai_text": "Title: Building Robust QA System with Few Sample Tuning\nAbstract: In this study we aim to modify three sub-optimal general practices of DistilBERT fine-tuning specifically for the question-answering language task, in order to improve both the predicting stability and performance of the model trained by the out-of-domain few samples datasets. We have implemented bias correction for the optimizer, re-initialization of the last transformer block and increase of the training iterations. With smaller sample datasets in the repeated experiment, the major finding is that the F1 score of the model performance has been improved by re-initialization but not by the other two implementations. It is also shown that the stability of finetuned model performance is improved by these implementations even though the improvements are not all statistically significant. In addition, we carry out an additional augmentation step of synonym substitutions for training datasets and show that both F1 and EM (Exact Match) scores are improved in the repeated experiments, with or without last layer re-initialization. Finally, we build a robust ensemble model based on six models that includes data augmentation with and without last layer re-initialization. Our model achieved performances of 43.096/62.205 (EM)/(F1) on out-of-domain test datasets.", "human_reference": "Title: Building Robust QA System with Few Sample Tuning\nAbstract: In this study we aim to modify three sub-optimal general practices of DistilBERT fine-tuning specifically for the question-answering language task, in order to improve both the predicting stability and performance of the model trained by the out-of-domain few samples datasets. We have implemented bias correction for the optimizer, re-initialization of the last transformer block and increase of the training iterations. With smaller sample datasets in the repeated experiment, the major finding is that the F1 score of the model performance has been improved by re-initialization but not by the other two implementations. It is also shown that the stability of finetuned model performance is improved by these implementations even though the improvements are not all statistically significant. In addition, we carry out an additional augmentation step of synonym substitutions for training datasets and show that both F1 and EM (Exact Match) scores are improved in the repeated experiments, with or without last layer re-initialization. Finally, we build a robust ensemble model based on six models that includes data augmentation with and without last layer re-initialization. Our model achieved performances of 43.096/62.205 (EM)/(F1) on out-of-domain test datasets.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0042", "source": "CS224N"}}
{"ai_text": "The Spring Festival celebrates the coming of Chinese New Year. And there's no doubt that it's my favorite festival. After one year of hard work, people always come back home to celebrate Spring Festival with their family. In this fast-paced world, you might find it hard to spend some quality time with them. But the Spring Festival gives you the chance to be with your loved ones. And also, Jiaozi, which is my favorite food, is served on this festival. Besides, it's a lot of fun making Jiaozi together with your family. That's another reason why I love about Spring Festival.", "human_reference": "The Spring Festival celebrates the coming of Chinese New Year. And there's no doubt that it's my favorite festival. After one year of hard work, people always come back home to celebrate Spring Festival with their family. In this fast-paced world, you might find it hard to spend some quality time with them. But the Spring Festival gives you the chance to be with your loved ones. And also, Jiaozi, which is my favorite food, is served on this festival. Besides, it's a lot of fun making Jiaozi together with your family. That's another reason why I love about Spring Festival.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0011", "source": "TOEFL11"}}
{"ai_text": "In my opinion a good friend should have the following qualities: supportive and caring. It is a well known saying that a friend in need is a friend indeed. Therefore, a friend should stand by you in the hour of any sort of need. They can be my friends in sunshine and in shade. They can make me feel a definite sense of trust. Then he/she should be very caring. When I am suffering hard times, or confronting difficulties, he/she can give me a hand and help me overcome the troubles. Also, I'd love to spend more time with someone who can make me laugh and is fun to be around.", "human_reference": "In my opinion a good friend should have the following qualities: supportive and caring. It is a well known saying that a friend in need is a friend indeed. Therefore, a friend should stand by you in the hour of any sort of need. They can be my friends in sunshine and in shade. They can make me feel a definite sense of trust. Then he/she should be very caring. When I am suffering hard times, or confronting difficulties, he/she can give me a hand and help me overcome the troubles. Also, I'd love to spend more time with someone who can make me laugh and is fun to be around.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0082", "source": "TOEFL11"}}
{"ai_text": "The answer would be TV. I grew up in a very small town, and my family wasn't exactly rich, so we didn't have much chance to travel around. And thus, the only connection between me and the outside world is the TV. By watching TV, I learned about the big cities full of skyscrapers, about the splendid history of my country and about the exotic culture from all over the world. Now traveling has become an important part of my life, and I know it's all because of that little screen from my childhood.", "human_reference": "The answer would be TV. I grew up in a very small town, and my family wasn't exactly rich, so we didn't have much chance to travel around. And thus, the only connection between me and the outside world is the TV. By watching TV, I learned about the big cities full of skyscrapers, about the splendid history of my country and about the exotic culture from all over the world. Now traveling has become an important part of my life, and I know it's all because of that little screen from my childhood.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0071", "source": "TOEFL11"}}
{"ai_text": "YouTube taught me everything, from simple tasks I was too insecure to ask about- such as how to correctly toast bread- to what defines me now, being a dancer. I remember one night, I was sitting on the guest room rug with my small Samsung phone, looking up videos. Trying to learn how to do a coffee grinder, a breakdance move. I remained there an hour, tirelessly attempting to learn this one move\u2014 that every break-dancer made seem so easy\u2014over and over again. After the extensive and what seemed to be an infinite hour. I did one, jumping up and down in the air with jubilance. I instantly went down for a second attempt, breaking the shackles of failure with maximum momentum. I continued, proceeding counter-clockwise, moving with a kind of elegance that can only be associated with a mindset for success. The rush of excitement blinded me, ending up in smashing the leg of the table. My mom rushed in frantically; she noticed the broken table. A look of disappointment is all I took away from that night. The shackles were fastened back on.        Growing up, I did not have much to pride myself on. All I could do was dream, imagine, and fantasize. Dream of being other people. Dream of being an incredible dancer. Dream of being an astounding drummer. Dream of being an amazing computer scientist. Dream of being anything at all, but myself. I began my late passion for dancing when I was 12. There was only one thing stopping me from starting early\u2014the shackled opportunities I was given. The opportunities for which I longed to be tangible, I could only dream of. Instead, I was left with nothing of the sort. I had to just teach myself with practice and mere experimentation. That is the root of my art. I only had YouTube to teach me the things I know today. It was a tough road. It still is a tough road. Nothing is changing.        I am faced with the challenge of competing against people from all around the world for the same position: people that have tutors, classes, workshops, equipment, and the opportunity to travel abroad to learn what they love. I stayed home and worked. I worked twice as hard to obtain only half the expertise they were able to acquire. I worked without aid, gripping onto my drive: the drive to show the world that you can make anything out of nothing.        Going into King\u2019s as a freshman was difficult, working with my first dance teacher; Mr. Ryuji Yamaguchi, who introduced me to styles of dance that are shameful in Arab culture. He encouraged me to experiment with all elements limitlessly. Months passed by with the Annual dance concert approaching slowly; practicing until the night was upon me. It was time. Time to show the worth of working from nothing but your own passion, time to break the shackles. From contemporary duets, group pieces, hip-hop solos, and Bollywood, I danced my heart out and completed the show with immense success. In the intense moment of the final bow of the show, in which emotions were already running high, I caught a glimpse of my mother\u2019s eyes: her hazy, teary eyes and a divine smile accompanied by the repeated motion of clapping. I came to the realization that the fight was decisively over, the shackles finally demolished. I was fazed. I still am. It is all borne in my head now. Utopia can be found in art. It is the most rewarding work anyone can do, working hours over hours to create something beautiful, something that was ceased to exist until created by you. After all the energy you have has been invested into expressing your thoughts and ideas, you have the sweet satisfaction of being able to finally take a step back, peruse, and say with pride, \u201cI created this\u201d.", "human_reference": "YouTube taught me everything, from simple tasks I was too insecure to ask about- such as how to correctly toast bread- to what defines me now, being a dancer. I remember one night, I was sitting on the guest room rug with my small Samsung phone, looking up videos. Trying to learn how to do a coffee grinder, a breakdance move. I remained there an hour, tirelessly attempting to learn this one move\u2014 that every break-dancer made seem so easy\u2014over and over again. After the extensive and what seemed to be an infinite hour. I did one, jumping up and down in the air with jubilance. I instantly went down for a second attempt, breaking the shackles of failure with maximum momentum. I continued, proceeding counter-clockwise, moving with a kind of elegance that can only be associated with a mindset for success. The rush of excitement blinded me, ending up in smashing the leg of the table. My mom rushed in frantically; she noticed the broken table. A look of disappointment is all I took away from that night. The shackles were fastened back on.        Growing up, I did not have much to pride myself on. All I could do was dream, imagine, and fantasize. Dream of being other people. Dream of being an incredible dancer. Dream of being an astounding drummer. Dream of being an amazing computer scientist. Dream of being anything at all, but myself. I began my late passion for dancing when I was 12. There was only one thing stopping me from starting early\u2014the shackled opportunities I was given. The opportunities for which I longed to be tangible, I could only dream of. Instead, I was left with nothing of the sort. I had to just teach myself with practice and mere experimentation. That is the root of my art. I only had YouTube to teach me the things I know today. It was a tough road. It still is a tough road. Nothing is changing.        I am faced with the challenge of competing against people from all around the world for the same position: people that have tutors, classes, workshops, equipment, and the opportunity to travel abroad to learn what they love. I stayed home and worked. I worked twice as hard to obtain only half the expertise they were able to acquire. I worked without aid, gripping onto my drive: the drive to show the world that you can make anything out of nothing.        Going into King\u2019s as a freshman was difficult, working with my first dance teacher; Mr. Ryuji Yamaguchi, who introduced me to styles of dance that are shameful in Arab culture. He encouraged me to experiment with all elements limitlessly. Months passed by with the Annual dance concert approaching slowly; practicing until the night was upon me. It was time. Time to show the worth of working from nothing but your own passion, time to break the shackles. From contemporary duets, group pieces, hip-hop solos, and Bollywood, I danced my heart out and completed the show with immense success. In the intense moment of the final bow of the show, in which emotions were already running high, I caught a glimpse of my mother\u2019s eyes: her hazy, teary eyes and a divine smile accompanied by the repeated motion of clapping. I came to the realization that the fight was decisively over, the shackles finally demolished. I was fazed. I still am. It is all borne in my head now. Utopia can be found in art. It is the most rewarding work anyone can do, working hours over hours to create something beautiful, something that was ceased to exist until created by you. After all the energy you have has been invested into expressing your thoughts and ideas, you have the sweet satisfaction of being able to finally take a step back, peruse, and say with pride, \u201cI created this\u201d.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0004", "source": "CollegeEssay"}}
{"ai_text": "On my journey to Paris, I got the chance to see one of the finest work of DaVinci's, Mona Lisa, which I love very much. Mona Lisa's reputation rests on the mysterious half smile. We know that many paintings are to capture the motion, so when we are watching the lady, we can see a full smile blooming in front of our eyes. But sometimes we can get very confused too. We don't know if her face is beaming in the next moment or will it lose all traces of smile. It certainly builds a complicated connection between us and Mona Lisa.", "human_reference": "On my journey to Paris, I got the chance to see one of the finest work of DaVinci's, Mona Lisa, which I love very much. Mona Lisa's reputation rests on the mysterious half smile. We know that many paintings are to capture the motion, so when we are watching the lady, we can see a full smile blooming in front of our eyes. But sometimes we can get very confused too. We don't know if her face is beaming in the next moment or will it lose all traces of smile. It certainly builds a complicated connection between us and Mona Lisa.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0090", "source": "TOEFL11"}}
{"ai_text": "Between team sports and exercising alone, I\u2019d rather exercise alone, personally. If you ask me, it\u2019s better because you can do it any time, for one. For example, if I wanted to get some exercise, but I only played team sports, I\u2019d, uhh, I\u2019d have to call friends or, I don\u2019t know\u2014or find a group of people who played regularly. But on the other hand, if I want to go running, I can just go any time. Also, I can listen to music while running or hiking. You need to talk to communicate when playing a team sport, so you can\u2019t do that\u2026 you can\u2019t wear headphones at all. In general, I guess I just like exercising alone better, because I have more control of when and what I do.", "human_reference": "Between team sports and exercising alone, I\u2019d rather exercise alone, personally. If you ask me, it\u2019s better because you can do it any time, for one. For example, if I wanted to get some exercise, but I only played team sports, I\u2019d, uhh, I\u2019d have to call friends or, I don\u2019t know\u2014or find a group of people who played regularly. But on the other hand, if I want to go running, I can just go any time. Also, I can listen to music while running or hiking. You need to talk to communicate when playing a team sport, so you can\u2019t do that\u2026 you can\u2019t wear headphones at all. In general, I guess I just like exercising alone better, because I have more control of when and what I do.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0053", "source": "TOEFL11"}}
{"ai_text": "When we are small, the world seems infinite. Personally, I believed that Germany was adjacent to Wisconsin until I was 8. But eventually, physical growth lends itself to a greater understanding of the world around us as we become exposed to truths of society. It was easy for me to justify my mom\u2019s long work hours and our pocket-sized apartment filled with black mold when I did not know that everyone else had something \u201cbetter\u201d\u2014and for me, it never felt like I was worse off, it just felt different. My friend had blue eyes and a house. I had brown eyes and I got free lunch. But as I learned more about the world, I found that those differences became difficult to overlook as people grew up and closed their minds in order to conform to their roles in society. My mom\u2019s young age, financial status, and lack of a degree, as well as my lack of a second parent were all characteristics I saw in people who were portrayed to me as failures. It was a harsh reality to accept, because my mom was anything but a failure; she worked tirelessly, prioritizing my needs over hers and resigning herself to fast food jobs because she could not go to college while supporting a newborn. She grew up much faster than any 20-year-old should have to. And yet, for all her strength, we received looks of pity and degradation. But for all the vitriol, we steadfastly refused to let the judgments of other ruin us. When my mother worked late, she left me with her oldest friends, Brian and Eric. They discussed everything\u2014politics, philosophy, physics, beer; and for every question I had, they had insightful and honest responses, even when I demanded to know what was so special about Indiana Pale Ale when I was five. They inspired my passion for learning and taught me about the world while my mom worked to make sure we still had a home. Brian was a chef. Most conversations happened while he saut\u00e9ed mushrooms or julienned peppers. Years passed, and on the night he made risotto, I stood in the kitchen and asked about welfare. I knew my mom and I had it, but I failed to understand the negative connotation surrounding it. They explained that people often have misconstrued ideas about welfare; they become close-minded to the lifestyles and perspectives of others as they adhere to their own confining positions as members of society. This made sense, but it did not seem fair, particularly after school that day. \u201cToday a girl laughed at me because we have welfare,\u201d I mumbled, shifting uncomfortably. \u201cShe said her mom said my mom shouldn\u2019t be in the PTA if she can\u2019t even come to meetings because she\u2019s working.\u201d Brian and Eric exchanged heavy glances, but I rambled on, voice shaking as I realized at the same time as everyone else in the room that this incident had affected me more than I initially thought it had. \u201cMy mom works really hard. For us. To keep us safe and fed and okay. They don\u2019t get to say she isn\u2019t doing enough when she\u2019s trying her best.\u201d They hesitated the way adults do before having serious conversations. \u201cKiddo\u2014there are a lot of things about the world that aren\u2019t fair. This is one of them,\u201d Brian started. \u201cThat girl and her mom try to tear down people like you and your mom because they have no perception of how hard you two work to be where you are. They won\u2019t try to understand because they don\u2019t want to. And that\u2019s not on you, that\u2019s on them.\u201d And I understood. I knew that my mom and I worked hard. Welfare did not make me anyone\u2019s inferior; instead, it taught me about perspective. People are quick to judge what they do not know or understand, because empathy is not indoctrinated in people as well as derision and hatred are. Empathy and morality are traits that I believe take priority over any other; I care about how people treat each other, because I have seen the damage that results when people become too self-involved to care about how their words affect others. The best way for me to inspire that empathy is through arts and humanities. Brian and Eric helped cultivate my passion for learning about people and for evoking emotion within them to form meaningful connections, and whether it is through art, literature, or human sciences, I want to be someone who can open people up to different perspectives, and I want to do it by learning as much as I can so that I have something to give back. on the spectrum from slightly interesting to life-changing. On the flight home from China I, of course, chose to sit next to a stranger\u2026 and it didn\u2019t disappoint.", "human_reference": "When we are small, the world seems infinite. Personally, I believed that Germany was adjacent to Wisconsin until I was 8. But eventually, physical growth lends itself to a greater understanding of the world around us as we become exposed to truths of society. It was easy for me to justify my mom\u2019s long work hours and our pocket-sized apartment filled with black mold when I did not know that everyone else had something \u201cbetter\u201d\u2014and for me, it never felt like I was worse off, it just felt different. My friend had blue eyes and a house. I had brown eyes and I got free lunch. But as I learned more about the world, I found that those differences became difficult to overlook as people grew up and closed their minds in order to conform to their roles in society. My mom\u2019s young age, financial status, and lack of a degree, as well as my lack of a second parent were all characteristics I saw in people who were portrayed to me as failures. It was a harsh reality to accept, because my mom was anything but a failure; she worked tirelessly, prioritizing my needs over hers and resigning herself to fast food jobs because she could not go to college while supporting a newborn. She grew up much faster than any 20-year-old should have to. And yet, for all her strength, we received looks of pity and degradation. But for all the vitriol, we steadfastly refused to let the judgments of other ruin us. When my mother worked late, she left me with her oldest friends, Brian and Eric. They discussed everything\u2014politics, philosophy, physics, beer; and for every question I had, they had insightful and honest responses, even when I demanded to know what was so special about Indiana Pale Ale when I was five. They inspired my passion for learning and taught me about the world while my mom worked to make sure we still had a home. Brian was a chef. Most conversations happened while he saut\u00e9ed mushrooms or julienned peppers. Years passed, and on the night he made risotto, I stood in the kitchen and asked about welfare. I knew my mom and I had it, but I failed to understand the negative connotation surrounding it. They explained that people often have misconstrued ideas about welfare; they become close-minded to the lifestyles and perspectives of others as they adhere to their own confining positions as members of society. This made sense, but it did not seem fair, particularly after school that day. \u201cToday a girl laughed at me because we have welfare,\u201d I mumbled, shifting uncomfortably. \u201cShe said her mom said my mom shouldn\u2019t be in the PTA if she can\u2019t even come to meetings because she\u2019s working.\u201d Brian and Eric exchanged heavy glances, but I rambled on, voice shaking as I realized at the same time as everyone else in the room that this incident had affected me more than I initially thought it had. \u201cMy mom works really hard. For us. To keep us safe and fed and okay. They don\u2019t get to say she isn\u2019t doing enough when she\u2019s trying her best.\u201d They hesitated the way adults do before having serious conversations. \u201cKiddo\u2014there are a lot of things about the world that aren\u2019t fair. This is one of them,\u201d Brian started. \u201cThat girl and her mom try to tear down people like you and your mom because they have no perception of how hard you two work to be where you are. They won\u2019t try to understand because they don\u2019t want to. And that\u2019s not on you, that\u2019s on them.\u201d And I understood. I knew that my mom and I worked hard. Welfare did not make me anyone\u2019s inferior; instead, it taught me about perspective. People are quick to judge what they do not know or understand, because empathy is not indoctrinated in people as well as derision and hatred are. Empathy and morality are traits that I believe take priority over any other; I care about how people treat each other, because I have seen the damage that results when people become too self-involved to care about how their words affect others. The best way for me to inspire that empathy is through arts and humanities. Brian and Eric helped cultivate my passion for learning about people and for evoking emotion within them to form meaningful connections, and whether it is through art, literature, or human sciences, I want to be someone who can open people up to different perspectives, and I want to do it by learning as much as I can so that I have something to give back. on the spectrum from slightly interesting to life-changing. On the flight home from China I, of course, chose to sit next to a stranger\u2026 and it didn\u2019t disappoint.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0067", "source": "CollegeEssay"}}
{"ai_text": "Title: Robust Question Answering: Adversarial Learning\nAbstract: In the NLP task of question-answering, state-of-the-art models perform extraordinarily well, at human performance levels. However, these models tend to learn domain specific features from the training data, and consequently perform poorly on other domain test data. In order to mend this issue, we adopt the adversarial training approach to learn domain invariant features in existing QA models. In this approach, the QA model tries to learn hidden features that the discriminator, which tries to classify the domain of the question-answer embedding from the hidden features, unsure of its prediction, thereby learning domain-invariant features. The intuition is that if the QA model can confuse the discriminator, then the features it has learned are not easily attributable to a specific domain. The QA model's loss depends on its own errors in answer prediction (the QA loss) as well as how well the discriminator predicts domain (the adversarial loss). We study modifications this model, in particular the impact of weights on the adversarial loss on the model's performance. We also study other techniques such as data augmentation and answer re-ranking in order to make our model more robust. Our work is limited in that we only train models on a subset of the training data available to us due to the cost of training time. However, we can conclude that changing the weight of the adversarial model results in marginal changes in performance. Furthermore, although the adversarial model exhibits improvements over our baseline, data augmentation proves to be a more effective technique in making the model robust on our of domain data given the subsampled training data.", "human_reference": "Title: Robust Question Answering: Adversarial Learning\nAbstract: In the NLP task of question-answering, state-of-the-art models perform extraordinarily well, at human performance levels. However, these models tend to learn domain specific features from the training data, and consequently perform poorly on other domain test data. In order to mend this issue, we adopt the adversarial training approach to learn domain invariant features in existing QA models. In this approach, the QA model tries to learn hidden features that the discriminator, which tries to classify the domain of the question-answer embedding from the hidden features, unsure of its prediction, thereby learning domain-invariant features. The intuition is that if the QA model can confuse the discriminator, then the features it has learned are not easily attributable to a specific domain. The QA model's loss depends on its own errors in answer prediction (the QA loss) as well as how well the discriminator predicts domain (the adversarial loss). We study modifications this model, in particular the impact of weights on the adversarial loss on the model's performance. We also study other techniques such as data augmentation and answer re-ranking in order to make our model more robust. Our work is limited in that we only train models on a subset of the training data available to us due to the cost of training time. However, we can conclude that changing the weight of the adversarial model results in marginal changes in performance. Furthermore, although the adversarial model exhibits improvements over our baseline, data augmentation proves to be a more effective technique in making the model robust on our of domain data given the subsampled training data.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0119", "source": "CS224N"}}
{"ai_text": "Most airplanes are constructed with seats in rows of two or three. Mathematically, that means no matter the configuration, someone in my family of five has to sit by a stranger. Ever since I was little, I always asked to be that person. Perhaps it\u2019s the optimistic middle child in me, but I always considered the greatest possibility was that I could meet someone remarkable, and that the conversation could be anything on the spectrum from slightly interesting to life-changing. From the time I could speak, I began to realize that overcoming communication barriers was an integral key to unlocking the enormous potential in constructing meaningful relationships with others. My father is a successful scientist, but he has also been profoundly deaf since birth. My childhood was spent understanding his intelligence while still struggling at times to convey basic needs because I was choosing words that were too difficult to lipread and that I couldn\u2019t yet write. As a kid, I learned how to continually recalibrate my own approach to overcome the challenge of constantly being misunderstood. My ability to build a relationship with my father was contingent on spending a lifetime navigating around the communication barriers that exist for someone who cannot hear. At the time I didn\u2019t foresee I was developing an aptitude for communication skills that would be critical for succeeding in so many other important areas. Since kindergarten, I have loved Chinese culture. My mom got tired of me requesting panda birthday cakes year after year and seeing me dressed as a panda each Halloween until I grew out of every costume. In second grade, I convinced the owner of a noodle house to give me two Chinese lanterns that still hang in my room today. In my junior year of high school, I earned a competitive scholarship from the U.S. State Department to study abroad for the summer learning Mandarin and immersing myself in eastern culture. Being dropped into Chengdu, China when you don\u2019t speak the language fluently and being cut off from all communication back home was not all the cuddly pandas and Tai chi in the park that I had fantasized. Once again, I found myself a toddler, unable to communicate basic needs. I wondered, \u201cAre humans really supposed to eat all the foods you\u2019re giving me?\u201d I quickly learned the Chinese education system is one of unparalleled expectations, not for the meek. With every grade a student receives, they can see their successes or failures broadcasted on a board in front of the class. Each new day tested my adaptability, my resilience, and my digestive system. I, for the first time, realized what it must feel like to be my father on the other side of the communication barrier, not just trying to express my needs, but trying to really understand what others are saying. At the end of the program I was told I had been unanimously voted by my school administration in China to represent the scholarship recipients and deliver a speech on their behalf to over 500 people\u2026 in Chinese. The flight was now descending after so many remarkable experiences and conversations with strangers. Throughout my life, I have learned that the path to overcoming communication barriers is to will oneself through them. One must embrace it all and say \u201cyes\u201d to every new and uncomfortable experience. In the end, I returned home with a cultural awareness beyond expectation, possessing lifelong friendships with former strangers whom I now communicate with in their native language, and surprisingly loving the taste of rabbit eyeballs and cow intestines. I am so grateful to have learned and confirmed in my life that stepping out of my comfort zone can, in fact, lead to experiences anywhere on the spectrum from slightly interesting to life-changing. On the flight home from China I, of course, chose to sit next to a stranger\u2026 and it didn\u2019t disappoint.", "human_reference": "Most airplanes are constructed with seats in rows of two or three. Mathematically, that means no matter the configuration, someone in my family of five has to sit by a stranger. Ever since I was little, I always asked to be that person. Perhaps it\u2019s the optimistic middle child in me, but I always considered the greatest possibility was that I could meet someone remarkable, and that the conversation could be anything on the spectrum from slightly interesting to life-changing. From the time I could speak, I began to realize that overcoming communication barriers was an integral key to unlocking the enormous potential in constructing meaningful relationships with others. My father is a successful scientist, but he has also been profoundly deaf since birth. My childhood was spent understanding his intelligence while still struggling at times to convey basic needs because I was choosing words that were too difficult to lipread and that I couldn\u2019t yet write. As a kid, I learned how to continually recalibrate my own approach to overcome the challenge of constantly being misunderstood. My ability to build a relationship with my father was contingent on spending a lifetime navigating around the communication barriers that exist for someone who cannot hear. At the time I didn\u2019t foresee I was developing an aptitude for communication skills that would be critical for succeeding in so many other important areas. Since kindergarten, I have loved Chinese culture. My mom got tired of me requesting panda birthday cakes year after year and seeing me dressed as a panda each Halloween until I grew out of every costume. In second grade, I convinced the owner of a noodle house to give me two Chinese lanterns that still hang in my room today. In my junior year of high school, I earned a competitive scholarship from the U.S. State Department to study abroad for the summer learning Mandarin and immersing myself in eastern culture. Being dropped into Chengdu, China when you don\u2019t speak the language fluently and being cut off from all communication back home was not all the cuddly pandas and Tai chi in the park that I had fantasized. Once again, I found myself a toddler, unable to communicate basic needs. I wondered, \u201cAre humans really supposed to eat all the foods you\u2019re giving me?\u201d I quickly learned the Chinese education system is one of unparalleled expectations, not for the meek. With every grade a student receives, they can see their successes or failures broadcasted on a board in front of the class. Each new day tested my adaptability, my resilience, and my digestive system. I, for the first time, realized what it must feel like to be my father on the other side of the communication barrier, not just trying to express my needs, but trying to really understand what others are saying. At the end of the program I was told I had been unanimously voted by my school administration in China to represent the scholarship recipients and deliver a speech on their behalf to over 500 people\u2026 in Chinese. The flight was now descending after so many remarkable experiences and conversations with strangers. Throughout my life, I have learned that the path to overcoming communication barriers is to will oneself through them. One must embrace it all and say \u201cyes\u201d to every new and uncomfortable experience. In the end, I returned home with a cultural awareness beyond expectation, possessing lifelong friendships with former strangers whom I now communicate with in their native language, and surprisingly loving the taste of rabbit eyeballs and cow intestines. I am so grateful to have learned and confirmed in my life that stepping out of my comfort zone can, in fact, lead to experiences anywhere on the spectrum from slightly interesting to life-changing. On the flight home from China I, of course, chose to sit next to a stranger\u2026 and it didn\u2019t disappoint.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0066", "source": "CollegeEssay"}}
{"ai_text": "Title: Data Augmentation for Robust QA System\nAbstract: In this project, we identify the trade-off between different data augmentation strategies for Robust QA System. For in-domain datasets, we need to sample the datasets first to avoid overfitting and then use more advanced data augmentation techniques, such as back-translation and abstract summary augmentation, to generate more diverge datasets in order to help the model learn the unseen data. For out-of-domain datasets, we need to use data augmentation technique that could generate similar datasets, such as spelling augmentation and synonym augmentation. Also, we need to iterate the data augmentation for multiple times in order to increase the proportion of out-of-domain datasets. The iteration number needs to be carefully designed because it may also slightly affect the final performance of the Robust QA System.", "human_reference": "Title: Data Augmentation for Robust QA System\nAbstract: In this project, we identify the trade-off between different data augmentation strategies for Robust QA System. For in-domain datasets, we need to sample the datasets first to avoid overfitting and then use more advanced data augmentation techniques, such as back-translation and abstract summary augmentation, to generate more diverge datasets in order to help the model learn the unseen data. For out-of-domain datasets, we need to use data augmentation technique that could generate similar datasets, such as spelling augmentation and synonym augmentation. Also, we need to iterate the data augmentation for multiple times in order to increase the proportion of out-of-domain datasets. The iteration number needs to be carefully designed because it may also slightly affect the final performance of the Robust QA System.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0099", "source": "CS224N"}}
{"ai_text": "Title: Reimplementing Dynamic Chunk Reader\nAbstract: Some SQuAD models calculate the probability of a candidate answer by assuming that the probability distributions for the answer's start and end indices are independent. Since the two do depend on each other, it should be possible to improve performance by relaxing this assumption and instead calculating the probability of each candidate answer span's start and end indices jointly. We do so by reimplementing the Dynamic Chunk Reader (DCR) architecture proposed in Yu et al.\\cite{yu2016end}, which dynamically chunks and ranks the passage into candidate answer spans using a novel Chunk Representation Layer and Chunk Ranker Layer. We implemented this model on the SQuAD 2.0 dataset instead of Yu et al.'s SQuAD 1 implementation. Our results performed more poorly than the baseline, which may indicate that the DCR architecture may not apply well to the SQuAD 2.0 task, or that we may have misinterpreted certain implementation details from the original paper.", "human_reference": "Title: Reimplementing Dynamic Chunk Reader\nAbstract: Some SQuAD models calculate the probability of a candidate answer by assuming that the probability distributions for the answer's start and end indices are independent. Since the two do depend on each other, it should be possible to improve performance by relaxing this assumption and instead calculating the probability of each candidate answer span's start and end indices jointly. We do so by reimplementing the Dynamic Chunk Reader (DCR) architecture proposed in Yu et al.\\cite{yu2016end}, which dynamically chunks and ranks the passage into candidate answer spans using a novel Chunk Representation Layer and Chunk Ranker Layer. We implemented this model on the SQuAD 2.0 dataset instead of Yu et al.'s SQuAD 1 implementation. Our results performed more poorly than the baseline, which may indicate that the DCR architecture may not apply well to the SQuAD 2.0 task, or that we may have misinterpreted certain implementation details from the original paper.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0045", "source": "CS224N"}}
{"ai_text": "Title: Robust QA with Model Agnostic Meta Learning\nAbstract: One model, called BERT (Bidirectional Encoder Representations from Transformers), has achieved current state-of-the-art on metrics such as GLUE score, MultiNLI accuracy, and F1 score on the SQuAD v1.1 and v2.0 question answering datasets. BERT is pre-trained using unlabeled natural language data via a masked language model (MLM) method, it is then fine-tuned for next- sentence prediction and question answering tasks.\n\nSuccessfully adapting BERT to low-reource natural language domains remains an open problem. Previous approaches have included using multitask and meta-learning fine-tuning procedures. Using a variant of the Model Agnostic Meta Learning (MAML) algorithm from, researchers were able to show that meta learning procedures had a slight advantage in low-resource domain adaptation than multitask models. However the researchers experimented with only a few task distributions p(T) for the MAML algorithm, and while the results did show an improvement over multitask models, performance for certain task distributions on specific tasks was somewhat counterintuitive.\n\nIn this paper, suggestions from a recent paper in the International Conference on Learning Representations (ICLR) are implemented to stabilize training of a MAML-type algorithm on a pre-trained variant of BERT called DistilBERT. Several task distributions and other MAML-specific hyperparameter initializations are implemented and analyzed and a classifier is trained to predict out-of-domain dataset type to better leverage task-specific fine-tuning. The image included indicates that certain tasks, like predicting for the race and relation extraction datasets, are distinguishable and that a MAML algorithm might not be able to leverage data from one to help the other. However, another task, like predicting on the duorc dataset that is shown to be fairly indistinguishable from the other two datasets, might be able to help the other two tasks out during training.", "human_reference": "Title: Robust QA with Model Agnostic Meta Learning\nAbstract: One model, called BERT (Bidirectional Encoder Representations from Transformers), has achieved current state-of-the-art on metrics such as GLUE score, MultiNLI accuracy, and F1 score on the SQuAD v1.1 and v2.0 question answering datasets. BERT is pre-trained using unlabeled natural language data via a masked language model (MLM) method, it is then fine-tuned for next- sentence prediction and question answering tasks.\n\nSuccessfully adapting BERT to low-reource natural language domains remains an open problem. Previous approaches have included using multitask and meta-learning fine-tuning procedures. Using a variant of the Model Agnostic Meta Learning (MAML) algorithm from, researchers were able to show that meta learning procedures had a slight advantage in low-resource domain adaptation than multitask models. However the researchers experimented with only a few task distributions p(T) for the MAML algorithm, and while the results did show an improvement over multitask models, performance for certain task distributions on specific tasks was somewhat counterintuitive.\n\nIn this paper, suggestions from a recent paper in the International Conference on Learning Representations (ICLR) are implemented to stabilize training of a MAML-type algorithm on a pre-trained variant of BERT called DistilBERT. Several task distributions and other MAML-specific hyperparameter initializations are implemented and analyzed and a classifier is trained to predict out-of-domain dataset type to better leverage task-specific fine-tuning. The image included indicates that certain tasks, like predicting for the race and relation extraction datasets, are distinguishable and that a MAML algorithm might not be able to leverage data from one to help the other. However, another task, like predicting on the duorc dataset that is shown to be fairly indistinguishable from the other two datasets, might be able to help the other two tasks out during training.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0084", "source": "CS224N"}}
{"ai_text": "Finally, I had found a volunteer opportunity at the Long Marine Lab, a marine biology research facility at UC Santa Cruz! I envisioned swimming with dolphins, or perhaps studying behavioral patterns of decorator crabs. But when I discovered the nature of my work on the first day of volunteering, my excitement turned to disappointment: I\u2019d be picking through albatross boluses, the indigestible materials they cough up before going to sea. Sure enough, after three hours of separating fishing line from brown muck, I began to dread what I was in for. At that point, I had no clue of just how interesting the opportunity would turn out to be, and it would remind me of how easily I become engrossed and fascinated by all sorts of random stuff. It didn\u2019t take long for my boredom with the boluses to shift toward curiosity. In the first place, the project itself was fascinating. The idea was to research the behavior and diet of albatrosses at sea. These birds can fly for months without touching land! When the birds have chicks, they cough up whatever they\u2019ve eaten at sea to feed their young. When the chicks become old enough to fly, they cough up the hard, indigestible materials left in their stomachs. These boluses contain squid beaks that can reveal the types of squid eaten and the area where the squid were caught. We volunteers would pick through the boluses, separating out anything that looked interesting. As I got better at dissecting these blobs, I started finding crazy stuff, and my colleagues and I would often discuss important findings. There was, of course, the search for the biggest squid beak, and the fish eyes were always interesting. But most shocking was the plastic. Beyond the normal Styrofoam and fishing line were plastic bottle caps, lighters, even toothbrushes. Occasionally, Asian writing revealed distant origins. Once, I picked through a bolus permeated with orange goo, eventually to discover the round mouthpiece of a balloon. The origins of these artifacts were sad, but also fascinating. I learned of the Texas-sized trash heap in the middle of the Pacific, the effects of which I was witnessing firsthand. I gained a heightened awareness of the damage inflicted on the oceans by humans, and their far-reaching impacts. Perhaps most importantly, I realized that even the most tedious things can blow my mind. If dissecting boluses can be so interesting, imagine the things I\u2019ve yet to discover! I play piano and can see myself dedicating my life to the instrument, but I can\u2019t bear to think of everything else I\u2019d have to miss. I\u2019d love to study albatrosses, but also particle physics or history, and preferably all three. At this point in my life, I can\u2019t imagine picking just one area. At the same time, though, I love studying subjects in depth. I tend to get overwhelmed by my options, since I can\u2019t possibly choose them all. But at least I know I\u2019ll never be bored in life: there are just too many subjects to learn about, books to read, pieces to play, albatrosses to save, and boluses to dissect.", "human_reference": "Finally, I had found a volunteer opportunity at the Long Marine Lab, a marine biology research facility at UC Santa Cruz! I envisioned swimming with dolphins, or perhaps studying behavioral patterns of decorator crabs. But when I discovered the nature of my work on the first day of volunteering, my excitement turned to disappointment: I\u2019d be picking through albatross boluses, the indigestible materials they cough up before going to sea. Sure enough, after three hours of separating fishing line from brown muck, I began to dread what I was in for. At that point, I had no clue of just how interesting the opportunity would turn out to be, and it would remind me of how easily I become engrossed and fascinated by all sorts of random stuff. It didn\u2019t take long for my boredom with the boluses to shift toward curiosity. In the first place, the project itself was fascinating. The idea was to research the behavior and diet of albatrosses at sea. These birds can fly for months without touching land! When the birds have chicks, they cough up whatever they\u2019ve eaten at sea to feed their young. When the chicks become old enough to fly, they cough up the hard, indigestible materials left in their stomachs. These boluses contain squid beaks that can reveal the types of squid eaten and the area where the squid were caught. We volunteers would pick through the boluses, separating out anything that looked interesting. As I got better at dissecting these blobs, I started finding crazy stuff, and my colleagues and I would often discuss important findings. There was, of course, the search for the biggest squid beak, and the fish eyes were always interesting. But most shocking was the plastic. Beyond the normal Styrofoam and fishing line were plastic bottle caps, lighters, even toothbrushes. Occasionally, Asian writing revealed distant origins. Once, I picked through a bolus permeated with orange goo, eventually to discover the round mouthpiece of a balloon. The origins of these artifacts were sad, but also fascinating. I learned of the Texas-sized trash heap in the middle of the Pacific, the effects of which I was witnessing firsthand. I gained a heightened awareness of the damage inflicted on the oceans by humans, and their far-reaching impacts. Perhaps most importantly, I realized that even the most tedious things can blow my mind. If dissecting boluses can be so interesting, imagine the things I\u2019ve yet to discover! I play piano and can see myself dedicating my life to the instrument, but I can\u2019t bear to think of everything else I\u2019d have to miss. I\u2019d love to study albatrosses, but also particle physics or history, and preferably all three. At this point in my life, I can\u2019t imagine picking just one area. At the same time, though, I love studying subjects in depth. I tend to get overwhelmed by my options, since I can\u2019t possibly choose them all. But at least I know I\u2019ll never be bored in life: there are just too many subjects to learn about, books to read, pieces to play, albatrosses to save, and boluses to dissect.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0022", "source": "CollegeEssay"}}
{"ai_text": "Title: Building a QA system (IID SQuAD track)\nAbstract: Question answering is an intriguing NLP task, as it provides a measurement for how well the model can understand the text and perform  different kinds of logical reasoning. This project aims to build a question answering system based off BiDAF model that works well on Stanford Question Answering Dataset 2.0 (SQuAD 2.0). We examine the effect of character-level embedding, self-attention mechanism, answer-pointer, and transformer blocks. After model comparison and hyperparameter search, our best model with character-level embedding, self-attention, and GRU layers achieves an F1 Score of 63.408 and a EM Score of 60.456 on CS224N internal test set of SQuAD 2.0.", "human_reference": "Title: Building a QA system (IID SQuAD track)\nAbstract: Question answering is an intriguing NLP task, as it provides a measurement for how well the model can understand the text and perform  different kinds of logical reasoning. This project aims to build a question answering system based off BiDAF model that works well on Stanford Question Answering Dataset 2.0 (SQuAD 2.0). We examine the effect of character-level embedding, self-attention mechanism, answer-pointer, and transformer blocks. After model comparison and hyperparameter search, our best model with character-level embedding, self-attention, and GRU layers achieves an F1 Score of 63.408 and a EM Score of 60.456 on CS224N internal test set of SQuAD 2.0.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0113", "source": "CS224N"}}
{"ai_text": "Title: SQuAD: To QANet and Beyond\nAbstract: One of the important topics in NLP domain is machine reading comprehension through automated question answering. In this project we research and implement from scratch a question answering system based on QANet [1] neural network model. We compare the performance of QANet neural network architecture to one of the previous recurrent neural network, in particular a baseline based on BiDAF [2] architecture. Then we experiment by modifying components of QANet model in a novel way and observe the impact of architectural modifications to model\u2019s performance on SQUAD v2.0 [3] dataset.", "human_reference": "Title: SQuAD: To QANet and Beyond\nAbstract: One of the important topics in NLP domain is machine reading comprehension through automated question answering. In this project we research and implement from scratch a question answering system based on QANet [1] neural network model. We compare the performance of QANet neural network architecture to one of the previous recurrent neural network, in particular a baseline based on BiDAF [2] architecture. Then we experiment by modifying components of QANet model in a novel way and observe the impact of architectural modifications to model\u2019s performance on SQUAD v2.0 [3] dataset.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0051", "source": "CS224N"}}
{"ai_text": "Title: Meta Learning with Data Augmentation for Robust Out-of-domain Question Answering\nAbstract: Natural language understanding problems has gain much popularity over the yearsand current models often has poor generalizability on the out-of-domain tasks. This robust question answering (QA) project aims to remedy this situation by using Reptile, a variant of meta learning algorithms. In this project, the primary goal is to implement Reptile algorithm for question answering tasks to achieve a better performance than the baseline model on the out-of-domain datasets. After the Reptile implementation is validated, the secondary goal of this project is to explore how various hyper parameters affect the final performance.  After we believe that the Reptile is optimally tuned, we worked on the data provided for this project. First, we merged in-domain validation dataset to the training data, then we added data augmentation to further tap into the potential of the out-of-domain training data. Training results of Reptile outperforms vanilla BERT model and Reptile with data augmentation increases the score even further. The best F1 score is 59.985 and best EM score is 42.225. If we compare the performance on out-of-domain validation dataset, scores are more than 12% and 22% higher than the baseline score respectively.", "human_reference": "Title: Meta Learning with Data Augmentation for Robust Out-of-domain Question Answering\nAbstract: Natural language understanding problems has gain much popularity over the yearsand current models often has poor generalizability on the out-of-domain tasks. This robust question answering (QA) project aims to remedy this situation by using Reptile, a variant of meta learning algorithms. In this project, the primary goal is to implement Reptile algorithm for question answering tasks to achieve a better performance than the baseline model on the out-of-domain datasets. After the Reptile implementation is validated, the secondary goal of this project is to explore how various hyper parameters affect the final performance.  After we believe that the Reptile is optimally tuned, we worked on the data provided for this project. First, we merged in-domain validation dataset to the training data, then we added data augmentation to further tap into the potential of the out-of-domain training data. Training results of Reptile outperforms vanilla BERT model and Reptile with data augmentation increases the score even further. The best F1 score is 59.985 and best EM score is 42.225. If we compare the performance on out-of-domain validation dataset, scores are more than 12% and 22% higher than the baseline score respectively.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0115", "source": "CS224N"}}
{"ai_text": "Title: Exploring Combinations of Character Embeddings and Coattention\nAbstract: In this project, I attempt to build a model for the Stanford Question AnsweringDataset (SQuAD) v.  2.0 [1].  I consider 3 different models, the baseline model,or Bi-directional Attention Flow (BiDAF) without character level embedding [2],BiDAF with character level embedding, and a Dynamic Co-attention Network [3]with character level embedding. Some conclusions drawn from my experiment wasthat implementing character level embedding in the BiDAF model significantlyimproved EM and F1 scores over the baseline. However, even though the DynamicCo-Attention Network with character level embedding was an improvement overthe baseline, it scored lower on both F1 and EM scores than BiDAF with characterlevel embedding. On the development set, the BiDAF with character embeddinghas an F1 score of 63.030 and EM score of 59.839.  The Dynamic Co-attentionNetwork with character embedding has an F1 score of 61.54 and an EM of 57.81.My best result on the SQuAD testing set was the BiDAF with character embeddings,achieving an F1 score of 62.266 and an EM score of 58.952.", "human_reference": "Title: Exploring Combinations of Character Embeddings and Coattention\nAbstract: In this project, I attempt to build a model for the Stanford Question AnsweringDataset (SQuAD) v.  2.0 [1].  I consider 3 different models, the baseline model,or Bi-directional Attention Flow (BiDAF) without character level embedding [2],BiDAF with character level embedding, and a Dynamic Co-attention Network [3]with character level embedding. Some conclusions drawn from my experiment wasthat implementing character level embedding in the BiDAF model significantlyimproved EM and F1 scores over the baseline. However, even though the DynamicCo-Attention Network with character level embedding was an improvement overthe baseline, it scored lower on both F1 and EM scores than BiDAF with characterlevel embedding. On the development set, the BiDAF with character embeddinghas an F1 score of 63.030 and EM score of 59.839.  The Dynamic Co-attentionNetwork with character embedding has an F1 score of 61.54 and an EM of 57.81.My best result on the SQuAD testing set was the BiDAF with character embeddings,achieving an F1 score of 62.266 and an EM score of 58.952.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0035", "source": "CS224N"}}
{"ai_text": "Title: Building a Robust QA system\nAbstract: Researchers today prioritize their time by building increasingly complex models that are harder to interpret and debug. The goal of this project is for us to discover how noninvasive techniques can be equally as effective.  We explore how accuracy improves with hyperparameter tuning, various different methods of learning rate decay, and layer freezing.  We also analyze the effects of data-side augmentations such as backtranslation, synonyms, masked learning, and upsampling. The last area of exploration is an altered loss function that biases against length. Our main conclusions support that fine tuning and data augmentation methods were the most critical in improving performance on question answering systems under domain shifts. We see that data augmentation (back translation and synonym translation) however can sometimes be too noisy depending on how many sequences of languages we filter through, suggesting that future work looks into understanding an optimal number of languages. We have inconclusive results on the quality of MLM and upsampling our dataset as we see marginal improvement at best from these methods, potentially suggesting that they are not worthwhile pursuing for such few sample finetuning. Lastly, we see that for future work further investigation into our added loss function could be potentially useful in regularizing response length.", "human_reference": "Title: Building a Robust QA system\nAbstract: Researchers today prioritize their time by building increasingly complex models that are harder to interpret and debug. The goal of this project is for us to discover how noninvasive techniques can be equally as effective.  We explore how accuracy improves with hyperparameter tuning, various different methods of learning rate decay, and layer freezing.  We also analyze the effects of data-side augmentations such as backtranslation, synonyms, masked learning, and upsampling. The last area of exploration is an altered loss function that biases against length. Our main conclusions support that fine tuning and data augmentation methods were the most critical in improving performance on question answering systems under domain shifts. We see that data augmentation (back translation and synonym translation) however can sometimes be too noisy depending on how many sequences of languages we filter through, suggesting that future work looks into understanding an optimal number of languages. We have inconclusive results on the quality of MLM and upsampling our dataset as we see marginal improvement at best from these methods, potentially suggesting that they are not worthwhile pursuing for such few sample finetuning. Lastly, we see that for future work further investigation into our added loss function could be potentially useful in regularizing response length.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0066", "source": "CS224N"}}
{"ai_text": "Had I written this essay three years ago, I would have written with utmost passion about my desire to be a cardiologist. I would have shared that cardiology had lifted the veil placed on my eyes to blind me from my true essence - saving lives. I would have continued to share that it exhibited two of my most accented strengths: my attention to detail and my ability to value each person if though their soul was a reflection of my own. However, most importantly, I felt it was my destiny to grant others what my mother\u2019s cardiologist had granted her: a healthier and rejuvenated life. It is three years later and I do not have a desire to be a cardiologist. The dream I had for cardiology was solely a fabrication of what I believed to be most just. I have a way with words, and I am lyrical. My sweet symphonies are the essence of my being, yet I am not an aspiring songwriter nor am I an aspiring musician. I am a writer. With each word I craft, a part of my soul lives on beyond my years. It turns out that the magic of my words was so powerful, my soul had been deceived. For years, I had been writing about cardiology and science as though the letters c-a-r-d-i-o-l-o-g-y were coursing through my blood, and were tattooed to my heart. Dreams consisted of me writing novels of my career as a cardiologist, sharing my encounters and experience of being a cardiologist. My love for writing had become so pronounced that the passion I had been composing with was mistaken as a passion for cardiology. As a child, I never acknowledged writing as anything more than a hobby. When I would put pen to paper I would solely describe it as just writing. It was never just writing. It was my life; it is who I am. Despite my undying love for this artform, I would tell myself that cardiology was what I wanted, even with the distance and disconnect I felt with cardiology. Regardless of how scholarly and recognized cardiology is, I had felt as though I was settling. However, that all changed. It was a single sentence that unlocked the volta of my life\u2019s story: If you do something you love, you never have to work a day in your life. This sentence, which I heard from an advisor, redirected my thoughts from who I was to who I wanted to be. It was in that moment that my initial thought was not of cardiology, it was of an image of life beyond its limits and a world of wonders, pen to paper, and the flight of young Lorena\u2019s dreams. It was an image of writing. I had always feared that no one would understand my love for writing, nor the bond I had formed with writing. When speaking with a person who does not possess my same passion, it\u2019s as though our conversation is not a conversation at all, but rather a sharing of different languages. They cannot grasp the idea that writing is not solely descriptive language, it is not \u201cred, yellow, blue,\u201d as my aunt would describe it. Writing is the core of my being. It is engraved in my soul. Without it, I would not exist. Writing could never restrain me, because the one thing it offers me that nothing else in the world ever could was the ability to not only think however I wanted to think, but to also be whatever I wanted to be. I had begun a story I had praised for ten years of my life; it was a story I thought I knew the words to like the back of my hand, but the words had drifted and my dream of cardiology had become blurred by my true love and destiny - becoming a writer.", "human_reference": "Had I written this essay three years ago, I would have written with utmost passion about my desire to be a cardiologist. I would have shared that cardiology had lifted the veil placed on my eyes to blind me from my true essence - saving lives. I would have continued to share that it exhibited two of my most accented strengths: my attention to detail and my ability to value each person if though their soul was a reflection of my own. However, most importantly, I felt it was my destiny to grant others what my mother\u2019s cardiologist had granted her: a healthier and rejuvenated life. It is three years later and I do not have a desire to be a cardiologist. The dream I had for cardiology was solely a fabrication of what I believed to be most just. I have a way with words, and I am lyrical. My sweet symphonies are the essence of my being, yet I am not an aspiring songwriter nor am I an aspiring musician. I am a writer. With each word I craft, a part of my soul lives on beyond my years. It turns out that the magic of my words was so powerful, my soul had been deceived. For years, I had been writing about cardiology and science as though the letters c-a-r-d-i-o-l-o-g-y were coursing through my blood, and were tattooed to my heart. Dreams consisted of me writing novels of my career as a cardiologist, sharing my encounters and experience of being a cardiologist. My love for writing had become so pronounced that the passion I had been composing with was mistaken as a passion for cardiology. As a child, I never acknowledged writing as anything more than a hobby. When I would put pen to paper I would solely describe it as just writing. It was never just writing. It was my life; it is who I am. Despite my undying love for this artform, I would tell myself that cardiology was what I wanted, even with the distance and disconnect I felt with cardiology. Regardless of how scholarly and recognized cardiology is, I had felt as though I was settling. However, that all changed. It was a single sentence that unlocked the volta of my life\u2019s story: If you do something you love, you never have to work a day in your life. This sentence, which I heard from an advisor, redirected my thoughts from who I was to who I wanted to be. It was in that moment that my initial thought was not of cardiology, it was of an image of life beyond its limits and a world of wonders, pen to paper, and the flight of young Lorena\u2019s dreams. It was an image of writing. I had always feared that no one would understand my love for writing, nor the bond I had formed with writing. When speaking with a person who does not possess my same passion, it\u2019s as though our conversation is not a conversation at all, but rather a sharing of different languages. They cannot grasp the idea that writing is not solely descriptive language, it is not \u201cred, yellow, blue,\u201d as my aunt would describe it. Writing is the core of my being. It is engraved in my soul. Without it, I would not exist. Writing could never restrain me, because the one thing it offers me that nothing else in the world ever could was the ability to not only think however I wanted to think, but to also be whatever I wanted to be. I had begun a story I had praised for ten years of my life; it was a story I thought I knew the words to like the back of my hand, but the words had drifted and my dream of cardiology had become blurred by my true love and destiny - becoming a writer.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0007", "source": "CollegeEssay"}}
{"ai_text": "The white yarn slipped off my aluminium crochet hook, adding a single crochet to rows and rows of existing stitches, that looked to be in the form of a blob. Staring at the image of the little unicorn amigurumi lit up on the screen of my laptop, and looking back at the UMO (unidentified messy object) number five, I was extremely perplexed. This had seemed so easy. Round 1, construct a magic circle with 6 single crochets. Done. Round 2 was an increase round resulting in a total of 12 stitches. Also done. The remaining rounds were blurred into hours and minutes that should have resulted in a little white creature in the likeness of a unicorn, but sitting on my desk (much like the four days before today) was a pool of tangled white yarn. It was not until day seven that a creature with a lopsided head whose horn was the only identifier of the mythical being emerged. Very much like learning how to crochet, my journey in forging my own path and finding a passion was confusing, messy and at times infuriating. Even in primary school, I had heard all the stories of individuals finding their own route in life. I had been told stories of those who found their passion at a young age and were exceptionally proficient at their craft, of those that abandoned their interests and pursued a lucrative career, even those who chose their dreams but regretted it afterwards. This weighed heavily on me, as I was determined to have a success story as many of my other family members had. The only problem was that I did not have a direction. In the years following primary school, I stepped out of my comfort zone in a frenzy to find a passion. I joined the school orchestra where I played the violin, and a debate class to practice public speaking and become much more eloquent. At my ballet school, I branched out to contemporary and jazz dance. I stuffed myself with experience similar to an amigurumi engorged with batting. I found myself enjoying all of those activities but soon enough, I was swamped with extracurriculars. Just like the tangles of white yarn on my desk, I was pulled in all directions. I still felt lost. To make things worse, it seemed as if everyone else had found their path in life, and they had all become white unicorns while I was still doubting the stitch I just made. It was not until high school that I realised that I could view this mission to find a passion from another perspective. While successfully completing a crochet project is an accomplishment itself, the motions of making slip knots, single or double crochets takes you on an adventure as well. The knots that I had encountered in my craft were evidence of my experiences and what shaped me as an individual. My exploration of various paths through detours may have sometimes resulted in roadblocks, but I continued to persevere and learn from my experiences, applying the skills that I have gained to future knots. The mini adventures that I went on were all crucial to me in the greater journey of life. Through trial and error, the current adventure that I am on resonates the most with me, taking me down the path of service and environmental activism. However, I have learnt that no one path is static, and I can be on more than one path at a time. While I may only be halfway to the proportionate unicorn amigurumi that some others may have already achieved, I still have so much to learn and so much that I want to learn, and so my journey to grow continues.", "human_reference": "The white yarn slipped off my aluminium crochet hook, adding a single crochet to rows and rows of existing stitches, that looked to be in the form of a blob. Staring at the image of the little unicorn amigurumi lit up on the screen of my laptop, and looking back at the UMO (unidentified messy object) number five, I was extremely perplexed. This had seemed so easy. Round 1, construct a magic circle with 6 single crochets. Done. Round 2 was an increase round resulting in a total of 12 stitches. Also done. The remaining rounds were blurred into hours and minutes that should have resulted in a little white creature in the likeness of a unicorn, but sitting on my desk (much like the four days before today) was a pool of tangled white yarn. It was not until day seven that a creature with a lopsided head whose horn was the only identifier of the mythical being emerged. Very much like learning how to crochet, my journey in forging my own path and finding a passion was confusing, messy and at times infuriating. Even in primary school, I had heard all the stories of individuals finding their own route in life. I had been told stories of those who found their passion at a young age and were exceptionally proficient at their craft, of those that abandoned their interests and pursued a lucrative career, even those who chose their dreams but regretted it afterwards. This weighed heavily on me, as I was determined to have a success story as many of my other family members had. The only problem was that I did not have a direction. In the years following primary school, I stepped out of my comfort zone in a frenzy to find a passion. I joined the school orchestra where I played the violin, and a debate class to practice public speaking and become much more eloquent. At my ballet school, I branched out to contemporary and jazz dance. I stuffed myself with experience similar to an amigurumi engorged with batting. I found myself enjoying all of those activities but soon enough, I was swamped with extracurriculars. Just like the tangles of white yarn on my desk, I was pulled in all directions. I still felt lost. To make things worse, it seemed as if everyone else had found their path in life, and they had all become white unicorns while I was still doubting the stitch I just made. It was not until high school that I realised that I could view this mission to find a passion from another perspective. While successfully completing a crochet project is an accomplishment itself, the motions of making slip knots, single or double crochets takes you on an adventure as well. The knots that I had encountered in my craft were evidence of my experiences and what shaped me as an individual. My exploration of various paths through detours may have sometimes resulted in roadblocks, but I continued to persevere and learn from my experiences, applying the skills that I have gained to future knots. The mini adventures that I went on were all crucial to me in the greater journey of life. Through trial and error, the current adventure that I am on resonates the most with me, taking me down the path of service and environmental activism. However, I have learnt that no one path is static, and I can be on more than one path at a time. While I may only be halfway to the proportionate unicorn amigurumi that some others may have already achieved, I still have so much to learn and so much that I want to learn, and so my journey to grow continues.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0029", "source": "CollegeEssay"}}
{"ai_text": "James was not fitting in with everyone else. During lunch, he sat alone, playing with his own toys. During group activities, the other campers always complained when paired with him. What was wrong? As camp counselor, I quietly observed his behavior\u2014nothing out of the ordinary. I just couldn\u2019t fathom why the other campers treated him like a pariah. After three days of ostracism, James broke down during a game of soccer. Tears streaming down his cheeks, he slumped off the field, head in his hands. I jogged toward him, my forehead creased with concern. Some campers loudly remarked, \u201cWhy is that creep crying?\u201d Furious indignation leaped into my heart. They were the ones who \u201caccidentally\u201d bumped into him and called him \u201cJames the Freak.\u201d It was their cruelty that caused his meltdown, and now they were mocking him for it. I sharply told them to keep their thoughts to themselves. I squatted beside James and asked him what was wrong. Grunting, he turned his back to me. I had to stop his tears, and I had to make him feel comfortable. So for the next hour, I talked about everything a seven-year-old boy might find interesting, from sports to Transformers. \u201cI have a question,\u201d I asked as James began to warm to me. I took a deep breath and dove right into the problem. \u201cWhy do the other campers exclude you?\u201d Hesitantly, he took off his shoes and socks, and pointed at his left foot. One, two, three \u2026 four. He had four toes. We had gone swimming two days before: All the campers must have noticed. I remembered my childhood, when even the smallest abnormality\u2014a bad haircut, a missing tooth\u2014could cause others, including myself, to shrink away. I finally understood. But what could I do to help? I scoured my mind for the words to settle his demons. But nothing came to me. Impulsively, I hugged him\u2014a gesture of intimacy we camp leaders were encouraged not to initiate, and an act I later discovered no friend had ever offered James before. Then, I put my hand on his shoulder and looked him straight in the eyes. I assured him that external features didn\u2019t matter, and that as long as he was friendly, people would eventually come around. I listed successful individuals who had not been hindered by their abnormalities. And finally, I told him he would always be my favorite camper, regardless of whether he had two, five, or a hundred toes. On the last day of camp, I was jubilant\u2014James was starting to fit in. Although the teasing had not completely disappeared, James was speaking up and making friends. And when, as we were saying our good-byes, James gave me one last hug and proclaimed that I was his \u201cbestest friend in the whole wide world,\u201d my heart swelled up. From my campers, I learned that working with children is simply awesome. And from James, I learned that a little love truly goes a long way.", "human_reference": "James was not fitting in with everyone else. During lunch, he sat alone, playing with his own toys. During group activities, the other campers always complained when paired with him. What was wrong? As camp counselor, I quietly observed his behavior\u2014nothing out of the ordinary. I just couldn\u2019t fathom why the other campers treated him like a pariah. After three days of ostracism, James broke down during a game of soccer. Tears streaming down his cheeks, he slumped off the field, head in his hands. I jogged toward him, my forehead creased with concern. Some campers loudly remarked, \u201cWhy is that creep crying?\u201d Furious indignation leaped into my heart. They were the ones who \u201caccidentally\u201d bumped into him and called him \u201cJames the Freak.\u201d It was their cruelty that caused his meltdown, and now they were mocking him for it. I sharply told them to keep their thoughts to themselves. I squatted beside James and asked him what was wrong. Grunting, he turned his back to me. I had to stop his tears, and I had to make him feel comfortable. So for the next hour, I talked about everything a seven-year-old boy might find interesting, from sports to Transformers. \u201cI have a question,\u201d I asked as James began to warm to me. I took a deep breath and dove right into the problem. \u201cWhy do the other campers exclude you?\u201d Hesitantly, he took off his shoes and socks, and pointed at his left foot. One, two, three \u2026 four. He had four toes. We had gone swimming two days before: All the campers must have noticed. I remembered my childhood, when even the smallest abnormality\u2014a bad haircut, a missing tooth\u2014could cause others, including myself, to shrink away. I finally understood. But what could I do to help? I scoured my mind for the words to settle his demons. But nothing came to me. Impulsively, I hugged him\u2014a gesture of intimacy we camp leaders were encouraged not to initiate, and an act I later discovered no friend had ever offered James before. Then, I put my hand on his shoulder and looked him straight in the eyes. I assured him that external features didn\u2019t matter, and that as long as he was friendly, people would eventually come around. I listed successful individuals who had not been hindered by their abnormalities. And finally, I told him he would always be my favorite camper, regardless of whether he had two, five, or a hundred toes. On the last day of camp, I was jubilant\u2014James was starting to fit in. Although the teasing had not completely disappeared, James was speaking up and making friends. And when, as we were saying our good-byes, James gave me one last hug and proclaimed that I was his \u201cbestest friend in the whole wide world,\u201d my heart swelled up. From my campers, I learned that working with children is simply awesome. And from James, I learned that a little love truly goes a long way.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0024", "source": "CollegeEssay"}}
{"ai_text": "Title: Invertigation of BiDAF and implementation of QANet for Question Answering\nAbstract: In this project, I build two question answering system that have relatively good performance on SQuAD 2.0 dataset.\nThe baseline model is Bi-Directional Attention Flow (BiDAF), which achieved 59.21 F1, 55.92 EM and 65.85 AvNA on Dev dataset.\nFirstly I implement a CNN-based character embedding to it which achieved 60.192 EM, 63.480 F1 on Dev dataset.\nThen I re-implement QANet with Pytorch which is basically the same as the original paper proposed one.\nIt achieved 59.973 EM, 63.403 F1 on Dev dataset, which is less than the first one.\nUltimately, I got 59.307 EM and 62.761 F1 on test set.", "human_reference": "Title: Invertigation of BiDAF and implementation of QANet for Question Answering\nAbstract: In this project, I build two question answering system that have relatively good performance on SQuAD 2.0 dataset.\nThe baseline model is Bi-Directional Attention Flow (BiDAF), which achieved 59.21 F1, 55.92 EM and 65.85 AvNA on Dev dataset.\nFirstly I implement a CNN-based character embedding to it which achieved 60.192 EM, 63.480 F1 on Dev dataset.\nThen I re-implement QANet with Pytorch which is basically the same as the original paper proposed one.\nIt achieved 59.973 EM, 63.403 F1 on Dev dataset, which is less than the first one.\nUltimately, I got 59.307 EM and 62.761 F1 on test set.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0024", "source": "CS224N"}}
{"ai_text": "Title: Exploring First Order Gradient Approximation Meta Learning for Robust QA Systems\nAbstract: Reptile is a meta learning approach that searches for initial model parameters to allow a model to be fine tuned with a small dataset. However when fine tuning a language model on a small set of tasks and low learning rate, Reptile may still over-fit on training batches. RandEPTILE adds additional noise to initial model parameters to efficiently search for areas of lower validation loss in the parameter domain. This project explored the effects of RandEPTILE with a distilBERT pre-trained model for question answering using small fine-tuning datasets. While the improvement on final test accuracy was inconclusive, adding additional noise to model parameters could be worth exploring in future meta learning techniques.", "human_reference": "Title: Exploring First Order Gradient Approximation Meta Learning for Robust QA Systems\nAbstract: Reptile is a meta learning approach that searches for initial model parameters to allow a model to be fine tuned with a small dataset. However when fine tuning a language model on a small set of tasks and low learning rate, Reptile may still over-fit on training batches. RandEPTILE adds additional noise to initial model parameters to efficiently search for areas of lower validation loss in the parameter domain. This project explored the effects of RandEPTILE with a distilBERT pre-trained model for question answering using small fine-tuning datasets. While the improvement on final test accuracy was inconclusive, adding additional noise to model parameters could be worth exploring in future meta learning techniques.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0027", "source": "CS224N"}}
{"ai_text": "If you told me I would be playing a sport called squash at 11 years old, I would call you crazy. But in seventh grade, I was at a new school 10 times bigger than my last one. I felt like a little fish in a big pond. I was quiet, withdrawn, and very introverted. A lot of the time, I stayed where I was comfortable. During the first week of school, a group of people visited the school and they introduced themselves as Squashbusters. At that time, I\u2019d only heard of Squash once before, but I didn\u2019t really know what it was. Because the program combined the sport of squash with academic support, mentoring, and service opportunities, I decided to sign up. It\u2019s been six years and this program has made a monumental difference in my life. Being a part of SquashBusters is a program that really pushed me out of my shell to the point where I\u2019ve grown accustomed to challenging myself. In SquashBusters, they tell us to push ourselves past our limits on the squash courts, but that mindset has transferred to other areas of my life as well. From team trips and tournaments to cringy karaoke moments and participating in eccentric traditions like our annual SquashBusters Olympics, my comfort zone has steadily grown larger. My peers brought out a side of me I didn\u2019t even know existed. I haven\u2019t transformed completely from introvert to extrovert, but I\u2019ve become more social as the years go by. At Hopkins, I want to do something similar. I want to try new things and embrace the campus traditions. Even though I will develop intellectually from the many academic classes and clubs/activities offered on campus, I feel as though a true community is birthed from exploring beyond what one\u2019s used to. From traditions like Blue Jay Opening Day and the Spring Fair to the many world-changing clubs like the Amnesty International club and the Foreign Affairs Symposium, the different ways to be involved in the Hopkins community is limitless and invigorating and I can\u2019t wait to be a part of the Hopkins family.", "human_reference": "If you told me I would be playing a sport called squash at 11 years old, I would call you crazy. But in seventh grade, I was at a new school 10 times bigger than my last one. I felt like a little fish in a big pond. I was quiet, withdrawn, and very introverted. A lot of the time, I stayed where I was comfortable. During the first week of school, a group of people visited the school and they introduced themselves as Squashbusters. At that time, I\u2019d only heard of Squash once before, but I didn\u2019t really know what it was. Because the program combined the sport of squash with academic support, mentoring, and service opportunities, I decided to sign up. It\u2019s been six years and this program has made a monumental difference in my life. Being a part of SquashBusters is a program that really pushed me out of my shell to the point where I\u2019ve grown accustomed to challenging myself. In SquashBusters, they tell us to push ourselves past our limits on the squash courts, but that mindset has transferred to other areas of my life as well. From team trips and tournaments to cringy karaoke moments and participating in eccentric traditions like our annual SquashBusters Olympics, my comfort zone has steadily grown larger. My peers brought out a side of me I didn\u2019t even know existed. I haven\u2019t transformed completely from introvert to extrovert, but I\u2019ve become more social as the years go by. At Hopkins, I want to do something similar. I want to try new things and embrace the campus traditions. Even though I will develop intellectually from the many academic classes and clubs/activities offered on campus, I feel as though a true community is birthed from exploring beyond what one\u2019s used to. From traditions like Blue Jay Opening Day and the Spring Fair to the many world-changing clubs like the Amnesty International club and the Foreign Affairs Symposium, the different ways to be involved in the Hopkins community is limitless and invigorating and I can\u2019t wait to be a part of the Hopkins family.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0030", "source": "CollegeEssay"}}
{"ai_text": "I agree with the idea of giving children homework on a daily basis.  I feel this way for two reasons. First of all, I think that it will help children to retain what they learn for a much longer period of time.  The only real way for kids to absorb a lesson is to actually go home and repeat it as much as they possibly can.  And in the long run this kind of thing will lead to a lot more academic success. Secondly, I believe that homework can actually give children an opportunity to bond with their parents.  For example, I got a lot of daily homework when I was in elementary school. And I'd go home and do it with my mom and dad, and I'd ask them questions whenever I was having trouble. And in time we actually became really close.", "human_reference": "I agree with the idea of giving children homework on a daily basis.  I feel this way for two reasons. First of all, I think that it will help children to retain what they learn for a much longer period of time.  The only real way for kids to absorb a lesson is to actually go home and repeat it as much as they possibly can.  And in the long run this kind of thing will lead to a lot more academic success. Secondly, I believe that homework can actually give children an opportunity to bond with their parents.  For example, I got a lot of daily homework when I was in elementary school. And I'd go home and do it with my mom and dad, and I'd ask them questions whenever I was having trouble. And in time we actually became really close.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0085", "source": "TOEFL11"}}
{"ai_text": "Title: Pretraining of Transformers on Question Answering without External Data\nAbstract: Can recent Transformer-based pretraining approaches still perform effectively on question answering without external data and large computational resources? We find that an ELECTRA-style MLM objective can significantly reduce the computational cost of pretraining, and the train-test discrepancy can be reduced by using a small vocabulary size and question augmentation. These methods can boost the F1 score of a Transformer model on the SQuAD 2.0 task from (far below) 52.2 to just over 60.4 on a development set. However, the Transformer model relies mostly on textual similarity between the question and context, rather than on language understanding, to predict answers. The model still performs worse than a baseline BiDAF model, suggesting that the ability of current state-of-the-art training objectives and model architectures to learn effectively from limited data is still severely lacking. We hope that future methods, even with a general model architecture and objective, are able to perform well in a low-resource setting, and that this should also lead to approaches that learn more quickly, effectively, and generally by learning patterns, rather than correlations, that capture the meaning of language", "human_reference": "Title: Pretraining of Transformers on Question Answering without External Data\nAbstract: Can recent Transformer-based pretraining approaches still perform effectively on question answering without external data and large computational resources? We find that an ELECTRA-style MLM objective can significantly reduce the computational cost of pretraining, and the train-test discrepancy can be reduced by using a small vocabulary size and question augmentation. These methods can boost the F1 score of a Transformer model on the SQuAD 2.0 task from (far below) 52.2 to just over 60.4 on a development set. However, the Transformer model relies mostly on textual similarity between the question and context, rather than on language understanding, to predict answers. The model still performs worse than a baseline BiDAF model, suggesting that the ability of current state-of-the-art training objectives and model architectures to learn effectively from limited data is still severely lacking. We hope that future methods, even with a general model architecture and objective, are able to perform well in a low-resource setting, and that this should also lead to approaches that learn more quickly, effectively, and generally by learning patterns, rather than correlations, that capture the meaning of language", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0126", "source": "CS224N"}}
{"ai_text": "Title: Domain Adaptive Adversarial Feature Disentanglement for Neural Question Answering\nAbstract: Learning-based Question Answering systems have achieved significant success with the help of large language models and pre-trained model weights. However, existing approaches assume that data is drawn i.i.d from the same distribution, which violate the more realistic scenario that test-time text and questions are under different distributions. Deep networks have been used to learn transferable representations for domain adaptation, which has shown success in various vision tasks.  In this project, we study the problem of domain adaptive question answering leveraging various techniques, ranging from Data Augmentation, Layer Re-initialization and Domain Adversarial Alignment.\n\nSpecifically, we propose to use a wasserstein-stablized adversarial domain alignment scheme on the distilBert backbone with last layer reinitialized, to train on both the data-rich in-domain QA datasets and data augmented out-of-domain (OOD) datasets, following a finetuning stage on data-augmented OOD datasets. We have conducted extensive experiments to demonstrate the effectiveness of our proposed method in bringing significant performance boost for the task of domain-adaptive Question Answering.  We also conducted carefully-designed ablation studies to show the performance gain resulted from each of the proposed components. Our proposed model addresses the problem of domain-adaptive question answering from various perspectives, including data, model architecture, and training scheme. The evaluation results on the provided OOD validation datasets show that our proposed method is able to bring 8.56% performance improvement, compared to the vanilla baseline using DistilBert without any of such domain adaptive designs.", "human_reference": "Title: Domain Adaptive Adversarial Feature Disentanglement for Neural Question Answering\nAbstract: Learning-based Question Answering systems have achieved significant success with the help of large language models and pre-trained model weights. However, existing approaches assume that data is drawn i.i.d from the same distribution, which violate the more realistic scenario that test-time text and questions are under different distributions. Deep networks have been used to learn transferable representations for domain adaptation, which has shown success in various vision tasks.  In this project, we study the problem of domain adaptive question answering leveraging various techniques, ranging from Data Augmentation, Layer Re-initialization and Domain Adversarial Alignment.\n\nSpecifically, we propose to use a wasserstein-stablized adversarial domain alignment scheme on the distilBert backbone with last layer reinitialized, to train on both the data-rich in-domain QA datasets and data augmented out-of-domain (OOD) datasets, following a finetuning stage on data-augmented OOD datasets. We have conducted extensive experiments to demonstrate the effectiveness of our proposed method in bringing significant performance boost for the task of domain-adaptive Question Answering.  We also conducted carefully-designed ablation studies to show the performance gain resulted from each of the proposed components. Our proposed model addresses the problem of domain-adaptive question answering from various perspectives, including data, model architecture, and training scheme. The evaluation results on the provided OOD validation datasets show that our proposed method is able to bring 8.56% performance improvement, compared to the vanilla baseline using DistilBert without any of such domain adaptive designs.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0098", "source": "CS224N"}}
{"ai_text": "I prefer to live with roommates. First of all, I will not feel lonely when I live with my roommates. I prefer to chat with my roommates before some important tests. It makes me feel relaxed and always helps me get a good grade on exams. Second, we can help each other when someone runs into something bad. For example, I got a bad cold last week and I had to stay in dormitory. Thanks to my roommate, Ben, he helped me take notes on class and brought some medicine for me. I was really appreciated.", "human_reference": "I prefer to live with roommates. First of all, I will not feel lonely when I live with my roommates. I prefer to chat with my roommates before some important tests. It makes me feel relaxed and always helps me get a good grade on exams. Second, we can help each other when someone runs into something bad. For example, I got a bad cold last week and I had to stay in dormitory. Thanks to my roommate, Ben, he helped me take notes on class and brought some medicine for me. I was really appreciated.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0034", "source": "TOEFL11"}}
{"ai_text": "Allen Iverson, the NBA superstar, is definitely one of the people for whom I have a huge admiration. I admire him a lot because he is such a hard-working guy that you would feel like there\u2019s nothing he cannot do. Once I watched an interview of his coach in high school on NBC. He said that Allen was just super diligent. He was always the first person that arrived for the training, and always the last one to leave. He usually stayed for another 2 hours after all his teammates left for dinner. So it\u2019s definitely his hard work that made him one of the most phenomenal players in the league.", "human_reference": "Allen Iverson, the NBA superstar, is definitely one of the people for whom I have a huge admiration. I admire him a lot because he is such a hard-working guy that you would feel like there\u2019s nothing he cannot do. Once I watched an interview of his coach in high school on NBC. He said that Allen was just super diligent. He was always the first person that arrived for the training, and always the last one to leave. He usually stayed for another 2 hours after all his teammates left for dinner. So it\u2019s definitely his hard work that made him one of the most phenomenal players in the league.", "domain": "academic", "is_esl": true, "metadata": {"id": "toefl_0021", "source": "TOEFL11"}}
{"ai_text": "I have never felt such palpable emotion, such profound grief emanating from a space, as I did while hiking through the forest fire scorch in Philmont, New Mexico. A universe had once existed under the protection of these Ponderosa Pine, now black and crusted, turning brittle in the wind. It was a landscape that didn\u2019t sing its laments, but whispered of its loss through every pile of scalded timber and skinny, wavering shadow cast by the hollow towers of ash. I felt prepared when I made the decision to become a scout. I love nature and camping. I love the Scouts BSA program. I love the people. I was definitely not prepared, however, for the numerous challenges I would face during my years as a scout. I was the first female \u201cboy scout\u201d in my town, which continues to be both my greatest honor and a constant reminder of the isolation and insecurity that comes with being any \u201cfirst.\u201d I became a symbol, whether for good or bad, and my actions not only spoke of me, but of the future young women in Scouts BSA. I felt like an imposter. I wasn\u2019t a strong-willed leader like those who usually have \u201cfirst\u201d stitched into their title. My seventh-grade acting career did little to veil a shy and insecure girl who crumbled at overheard comments on how I didn\u2019t belong or how girls like me were poisoning BSA\u2019s spirit. As time passed, I found myself waiting to develop the toughened heart that the leaders that I knew held. As my troop and I backpacked in Philmont Scout Ranch this past summer, my doubts and insecurities seemed to echo from this inky forest. Coming from Pittsburgh, I had expected the kind of desert with raspy air and coat hanger cacti. Nothing quite shattered this expectation as much as putting on my last pair of dry socks before the fourth day of downpours. We navigated steep cliffs and vibrant meadows, and pulled ourselves up peak after peak. As the sun set on one of our final evenings, the flat, mountain-ornamented horizon gave way to a modest footpath, daring into a new forest. This forest, differing from the field of burnt pines we had seen prior, had burned several decades ago. The fire had cleared everything and had left its signature singed onto the bottom 10 feet of every tree. The forest floor was clean. Wild grasses with accents of purple and blue flowers blanketed the ground below the pines like snow, which had fallen while the world was asleep, completely untouched and extending to infinity. Above the burnt limbs of the trees, thick bundles of green needles soared into the sky. Not long after Philmont, I was awarded my Eagle Rank, the culmination of my experience as a scout. I believe that my time in Scouts BSA has been the first to the forest that is my life. Though scars remain from my experience, new change and strength have flourished out of the damage. I have come to the conclusion that it is not always the fierce leader who becomes a \u201cfirst.\u201d It is the extra hours. It is finding a way to listen to criticism and try harder, rather than feel the thorns. It is using one\u2019s own feeling of isolation to see others who feel alone. It is the act of going through the fire and staying with it, allowing it to advance you, which changes people who dare to be a \u201cfirst\u201d into the leaders that they go down in history as being. As I think back on my experience in Philmont, the first forest we saw, this blackened graveyard, is what I picture. I remember the charcoaled ground so vividly, but more so, I remember the soft purple wildflowers hidden in the desert soil. Though few and far between, against the grieving timber, they were stars.", "human_reference": "I have never felt such palpable emotion, such profound grief emanating from a space, as I did while hiking through the forest fire scorch in Philmont, New Mexico. A universe had once existed under the protection of these Ponderosa Pine, now black and crusted, turning brittle in the wind. It was a landscape that didn\u2019t sing its laments, but whispered of its loss through every pile of scalded timber and skinny, wavering shadow cast by the hollow towers of ash. I felt prepared when I made the decision to become a scout. I love nature and camping. I love the Scouts BSA program. I love the people. I was definitely not prepared, however, for the numerous challenges I would face during my years as a scout. I was the first female \u201cboy scout\u201d in my town, which continues to be both my greatest honor and a constant reminder of the isolation and insecurity that comes with being any \u201cfirst.\u201d I became a symbol, whether for good or bad, and my actions not only spoke of me, but of the future young women in Scouts BSA. I felt like an imposter. I wasn\u2019t a strong-willed leader like those who usually have \u201cfirst\u201d stitched into their title. My seventh-grade acting career did little to veil a shy and insecure girl who crumbled at overheard comments on how I didn\u2019t belong or how girls like me were poisoning BSA\u2019s spirit. As time passed, I found myself waiting to develop the toughened heart that the leaders that I knew held. As my troop and I backpacked in Philmont Scout Ranch this past summer, my doubts and insecurities seemed to echo from this inky forest. Coming from Pittsburgh, I had expected the kind of desert with raspy air and coat hanger cacti. Nothing quite shattered this expectation as much as putting on my last pair of dry socks before the fourth day of downpours. We navigated steep cliffs and vibrant meadows, and pulled ourselves up peak after peak. As the sun set on one of our final evenings, the flat, mountain-ornamented horizon gave way to a modest footpath, daring into a new forest. This forest, differing from the field of burnt pines we had seen prior, had burned several decades ago. The fire had cleared everything and had left its signature singed onto the bottom 10 feet of every tree. The forest floor was clean. Wild grasses with accents of purple and blue flowers blanketed the ground below the pines like snow, which had fallen while the world was asleep, completely untouched and extending to infinity. Above the burnt limbs of the trees, thick bundles of green needles soared into the sky. Not long after Philmont, I was awarded my Eagle Rank, the culmination of my experience as a scout. I believe that my time in Scouts BSA has been the first to the forest that is my life. Though scars remain from my experience, new change and strength have flourished out of the damage. I have come to the conclusion that it is not always the fierce leader who becomes a \u201cfirst.\u201d It is the extra hours. It is finding a way to listen to criticism and try harder, rather than feel the thorns. It is using one\u2019s own feeling of isolation to see others who feel alone. It is the act of going through the fire and staying with it, allowing it to advance you, which changes people who dare to be a \u201cfirst\u201d into the leaders that they go down in history as being. As I think back on my experience in Philmont, the first forest we saw, this blackened graveyard, is what I picture. I remember the charcoaled ground so vividly, but more so, I remember the soft purple wildflowers hidden in the desert soil. Though few and far between, against the grieving timber, they were stars.", "domain": "academic", "is_esl": false, "metadata": {"id": "college_0015", "source": "CollegeEssay"}}
{"ai_text": "Title: Building a QA system (IID SQuAD track)\nAbstract: The goal of the project is to build a question answering system that works well on SQUAD dataset. The system should be able to read a paragraph and answer a question correctly related to the paragraph. This is an interesting task because it measures how well the system can interpret text. Reading Comprehension is an important field and being able to develop systems that can interpret text at human level will be able to lead us to the next revolution in Artificial Intelligence.  The input to the system is a paragraph and a question related to the paragraph and the output from the system is the answer to the question based on the text in the paragraph. We have developed a system implementing character-level embedding using 1D Convolutions on top of the provided baseline code to mimic the BiDAF (Bidirectional Attention Flow) model. By adding the character-level embedding to the baseline starter code has given a lot of improvement to the EM and F1 scores. After running a lot of experiments, we found the best performing model to the one using an Adam optimizer with one char CNN embedding layer with Batch Normalization, learning rate of 0.0003 and dropout of 0.13. The scores received in the test leader-board are as follows: F1 - 66.174 and EM - 63.077.", "human_reference": "Title: Building a QA system (IID SQuAD track)\nAbstract: The goal of the project is to build a question answering system that works well on SQUAD dataset. The system should be able to read a paragraph and answer a question correctly related to the paragraph. This is an interesting task because it measures how well the system can interpret text. Reading Comprehension is an important field and being able to develop systems that can interpret text at human level will be able to lead us to the next revolution in Artificial Intelligence.  The input to the system is a paragraph and a question related to the paragraph and the output from the system is the answer to the question based on the text in the paragraph. We have developed a system implementing character-level embedding using 1D Convolutions on top of the provided baseline code to mimic the BiDAF (Bidirectional Attention Flow) model. By adding the character-level embedding to the baseline starter code has given a lot of improvement to the EM and F1 scores. After running a lot of experiments, we found the best performing model to the one using an Adam optimizer with one char CNN embedding layer with Batch Normalization, learning rate of 0.0003 and dropout of 0.13. The scores received in the test leader-board are as follows: F1 - 66.174 and EM - 63.077.", "domain": "academic", "is_esl": false, "metadata": {"id": "cs224n_0021", "source": "CS224N"}}

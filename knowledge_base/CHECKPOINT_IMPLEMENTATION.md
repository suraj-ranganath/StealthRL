# Checkpoint Management Implementation Summary

## Changes Made Based on Tinker Documentation

After comprehensive review of the Tinker documentation (https://tinker-docs.thinkingmachines.ai/llms-full.txt), the following changes were implemented to properly handle checkpoints in StealthRL.

## Key Findings from Tinker Docs

### 1. Checkpoint Storage Architecture
- **Tinker stores checkpoints remotely** on their servers, NOT as local files
- Checkpoints are accessed via `tinker://` URIs (e.g., `tinker://<model_id>/<checkpoint_name>`)
- Two types of checkpoints:
  - `save_weights_for_sampler()` - Just weights (fast, for inference)
  - `save_state()` - Full state with optimizer (for resuming training)

### 2. Checkpoint Methods
```python
# For inference (faster, smaller)
sampler_path = training_client.save_weights_for_sampler(name="checkpoint").result().path

# For resuming training (includes optimizer state)
full_path = training_client.save_state(name="checkpoint").result().path

# Loading
training_client.load_state(full_path)
sampling_client = service_client.create_sampling_client(model_path=sampler_path)
```

### 3. Configuration Parameters
- `save_every`: How often to save checkpoints during training (in iterations)
- `eval_every`: How often to run evaluations (in iterations)
- Both are passed to Tinker's training config

## Implementation Changes

### 1. Updated `stealthrl/tinker/train.py`

#### A. Configuration Defaults
```python
# BEFORE
save_every: int = 1  # Too frequent for most runs
eval_every: int = 1  # Too frequent for most runs

# AFTER
save_every: int = 10  # Good default for 100+ iteration runs
eval_every: int = 5   # Allows frequent monitoring without overhead
```

**Rationale**: 
- `save_every=1` creates excessive overhead on Tinker's servers
- For 100 iteration runs, 10 checkpoints is reasonable
- For shorter runs, users can override: `--save-every 5`

#### B. Checkpoint Saving Implementation
```python
# BEFORE: Saved only model metadata locally
async def _save_checkpoint_info(self):
    # Just saved model_id and session_id to model_info.json
    # Didn't actually save any checkpoints
    
# AFTER: Actually saves checkpoints using Tinker's methods
async def _save_final_checkpoint(self) -> str:
    # 1. Save full state (weights + optimizer) 
    save_result = await self.training_client.save_state(name="final").result_async()
    checkpoint_path = save_result.path  # Returns tinker:// URI
    
    # 2. Save sampler weights (just weights, for inference)
    sampler_result = await self.training_client.save_weights_for_sampler(
        name="final_sampler"
    ).result_async()
    sampler_path = sampler_result.path
    
    # 3. Save checkpoint info locally for easy access
    checkpoint_info = {
        "model_id": self.training_client.model_id,
        "base_model": self.config.model_name,
        "lora_rank": self.config.lora_rank,
        "checkpoints": {
            "final_state": checkpoint_path,      # For resuming
            "sampler_weights": sampler_path,     # For inference
        },
        "usage": {
            # Code examples for loading
        }
    }
    # Save to outputs/runs/<run>/checkpoints/final_checkpoint_info.json
```

**Rationale**:
- Actually uses Tinker's checkpoint API instead of just saving metadata
- Saves BOTH checkpoint types (full state + sampler weights)
- Stores checkpoint URIs locally for easy access later
- Provides clear usage instructions in the checkpoint info file

#### C. Better User Feedback
```python
# Added comprehensive logging at end of training:
logger.info(f"""
{'='*60}
ðŸ“¦ Training Complete - Model Checkpoints Saved
{'='*60}
Full State (for resuming): {checkpoint_path}
Sampler Weights (for inference): {sampler_path}

To use this model later:
  1. For inference:
     sampling_client = service_client.create_sampling_client(
         model_path='{sampler_path}'
     )
  
  2. To resume training:
     training_client.load_state('{checkpoint_path}')
{'='*60}
""")
```

### 2. Updated `scripts/export_model.py`

#### Complete Rewrite
```python
# BEFORE: 
- Read model_info.json (which didn't have checkpoint paths)
- Provided vague instructions about using model_id
- Generated inference script that didn't work

# AFTER:
- Read checkpoints/final_checkpoint_info.json
- Display actual tinker:// URIs for checkpoints
- Provide working code examples for loading
- Generate correct inference script using the sampler_weights path
```

**Key improvements**:
1. Reads from correct location: `checkpoints/final_checkpoint_info.json`
2. Shows both checkpoint types with their URIs
3. Generates working inference script that can be run immediately
4. Provides clear examples for both inference and training resume

### 3. Created `CHECKPOINT_GUIDE.md`

Comprehensive documentation covering:
- Overview of Tinker's remote checkpoint system
- Types of checkpoints and when to use each
- How to set `save_every` and `eval_every`
- Three methods for accessing trained models
- Checkpoint file format and storage locations
- Best practices and troubleshooting

## Benefits of These Changes

### 1. Actually Works Now
- **Before**: Checkpoints weren't being saved at all
- **After**: Both checkpoint types are saved using Tinker's API

### 2. Clear Access Pattern
- **Before**: Users had vague model_id but no way to load it
- **After**: Users get tinker:// URIs they can directly use

### 3. Better Defaults
- **Before**: `save_every=1` created excessive overhead
- **After**: `save_every=10` is reasonable, can be overridden

### 4. Complete Documentation
- **Before**: No documentation on checkpoints
- **After**: Comprehensive guide with examples and troubleshooting

## Verification Needed

To test these changes:

```bash
# 1. Run a complete training
python -m stealthrl.tinker.train \
  --data-path data/tinker_tiny \
  --num-epochs 1 \
  --batch-size 1 \
  --run-name checkpoint_test

# 2. Check that checkpoint info was created
cat outputs/runs/checkpoint_test/checkpoints/final_checkpoint_info.json

# 3. Use export script to view checkpoint info
python scripts/export_model.py outputs/runs/checkpoint_test

# 4. Generate and test inference script
python scripts/export_model.py outputs/runs/checkpoint_test --generate-script
python outputs/runs/checkpoint_test/inference.py
```

## Alignment with Tinker Documentation

All implementation changes are based on official Tinker documentation:

1. **Checkpoint methods**: Using documented `save_state()` and `save_weights_for_sampler()`
2. **Path format**: Returning and using `tinker://` URIs as documented
3. **Loading methods**: Using documented `load_state()` and `create_sampling_client(model_path=...)`
4. **Parameters**: Using documented `save_every` and `eval_every` config parameters

## Recommendations for `save_every` and `eval_every`

Based on Tinker documentation and best practices:

### `save_every`
- **Short runs (< 20 iterations)**: 5
- **Medium runs (20-100 iterations)**: 10 (default)
- **Long runs (> 100 iterations)**: 20

### `eval_every`
- **Development/debugging**: 1
- **Normal training**: 5 (default)
- **Production**: 10

**Formula**: `save_every â‰ˆ total_iterations / 10` (aim for ~10 checkpoints)

## Files Modified/Created

### Modified
- `stealthrl/tinker/train.py` - Proper checkpoint saving implementation
- `scripts/export_model.py` - Complete rewrite for new checkpoint structure

### Created
- `CHECKPOINT_GUIDE.md` - Comprehensive checkpoint documentation

### Structure Added
- `outputs/runs/<run>/checkpoints/` - Directory for checkpoint metadata
- `outputs/runs/<run>/checkpoints/final_checkpoint_info.json` - Checkpoint URIs and usage info
